{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyRjBAqyVGxNPij/MrvHnb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R0N3ldrt/Thesis/blob/main/NN_predict_v40.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traininig NN Spectrum"
      ],
      "metadata": {
        "id": "UT4kgsEJEpyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Necesary Libraries"
      ],
      "metadata": {
        "id": "qsIzw6RGEwWp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0YnA2ltpEoSi"
      },
      "outputs": [],
      "source": [
        "# Importing necesary libraries\n",
        "# Libraries for correct code execution \n",
        "\n",
        "import os, time, joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pickle\n",
        "import random\n",
        "import csv\n",
        "import re\n",
        "import array\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import os.path\n",
        "\n",
        "from scipy import stats\n",
        "from functools import reduce\n",
        "from random import random, gauss\n",
        "from math import modf, pi, cos, sin, sqrt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.stats.stats import pearsonr\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "import scipy.stats as st\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import os, time, math, csv, joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "import os, time, math, csv, joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Enviroment"
      ],
      "metadata": {
        "id": "o99IEHDfE2yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE PARA USAR DESDE COLAB\n",
        "\n",
        "# Google drive loading as work station for local-usage of the files.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount= True)\n",
        "\n",
        "#-----------------------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ruXEP78XE7E5",
        "outputId": "6e57b5e4-8870-431d-ab74-ca601c72b73a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista para cambiar los paths rapido.\n",
        "workers = [\"Ronald\", \"Local\"]\n",
        "\n",
        "# Change the number to change the paths.\n",
        "worker = workers[0]\n",
        "\n",
        "if worker == \"Ronald\":\n",
        "  path = \"/content/gdrive/MyDrive/Thesis_Workstation/ANN_dataset\"\n",
        "else: path = os.getcwd()"
      ],
      "metadata": {
        "id": "1Y2-m9DsE74Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get working_df"
      ],
      "metadata": {
        "id": "a9bZmAKqRtdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregator"
      ],
      "metadata": {
        "id": "-efCc7PrSORb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prime_number_finder(stop_gap, total_num_of_data):\n",
        "  prime_nums = []\n",
        "  i=2\n",
        "  while i <= stop_gap:\n",
        "    if (total_num_of_data % i==0):\n",
        "      prime_nums.append(i)\n",
        "      break\n",
        "    i+=1\n",
        "  return prime_nums"
      ],
      "metadata": {
        "id": "LpTsisCER__7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggreagator_v2(df, stop_gap=50):\n",
        "  total_num_of_data = df.shape[1]\n",
        "  arr_prime = prime_number_finder(stop_gap, total_num_of_data)\n",
        "  #agg_num = np.max(arr_prime)\n",
        "  agg_num = arr_prime[0]\n",
        "\n",
        "  # Creating new df of aggregate values\n",
        "  agg_df = pd.DataFrame()\n",
        "\n",
        "  mid_point = df.shape[1]/2\n",
        "\n",
        "  agg=0\n",
        "  loop_cnt=0\n",
        "  init_column_cnt = 5\n",
        "  while agg<=total_num_of_data:\n",
        "    loop_cnt+=1\n",
        "    # Obtain current last columns stop\n",
        "    agg=agg_num+init_column_cnt\n",
        "    # Select working columns\n",
        "    new_df = df[df.columns[init_column_cnt:agg]]\n",
        "\n",
        "    init_column_cnt += (agg_num)\n",
        "\n",
        "    headers = list(new_df.columns.values) \n",
        "    if loop_cnt <= mid_point:\n",
        "      # Get average of freq values for new header\n",
        "      new_header_name  = headers[-1]\n",
        "    else:\n",
        "      new_header_name = headers[0]\n",
        "    \n",
        "    # Add aggragated values to new df\n",
        "    agg_df[new_header_name] = new_df.mean(axis = 1)\n",
        "  return agg_df"
      ],
      "metadata": {
        "id": "PqkcfV-CSDYv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find cutoff index"
      ],
      "metadata": {
        "id": "W4h81smRSKUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_tail(df, cut_val, sample_id):\n",
        "  columns_selected = []\n",
        "  old_val = 0\n",
        "  delta = 0\n",
        "  mid_point = int(df.shape[1]/2)\n",
        "  cols_headers  = list(df.columns.values)\n",
        "\n",
        "  for i in range(0, df.shape[1]):\n",
        "    if i == 0:\n",
        "      cell_val = df.iloc[[0], i]\n",
        "      old_val = cell_val[sample_id]\n",
        "    else:\n",
        "      cell_val = df.iloc[[0], i]\n",
        "      val = cell_val[sample_id]\n",
        "      delta = abs(old_val-val)\n",
        "      old_val = val\n",
        "      if delta > cut_val:\n",
        "        if i <= mid_point:\n",
        "          col_name_selected = cols_headers[i+2]\n",
        "        else:\n",
        "          col_name_selected = cols_headers[i-2]\n",
        "          col_name_selected = round(float(col_name_selected), 5)\n",
        "        columns_selected.append(col_name_selected)\n",
        "\n",
        "  return columns_selected"
      ],
      "metadata": {
        "id": "hgUHEniRSJil"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing tails conducting multiples loops with variable cutoff value until we get only two columns\n",
        "def remove_tail_main(df, cut_val, sample_id):\n",
        "  old_drop_cols = []\n",
        "  drop_cols = remove_tail(df, cut_val, sample_id)\n",
        "  while len(drop_cols) != 2:  \n",
        "    # If len of columns to be dropped are 0 the select the 2 values smaller and bigger of the previous iteration\n",
        "    if len(drop_cols) == 0 or len(drop_cols) == 1:\n",
        "      drop_cols = [np.min(old_drop_cols), np.max(old_drop_cols)]\n",
        "      break\n",
        "    else:\n",
        "      old_drop_cols = drop_cols\n",
        "      drop_cols = remove_tail(df, cut_val, sample_id)\n",
        "      cut_val += 2\n",
        "  return drop_cols"
      ],
      "metadata": {
        "id": "CNPLl420STJV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_working_data_idx_v2(df_mean_sample, sample_id, cut_val = 0):\n",
        "\n",
        "  df = df_mean_sample.iloc[[sample_id - 1]]\n",
        "\n",
        "  #agg_df = aggreagator_v2(df) disable aggragator causing odd data points\n",
        "\n",
        "  col_vals = remove_tail_main(df, cut_val=cut_val, sample_id=sample_id)\n",
        "\n",
        "  left_index_no = df.columns.get_loc(col_vals[0])\n",
        "  rigth_index_no = df.columns.get_loc(col_vals[1])\n",
        "\n",
        "  return left_index_no, rigth_index_no"
      ],
      "metadata": {
        "id": "WFhRhx5USZR7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_off_idx(df): # return tuple with cut-off values (index_left_side, index_rigth_side)\n",
        "  df_mean_sample = df.groupby(['PBRS_id']).mean()\n",
        "  df_mean_sample = df_mean_sample.drop(['row', 'Channels', 'Distance_km', 'power_dBm'], axis = 1)\n",
        "\n",
        "  cut_points = {}\n",
        "  print('Calculating the cutoff values:')\n",
        "  for sample in tqdm(df_mean_sample.index):\n",
        "    left_index_no, rigth_index_no = get_working_data_idx_v2(df_mean_sample, sample_id = sample)\n",
        "    cut_points[sample] = (left_index_no, rigth_index_no)\n",
        "\n",
        "  left_cut_off = np.min([v[0] for k, v in cut_points.items()])\n",
        "  rigth_cut_off = np.max([v[1] for k, v in cut_points.items()])\n",
        "\n",
        "  cut_point = (left_cut_off, rigth_cut_off)\n",
        "  return cut_point"
      ],
      "metadata": {
        "id": "1tG_j9gRSeJ0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select working data"
      ],
      "metadata": {
        "id": "_2p4y95YSlGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_working_data(df, cut_point): # implement cut_off values and Smoothing original df after cutt-of\n",
        "  extra_info_df = df.iloc[:, 0:6]\n",
        "  data_df = df.iloc[:, cut_point[0]+6:cut_point[1]+6]\n",
        "  smoothed_data_frame = pd.DataFrame(savgol_filter(data_df, window_length = 5, polyorder = 2))\n",
        "  \n",
        "  # Adding back headers to the smoothed data\n",
        "  rename_col = {}\n",
        "  cnt = 0\n",
        "  for col in data_df.columns:\n",
        "    h_col = round(float(col), 5)\n",
        "    rename_col[cnt] = h_col\n",
        "    cnt += 1\n",
        "  smoothed_data_frame.rename(columns=rename_col, inplace=True)\n",
        "\n",
        "  working_df = pd.merge(extra_info_df, smoothed_data_frame, left_index=True, right_index=True)\n",
        "  return working_df"
      ],
      "metadata": {
        "id": "TC0PK42wSnTw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_cutoff(input_path):\n",
        "  print('Reading data from file.')\n",
        "  df = pd.read_excel(input_path, sheet_name = \"Sheet1\", skiprows=1)\n",
        "  df.iloc[0].fillna(method='bfill', inplace=True)\n",
        "  df.iloc[1:df.shape[0]].fillna(method='pad', inplace=True)\n",
        "  # Adding missing PBRS_id\n",
        "  snippet = []\n",
        "  val_cnt = 1\n",
        "  idx = 0\n",
        "  for sample in range(1, 101):\n",
        "    for sample_id in range(0, 25):\n",
        "      df.at[idx, 'PBRS_id'] = val_cnt\n",
        "      idx +=1\n",
        "      snippet.append(val_cnt)\n",
        "    val_cnt += 1\n",
        "  df['PBRS_id'] = df['PBRS_id'].astype(int)\n",
        "\n",
        "  cut_point = cut_off_idx(df)\n",
        "  working_df = select_working_data(df, cut_point)\n",
        "\n",
        "  return working_df"
      ],
      "metadata": {
        "id": "n5gp_DYpSqEV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run main cutoff"
      ],
      "metadata": {
        "id": "_IszMVtETKej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input_path = path + \"/Spectrum/16QAM_v2/Copy of dataSet_gamma0_Spectrum31MHz_Samples_16QAM_75GHz_LongHaul_input_25x81km_primeStep.xlsx\"\n",
        "#orking_df = main_cutoff(input_path)\n",
        "#display(working_df)"
      ],
      "metadata": {
        "id": "0MCaO3l2TOEZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#working_df.to_csv(path+\"/Spectrum/NN/new_data_working_df.csv\", index=False)"
      ],
      "metadata": {
        "id": "OTlGF2TaTiAT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#New Train/Test Split"
      ],
      "metadata": {
        "id": "rIpeu9O5teEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Reference data"
      ],
      "metadata": {
        "id": "bgUl5jOx2vOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reference_and_sample_data(working_df, num_reference):\n",
        "  import random\n",
        "  num_of_ids = len(working_df['PBRS_id'].unique())\n",
        "\n",
        "  l = list(range(1, num_of_ids+1))\n",
        "  random.shuffle(l)\n",
        "  reference_arr = (l[:num_reference])\n",
        "  sample_arr = l[-(num_of_ids-num_reference):]\n",
        "\n",
        "  reference_data_df = working_df[working_df['PBRS_id'].isin(reference_arr)]\n",
        "  sample_data_df = working_df[working_df['PBRS_id'].isin(sample_arr)]\n",
        "  \n",
        "  return reference_data_df, sample_data_df"
      ],
      "metadata": {
        "id": "ec3YhJwn9Hxx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get test/train split"
      ],
      "metadata": {
        "id": "gaVlwtnN-Ezo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_idx_train_test_split(working_df, trainingProp = 0.8):\n",
        "  rows_training = np.array([])\n",
        "  rows_testing = np.array([])\n",
        "  distances = [x*80 for x in range(1, 26)]\n",
        "  for d in distances:\n",
        "    distance_working_df = working_df.loc[working_df['Distance_km'] == d]\n",
        "    rows_mixed=np.random.permutation(distance_working_df.shape[0])\n",
        "\n",
        "    training_amt = math.ceil(distance_working_df.shape[0]*trainingProp)\n",
        "    testing_amt = distance_working_df.shape[0] - training_amt\n",
        "\n",
        "    rows_training = np.append(rows_training, rows_mixed[:training_amt])\n",
        "    rows_testing = np.append(rows_testing, rows_mixed[-testing_amt:])\n",
        "  \n",
        "  rows_training = rows_training.astype('int').tolist()\n",
        "  rows_testing = rows_testing.astype('int').tolist()\n",
        "\n",
        "  return rows_training, rows_testing"
      ],
      "metadata": {
        "id": "yKHiNNANyyDB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get frequencies filter"
      ],
      "metadata": {
        "id": "WEbwMwMG-9BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frequencies_filter(df, num_selected_freq=20):\n",
        "  info_df = df.iloc[:, 0:6]\n",
        "  data_df = df.iloc[:, 6:df.shape[1]]\n",
        "  span_val = int(data_df.shape[1]/num_selected_freq)\n",
        "  for i in range(num_selected_freq):\n",
        "    df2 = data_df.iloc[:, (i+1)*span_val].to_frame()\n",
        "    if i+1 == 1:\n",
        "      new_df = df2\n",
        "    else:\n",
        "      new_df = pd.merge(new_df, df2, left_index=True, right_index=True)\n",
        "  new_df = info_df.join(new_df)\n",
        "  \n",
        "  return new_df"
      ],
      "metadata": {
        "id": "RqeELlLC_vlH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get features"
      ],
      "metadata": {
        "id": "S2Z2xg2yHuoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample_freq_data_df\n",
        "#distance_reference_df\n",
        "#selected_rows = rows_training\n",
        "def get_features(sample_freq_data_df, reference_freq_data_df, selected_rows, test_references_params):\n",
        "  data_distance = []\n",
        "  data_mean = []\n",
        "  data_std = []\n",
        "  data_pearson_min = []\n",
        "  data_pearson_mean = []\n",
        "  data_pearson_max = []\n",
        "  data_CI_lower = []\n",
        "  data_CI_upper = []\n",
        "\n",
        "  if test_references_params[0] == True: # Test reference distance portion\n",
        "    reference_freq_data_df = reference_freq_data_df['Distance_km'].isin(test_references_params[1])\n",
        "\n",
        "  for row_idx in tqdm(selected_rows):\n",
        "    reference_Y = reference_freq_data_df.iloc[row_idx,3] # distance of row selected\n",
        "\n",
        "    data_Y = sample_freq_data_df.iloc[row_idx,3] # distance of row selected\n",
        "    data_X = np.array(sample_freq_data_df.iloc[row_idx,6:sample_freq_data_df.shape[1]]) #array of data in row selected\n",
        "    data_distance.append(data_Y)\n",
        "    data_std.append(np.std(data_X))\n",
        "    data_mean.append(np.mean(data_X))\n",
        "\n",
        "    data_ci = st.t.interval(alpha=0.90, df=len(data_X)-1, loc=np.mean(data_X), scale=st.sem(data_X))\n",
        "    data_confidence_int_lower = data_ci[0]\n",
        "    data_confidence_int_upper = data_ci[1]\n",
        "\n",
        "    # Get reference data -----------------\n",
        "    distance_reference_df = reference_freq_data_df.loc[reference_freq_data_df['Distance_km'] == data_Y]\n",
        "    if distance_reference_df.empty:\n",
        "      distance_reference_df = reference_freq_data_df.copy()\n",
        "\n",
        "    pearson_vals = []\n",
        "    confidence_vals_lower = []\n",
        "    confidence_vals_upper = []\n",
        "    for reference_row in range(0, distance_reference_df.shape[0]):\n",
        "      reference_X = np.array(distance_reference_df.iloc[reference_row,6:distance_reference_df.shape[1]])\n",
        "      pearson_vals.append(round(pearsonr(data_X, reference_X)[0], 5))\n",
        "\n",
        "      reference_ci = st.t.interval(alpha=0.90, df=len(reference_X)-1, loc=np.mean(reference_X), scale=st.sem(reference_X))\n",
        "      reference_confidence_int_lower = reference_ci[0]\n",
        "      confidence_vals_lower.append(round(abs(reference_confidence_int_lower - data_confidence_int_lower), 5))\n",
        "      reference_confidence_int_upper = reference_ci[1]\n",
        "      confidence_vals_upper.append(round(abs(reference_confidence_int_upper - data_confidence_int_upper),5))\n",
        "\n",
        "    data_pearson_min.append(np.min(pearson_vals))\n",
        "    data_pearson_mean.append(np.mean(pearson_vals))    \n",
        "    data_pearson_max.append(np.max(pearson_vals))\n",
        "\n",
        "    data_CI_lower.append(np.min(confidence_vals_lower))\n",
        "    data_CI_upper.append(np.min(confidence_vals_upper))\n",
        "\n",
        "  data = {'distance':data_distance,\n",
        "                  'mean':data_mean,\n",
        "                  'std':data_std,\n",
        "                  'pearson_min':data_pearson_min,\n",
        "                  'pearson_mean':data_pearson_mean,\n",
        "                  'pearson_max':data_pearson_max,\n",
        "                  'delta_CI_min':data_CI_lower,\n",
        "                  'delta_CI_max':data_CI_upper}\n",
        "\n",
        "  data_df = pd.DataFrame(data)\n",
        "\n",
        "  \n",
        "  return data_df"
      ],
      "metadata": {
        "id": "8FNsUKTVxZmP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main train/test split"
      ],
      "metadata": {
        "id": "9V5WNuN9jSZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test_split_main(input_path, num_of_freq_selected, reference_amt, test_references_params):\n",
        "  working_df = pd.read_csv(input_path)\n",
        "  working_df['Distance_km'] = working_df['Distance_km'].astype(int)\n",
        "  #working_df = working_df.sample(frac = 1) # Shuffle df\n",
        "\n",
        "  reference_data_df, sample_data_df = get_reference_and_sample_data(working_df, reference_amt)\n",
        "\n",
        "  reference_freq_data_df = frequencies_filter(reference_data_df, num_selected_freq=num_of_freq_selected)\n",
        "  sample_freq_data_df = frequencies_filter(sample_data_df, num_selected_freq=num_of_freq_selected)\n",
        "\n",
        "  rows_training, rows_testing = get_idx_train_test_split(sample_data_df, trainingProp = 0.8)\n",
        "\n",
        "  print('Calculating train split:')\n",
        "  training_df = get_features(sample_freq_data_df, reference_freq_data_df, rows_training, test_references_params)\n",
        "  print('Calculating test split:')\n",
        "  testing_df = get_features(sample_freq_data_df, reference_freq_data_df, rows_testing, test_references_params)\n",
        "\n",
        "  return training_df, testing_df, reference_freq_data_df, sample_freq_data_df, rows_training, rows_testing"
      ],
      "metadata": {
        "id": "uL35uf_RjV3r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bootstrapping"
      ],
      "metadata": {
        "id": "S_ysZa0NcV0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get max and min values of each column"
      ],
      "metadata": {
        "id": "WcOO339iclrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_min_val_col(working_df):\n",
        "  distances = [x*80 for x in range(1, 26)]\n",
        "  data_working_df = working_df.iloc[:, 6:working_df.shape[1]]\n",
        "  h_list = list(data_working_df.columns.values)\n",
        "  max_min_col_dict = {header:[] for header in h_list}\n",
        "\n",
        "  for d in tqdm(distances):\n",
        "    boot_dist_df = working_df.loc[working_df['Distance_km'] == d]\n",
        "    for idx in range(6, boot_dist_df.shape[1]):\n",
        "      max_val_col = float(boot_dist_df.iloc[:, [idx]].max()) # max val in col\n",
        "      min_val_col = float(boot_dist_df.iloc[:, [idx]].min()) # min val in col\n",
        "      h = boot_dist_df.iloc[:, [idx]].columns # header of col\n",
        "      h = h[0]\n",
        "      max_min_col_dict[h].append((d, min_val_col, max_val_col))\n",
        "  return max_min_col_dict"
      ],
      "metadata": {
        "id": "aekr6Y0OcZ3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create new samples using bootstrap technique"
      ],
      "metadata": {
        "id": "_0nAH77NcoA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bootstrap_samples(working_df, max_min_col_dict, num_of_new_samples, previos_amt_of_samples=13):\n",
        "  import random\n",
        "\n",
        "  row_data = []\n",
        "  PBRS_id_data = []\n",
        "  Distance_km_data = []\n",
        "  power_dBm_data = []\n",
        "  span_data = []\n",
        "\n",
        "  h_list = list(working_df.columns.values)\n",
        "  new_data = {header:[] for header in h_list}\n",
        "\n",
        "  last_row_in_old_df_val = working_df.shape[0]\n",
        "  for new_sample in tqdm(range(num_of_new_samples)): # generating (num_of_new_samples) new samples for en data\n",
        "    for d in distances:\n",
        "      new_data['row'].append(last_row_in_old_df_val)\n",
        "      last_row_in_old_df_val += 1\n",
        "      new_data['PBRS_id'].append(new_sample+(previos_amt_of_samples+1))\n",
        "      new_data['Channels'].append(1)\n",
        "\n",
        "      new_data['Distance_km'].append(d)\n",
        "      new_data['power_dBm'].append(0)\n",
        "      new_data['#span'].append(int(d/80))\n",
        "\n",
        "      for k, v in max_min_col_dict.items(): # header:(d, min_val_col, max_val_col)\n",
        "        for val in v:\n",
        "          if val[0] == d:\n",
        "            bootstrap_val = round(random.uniform(val[1], val[2]), 14)\n",
        "            new_data[str(k)].append(bootstrap_val)\n",
        "            break\n",
        "  new_data_df = pd.DataFrame.from_dict(new_data)\n",
        "\n",
        "  return new_data_df"
      ],
      "metadata": {
        "id": "reVgqwbfckwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine previos df with bootstraped df"
      ],
      "metadata": {
        "id": "Yqct6pgmctpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = path + \"/Spectrum/NN/new_data_working_df.csv\"\n",
        "working_df = pd.read_csv(input_path)\n",
        "working_df['Distance_km'] = working_df['Distance_km'].astype(int)\n",
        "\n",
        "max_min_col_dict = get_max_min_val_col(working_df)\n",
        "new_data_df = create_bootstrap_samples(working_df, max_min_col_dict, num_of_new_samples=400, previos_amt_of_samples=100)\n",
        "new_working_df = working_df.append(new_data_df)\n",
        "\n",
        "new_working_df.to_csv(path + \"/Spectrum/NN/bootstraped_new_data_working_df.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1mZ4Wb0cvLv",
        "outputId": "19fb3e51-3723-41f7-eb46-e7d871a2c485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [01:52<00:00,  4.49s/it]\n",
            "100%|██████████| 400/400 [01:48<00:00,  3.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test new data"
      ],
      "metadata": {
        "id": "-4aouQ6qQRor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hope better"
      ],
      "metadata": {
        "id": "8Ou1UMjPtiWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split_features(training_df, testing_df):\n",
        "  Y = training_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "  X = training_df.iloc[:, 1:(training_df.shape[1])]\n",
        "\n",
        "  sc_input = MinMaxScaler()\n",
        "  sc_output = MinMaxScaler()\n",
        "  Y_train = sc_output.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "  X_train = sc_input.fit_transform(X) # convert features to values from 0 to 1\n",
        "\n",
        "  Y = testing_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "  X = testing_df.iloc[:, 1:(testing_df.shape[1])]\n",
        "\n",
        "  Y_test = sc_output.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "  X_test = sc_input.fit_transform(X) # convert features to values from 0 to 1\n",
        "\n",
        "  joblib.dump(sc_input, path+'/Spectrum/NN/train_test_splits/mod_scaler_input_30_10.joblib')\n",
        "  joblib.dump(sc_output, path+'/Spectrum/NN/train_test_splits/mod_scaler_output_30_10.joblib')\n",
        "\n",
        "  return Y_train, X_train, Y_test, X_test, sc_input, sc_output"
      ],
      "metadata": {
        "id": "Y5i05MQSFkjW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_data_splits(input_path, num_of_freq_selected, reference_amt, split_done=False, test_references_params=(False, [80, 1040, 2000])):\n",
        "\n",
        "  if split_done == False:\n",
        "    training_df, testing_df, reference_freq_data_df, sample_freq_data_df, rows_training, rows_testing = get_train_test_split_main(input_path, num_of_freq_selected, reference_amt, test_references_params)\n",
        "\n",
        "    training_df.to_csv(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_new_data_training_data_NN_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\", header=True, index=False)\n",
        "    testing_df.to_csv(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_new_data_testing_data_NN_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\", header=True, index=False)\n",
        "\n",
        "    reference_freq_data_df.to_csv(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_new_data_Reference_NN_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\", header=True, index=False)\n",
        "    sample_freq_data_df.to_csv(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_new_data_Sample_NN_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\", header=True, index=False)\n",
        "\n",
        "    pickle.dump(rows_training, open(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_traininig_Rows_NN_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".pkl\",\"wb\"))\n",
        "    pickle.dump(rows_testing, open(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_testing_Rows_NN_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".pkl\",\"wb\"))\n",
        "  else:\n",
        "    training_df = pd.read_csv(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_new_data_training_data_NN_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\")\n",
        "    testing_df = pd.read_csv(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_new_data_testing_data_NN_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\")\n",
        "    \n",
        "    reference_freq_data_df = pd.read_csv(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_new_data_Reference_NN_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\")\n",
        "    sample_freq_data_df = pd.read_csv(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_new_data_Sample_NN_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\")\n",
        "    with open(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_traininig_Rows_NN_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".pkl\", 'rb') as f:\n",
        "      rows_training = pickle.load(f)\n",
        "    with open(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_testing_Rows_NN_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".pkl\", 'rb') as f:\n",
        "      rows_testing = pickle.load(f)  \n",
        "  \n",
        "  Y_train, X_train, Y_test, X_test, sc_input, sc_output = train_test_split_features(training_df, testing_df)\n",
        "\n",
        "  return Y_train, X_train, Y_test, X_test, training_df, testing_df, reference_freq_data_df, sample_freq_data_df, rows_training, rows_testing, sc_input, sc_output"
      ],
      "metadata": {
        "id": "vuhTggzTfTlX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_test(model_ann, Y_test, X_test, sc_output_c, testing_df, remove_outliers=0.05):\n",
        "  print('Results (freq_'+str(num_of_freq_selected)+'_reference_'+str(reference_amt)+')')\n",
        "  # Predit\n",
        "  start_time = time.time()\n",
        "  Y_test_pred=model_ann.predict(X_test)\n",
        "  pred=list(Y_test_pred)\n",
        "\n",
        "  print('Pred:')\n",
        "  print(pred)\n",
        "  \n",
        "  time_eval_ann=time.time()-start_time\n",
        "\n",
        "  dist_min=sc_output_c.data_min_[0] # 80\n",
        "  dist_max=sc_output_c.data_max_[0] # 2000\n",
        "\n",
        "  real=list(list(zip(*Y_test))[0])\n",
        "  real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "  real_abs=[int(np.round(i)) for i in real_abs]\n",
        "  pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "  print('Real_abs:')\n",
        "  print(real_abs)\n",
        "  print('Pred_abs:')\n",
        "  print(pred_abs)\n",
        "\n",
        "  real_abs_e1 = real_abs\n",
        "  real_abs_e2 = real_abs\n",
        "\n",
        "  pred_abs_e1 = pred_abs\n",
        "  pred_abs_e2 = pred_abs\n",
        "\n",
        "  error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "  error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "\n",
        "  remove_outliers = math.ceil((len(error_v2)*remove_outliers)/2)\n",
        "\n",
        "  for outlier in range(1, remove_outliers+1):\n",
        "    min_idx_e1 = np.argmin(error)\n",
        "    max_idx_e1 = np.argmax(error)\n",
        "\n",
        "    min_idx_e2 = np.argmin(error_v2)\n",
        "    max_idx_e2 = np.argmax(error_v2)\n",
        "\n",
        "    error = np.delete(error, min_idx_e1)\n",
        "    error = np.delete(error, max_idx_e1)\n",
        "    real_abs_e1 = np.delete(real_abs_e1, min_idx_e1)\n",
        "    real_abs_e1 = np.delete(real_abs_e1, max_idx_e1)\n",
        "    pred_abs_e1 = np.delete(pred_abs_e1, min_idx_e1)\n",
        "    pred_abs_e1 = np.delete(pred_abs_e1, max_idx_e1)\n",
        "\n",
        "    error_v2 = np.delete(error_v2, min_idx_e2)\n",
        "    error_v2 = np.delete(error_v2, max_idx_e2)\n",
        "    real_abs_e2 = np.delete(real_abs_e2, min_idx_e1)\n",
        "    real_abs_e2 = np.delete(real_abs_e2, max_idx_e1)\n",
        "    pred_abs_e2 = np.delete(pred_abs_e2, min_idx_e1)\n",
        "    pred_abs_e2 = np.delete(pred_abs_e2, max_idx_e1)\n",
        "\n",
        "  res=pd.DataFrame({\"dist\":real_abs_e1,\"pred\":pred_abs_e1,\"error\":error})\n",
        "  res.to_csv(path+\"/Spectrum/NN/NN_results_error/mod_bootstraped_new_data_results_curve_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\", header=True, index=False)\n",
        "\n",
        "  res_v2=pd.DataFrame({\"dist\":real_abs_e2,\"pred\":pred_abs_e2,\"error\":error_v2})\n",
        "  res.to_csv(path+\"/Spectrum/NN/NN_results_error/mod_bootstraped_new_data_results_deltas_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\", header=True, index=False)\n",
        "  \n",
        "  plt.plot(real_abs_e1,error,'bo')\n",
        "  plt.xlabel('Distances') \n",
        "  plt.ylabel('Error') \n",
        "  plt.title(\"Results Curve (freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\")\")\n",
        "  plt.savefig(path+\"/Spectrum/NN/NN_results_plots/mod_bootstraped_new_data_results_curve_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".png\")\n",
        "  plt.show()\n",
        "  \n",
        "  plt.plot(real_abs_e2,error_v2,'bo')\n",
        "  plt.xlabel('Distances') \n",
        "  plt.ylabel('Error') \n",
        "  plt.title(\"Results Deltas (freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\")\")\n",
        "  plt.savefig(path+\"/Spectrum/NN/NN_results_plots/bootstraped_new_data_results_deltas_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".png\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "VM7Qp086iJzS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, Y_train):\n",
        "  print('Training model (freq_'+str(num_of_freq_selected)+'_reference_'+str(reference_amt)+')...')\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense,Conv2D, Flatten, Dropout\n",
        "  from keras.wrappers.scikit_learn import KerasRegressor\n",
        "  from keras.callbacks import EarlyStopping\n",
        "  from tqdm.keras import TqdmCallback\n",
        "\n",
        "  def ann():\n",
        "      model = Sequential()\n",
        "      model.add(Dense(76, input_dim = 7, activation = 'tanh'))\n",
        "      model.add(Dense(38,activation='tanh'))      \n",
        "      model.add(Dense(19,activation='tanh'))\n",
        "      model.add(Dense(19,activation='tanh'))\n",
        "      model.add(Dense(9,activation='tanh'))\n",
        "      model.add(Dense(1))\n",
        "      model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "      return model\n",
        "\n",
        "  model_ann = KerasRegressor(build_fn=ann,epochs=8000,batch_size=32, verbose=0)\n",
        "\n",
        "  start_time = time.time()\n",
        "  callback = [EarlyStopping(monitor='loss', patience=500)]#, TqdmCallback(verbose=2)]\n",
        "  model_ann.fit(X_train, Y_train, callbacks=callback)\n",
        "  time_train_ann = time.time() - start_time\n",
        "\n",
        "  model_ann.model.save(path+\"/Spectrum/NN/NNs/mod_bootstraped_final_new_data_NN_model_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".h5\")\n",
        "  model_ann.model.save_weights(path+\"/Spectrum/NN/NNs/mod_bootstraped_final_new_data_NN_model_Weights_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".h5\", overwrite=True)\n",
        "  \n",
        "  return model_ann"
      ],
      "metadata": {
        "id": "eg-RpJrnfDCC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_freq_selected = 10\n",
        "reference_amt = 30\n",
        "input_path = path + \"/Spectrum/NN/bootstraped_new_data_working_df.csv\"\n",
        "\n",
        "Y_train_c, X_train_c, Y_test_c, X_test_c, training_df_c, testing_df_c, reference_freq_data_df_c, sample_freq_data_df_c, rows_training_c, rows_testing_c, sc_input_c, sc_output_c = get_label_data_splits(input_path, num_of_freq_selected, reference_amt, split_done=False)"
      ],
      "metadata": {
        "id": "HO4JJwuiQTCe",
        "outputId": "9d838f24-429b-4982-89e2-c3498081ee5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating train split:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9400/9400 [05:39<00:00, 27.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating test split:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2350/2350 [01:25<00:00, 27.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ann_c = train_model(X_train_c, Y_train_c)\n",
        "'''\n",
        "# Load network\n",
        "from keras.models import load_model\n",
        "model_ann_c = KerasRegressor(build_fn=ann, epochs=8000, batch_size=32, verbose=0)\n",
        "model_ann_c.model = load_model(path+\"/Spectrum/NN/NNs/bootstraped_final_new_data_NN_model_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".h5\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XynTocCnzhi",
        "outputId": "16856d47-0a4a-4ef3-f8ef-60173b267f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model (freq_10_reference_30)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict_test(model_ann_c, Y_test_c, X_test_c, sc_output_c)\n",
        "predict_test(model_ann_c, Y_test_c, X_test_c, sc_output_c, testing_df_c, remove_outliers=0.05)"
      ],
      "metadata": {
        "id": "sNy39pL-mxsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Short-cut"
      ],
      "metadata": {
        "id": "2Kx_wOsmPX4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reference_and_sample_data(working_df, num_reference):\n",
        "  import random\n",
        "  num_of_ids = len(working_df['PBRS_id'].unique())\n",
        "\n",
        "  l = list(range(1, num_of_ids+1))\n",
        "  random.shuffle(l)\n",
        "  reference_arr = (l[:num_reference])\n",
        "  sample_arr = l[-(num_of_ids-num_reference):]\n",
        "\n",
        "  reference_data_df = working_df[working_df['PBRS_id'].isin(reference_arr)]\n",
        "  sample_data_df = working_df[working_df['PBRS_id'].isin(sample_arr)]\n",
        "  \n",
        "  return reference_data_df, sample_data_df\n",
        "\n",
        "def frequencies_filter(df, num_selected_freq=20):\n",
        "  info_df = df.iloc[:, 0:6]\n",
        "  data_df = df.iloc[:, 6:df.shape[1]]\n",
        "  span_val = int(data_df.shape[1]/num_selected_freq)\n",
        "  for i in range(num_selected_freq):\n",
        "    df2 = data_df.iloc[:, (i+1)*span_val].to_frame()\n",
        "    if i+1 == 1:\n",
        "      new_df = df2\n",
        "    else:\n",
        "      new_df = pd.merge(new_df, df2, left_index=True, right_index=True)\n",
        "  new_df = info_df.join(new_df)\n",
        "  \n",
        "  return new_df\n",
        "\n",
        "def get_idx_train_test_split(working_df, trainingProp = 0.8):\n",
        "  rows_training = np.array([])\n",
        "  rows_testing = np.array([])\n",
        "  distances = [x*80 for x in range(1, 26)]\n",
        "  for d in distances:\n",
        "    distance_working_df = working_df.loc[working_df['Distance_km'] == d]\n",
        "    rows_mixed=np.random.permutation(distance_working_df.shape[0])\n",
        "\n",
        "    training_amt = math.ceil(distance_working_df.shape[0]*trainingProp)\n",
        "    testing_amt = distance_working_df.shape[0] - training_amt\n",
        "\n",
        "    rows_training = np.append(rows_training, rows_mixed[:training_amt])\n",
        "    rows_testing = np.append(rows_testing, rows_mixed[-testing_amt:])\n",
        "  \n",
        "  rows_training = rows_training.astype('int').tolist()\n",
        "  rows_testing = rows_testing.astype('int').tolist()\n",
        "\n",
        "  return rows_training, rows_testing\n",
        "\n",
        "# Load network\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tqdm.keras import TqdmCallback\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(76, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(38,activation='tanh'))\n",
        "    model.add(Dense(38,activation='tanh'))\n",
        "    model.add(Dense(19,activation='tanh'))\n",
        "    model.add(Dense(19,activation='tanh'))\n",
        "    model.add(Dense(9,activation='tanh'))\n",
        "    model.add(Dense(9,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "def get_feature(data_split_df, reference_freq_data_df, reference_distance_eval_params=(False, 80)):\n",
        "  reference_eval = reference_freq_data_df.copy()\n",
        "  if reference_distance_eval_params[0] == True:\n",
        "    reference_eval = reference_freq_data_df.loc[reference_freq_data_df['Distance_km'] == reference_distance_eval_params[1]]\n",
        "\n",
        "  data_distance = []\n",
        "  data_mean = []\n",
        "  data_std = []\n",
        "  data_pearson_min = []\n",
        "  data_pearson_mean = []\n",
        "  data_pearson_max = []\n",
        "  data_CI_lower = []\n",
        "  data_CI_upper = []\n",
        "\n",
        "  for idx in tqdm(range(len(data_split_df))):\n",
        "    Y_label = data_split_df.iloc[idx, 3]\n",
        "    X_label = data_split_df.iloc[idx, 6:data_split_df.shape[1]]\n",
        "    data_distance.append(int(Y_label))\n",
        "    data_std.append(round(np.std(X_label), 5))\n",
        "    data_mean.append(round(np.mean(X_label), 5))\n",
        "    \n",
        "    data_ci = st.t.interval(alpha=0.90, df=len(X_label)-1, loc=np.mean(X_label), scale=st.sem(X_label))\n",
        "    data_confidence_int_lower = data_ci[0]\n",
        "    data_confidence_int_upper = data_ci[1]\n",
        "\n",
        "    pearson_vals = []\n",
        "    ci_lower_delta = []\n",
        "    ci_upper_delta = []\n",
        "    for reference_idx in range(len(reference_eval)):\n",
        "      X_reference = reference_eval.iloc[reference_idx, 6:reference_eval.shape[1]]\n",
        "      pearson_vals.append(round(pearsonr(X_label, X_reference)[0], 5))\n",
        "\n",
        "      reference_ci = st.t.interval(alpha=0.90, df=len(X_reference)-1, loc=np.mean(X_reference), scale=st.sem(X_reference))\n",
        "      reference_confidence_int_lower = reference_ci[0]\n",
        "      reference_confidence_int_upper = reference_ci[1]\n",
        "\n",
        "      delta_ci_lower = abs(reference_confidence_int_lower - data_confidence_int_lower)\n",
        "      ci_lower_delta.append(round(delta_ci_lower, 5))\n",
        "      delta_ci_upper = abs(reference_confidence_int_upper - data_confidence_int_upper)\n",
        "      ci_upper_delta.append(round(delta_ci_upper, 5))\n",
        "\n",
        "    data_pearson_min.append(np.min(pearson_vals))\n",
        "    data_pearson_mean.append(np.mean(pearson_vals))    \n",
        "    data_pearson_max.append(np.max(pearson_vals))\n",
        "\n",
        "    data_CI_lower.append(np.min(ci_lower_delta))\n",
        "    data_CI_upper.append(np.min(ci_upper_delta))\n",
        "\n",
        "  data = {'distance':data_distance,\n",
        "                  'mean':data_mean,\n",
        "                  'std':data_std,\n",
        "                  'pearson_min':data_pearson_min,\n",
        "                  'pearson_mean':data_pearson_mean,\n",
        "                  'pearson_max':data_pearson_max,\n",
        "                  'delta_CI_min':data_CI_lower,\n",
        "                  'delta_CI_max':data_CI_upper}\n",
        "\n",
        "  data_df = pd.DataFrame(data)\n",
        "  return data_df"
      ],
      "metadata": {
        "id": "PEUYbMvql5Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_new_data_training_data_NN_freq_10_reference_30.csv\")\n",
        "test_df = pd.read_csv(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_new_data_testing_data_NN_freq_10_reference_30.csv\")\n",
        "\n",
        "reference_freq_data_df = pd.read_csv(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_new_data_Reference_NN_freq_10_reference_30.csv\")\n",
        "sample_freq_data_df = pd.read_csv(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_new_data_Sample_NN_freq_10_reference_30.csv\")\n",
        "\n",
        "with open(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_traininig_Rows_NN_freq_10_reference_30.pkl\", 'rb') as f:\n",
        "  rows_training = pickle.load(f)\n",
        "with open(path+\"/Spectrum/NN/train_test_splits/mod_bootstraped_testing_Rows_NN_freq_10_reference_30.pkl\", 'rb') as f:\n",
        "  rows_testing = pickle.load(f) "
      ],
      "metadata": {
        "id": "-s_khYUdNgcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_freq_selected = 10\n",
        "reference_amt = 30\n",
        "\n",
        "# Load network\n",
        "from keras.models import load_model\n",
        "mm = KerasRegressor(build_fn=ann, epochs=8000, batch_size=32, verbose=0)\n",
        "mm.model = load_model(path+\"/Spectrum/NN/NNs/mod_bootstraped_final_new_data_NN_model_freq_10_reference_30.h5\")\n",
        "\n",
        "res = {}\n",
        "reference_test = [80, 1040, 2000]\n",
        "distances = [x*80 for x in range(1, 26)]\n",
        "\n",
        "#Y = train_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "#X = train_df.iloc[:, 1:train_df.shape[1]-2]\n",
        "\n",
        "#sc_input = MinMaxScaler()\n",
        "#sc_output = MinMaxScaler()\n",
        "#Y_train = sc_output.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "#X_train = sc_input.fit_transform(X) # convert features to values from 0 to 1\n",
        "\n",
        "for r in reference_test:\n",
        "  test_split = sample_freq_data_df.iloc[rows_testing]\n",
        "  test_df = get_feature(test_split, reference_freq_data_df, reference_distance_eval_params=(True, r))\n",
        "  \n",
        "  dist_min=sc_output_c.data_min_[0] # 80\n",
        "  dist_max=sc_output_c.data_max_[0] # 2000\n",
        "\n",
        "  Y = test_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "  X = test_df.iloc[:, 1:test_df.shape[1]]\n",
        "\n",
        "  Y_test = sc_output_c.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "  X_test = sc_input_c.fit_transform(X) # convert features to values from 0 to 1\n",
        "\n",
        "  # Prediction\n",
        "  Y_test_pred=mm.predict(X_test)\n",
        "  pred=list(Y_test_pred)\n",
        "\n",
        "  real=list(list(zip(*Y_test))[0])\n",
        "  real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "  real_abs=[int(np.round(i)) for i in real_abs]\n",
        "  pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "  real_split ={}\n",
        "  for d in distances:\n",
        "    matches = [i for i in range(0,len(real_abs)) if real_abs[i]==d]\n",
        "    real_split[d] = matches\n",
        "  \n",
        "  pred_res = []\n",
        "  pred_m = []\n",
        "  for d_real, matches_idx in real_split.items():\n",
        "    pred_m = [pred_abs[i] for i in matches_idx]\n",
        "    Q1 = np.percentile(pred_m, 25,interpolation = 'midpoint')\n",
        "    Q3 = np.percentile(pred_m, 75,interpolation = 'midpoint')\n",
        "\n",
        "    IQR = Q3 - Q1\n",
        "    upper = Q3+1.5*IQR\n",
        "    lower = Q1-1.5*IQR\n",
        "    filtered_pred_m = []\n",
        "    for v in pred_m:\n",
        "      if v <= upper and v >= lower:\n",
        "        filtered_pred_m.append(v)\n",
        "    pred_res.append(round(np.mean(filtered_pred_m), 5))\n",
        "  \n",
        "  res[r] = pred_res\n",
        "print('-----------------------------------------------------------------------')\n",
        "print('Resuslts')\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(distances, distances,  linestyle='--')\n",
        "for re in reference_test:\n",
        "  plt.plot(distances, res[re], label='Reference '+str(re))\n",
        "\n",
        "plt.title('Real vs Predicted Distances')\n",
        "plt.xlabel('Real Distances')\n",
        "plt.ylabel('Predicted Distances')\n",
        "plt.xlim(np.min(distances)-25, np.max(distances)+25)\n",
        "plt.ylim(np.min(distances)-25, np.max(distances)+25)\n",
        "plt.yticks(distances)\n",
        "plt.xticks(distances)\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "\n",
        "for re in reference_test:\n",
        "  print(res[re])\n",
        "\n",
        "print('-----------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "UictGjFkIqr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_freq_selected = 10\n",
        "reference_amt = 30\n",
        "\n",
        "# Load network\n",
        "from keras.models import load_model\n",
        "mm = KerasRegressor(build_fn=ann, epochs=8000, batch_size=32, verbose=0)\n",
        "mm.model = load_model(path+\"/Spectrum/NN/NNs/mod_bootstraped_final_new_data_NN_model_freq_10_reference_30.h5\")\n",
        "\n",
        "res = {}\n",
        "reference_test = [x*80 for x in range(1, 26)]\n",
        "distances = [x*80 for x in range(1, 26)]\n",
        "\n",
        "#Y = train_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "#X = train_df.iloc[:, 1:train_df.shape[1]-2]\n",
        "\n",
        "#sc_input = MinMaxScaler()\n",
        "#sc_output = MinMaxScaler()\n",
        "#Y_train = sc_output.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "#X_train = sc_input.fit_transform(X) # convert features to values from 0 to 1\n",
        "\n",
        "sc_input_c, sc_output_c\n",
        "\n",
        "for r in reference_test:\n",
        "  test_split = sample_freq_data_df.iloc[rows_testing]\n",
        "  test_df = get_feature(test_split, reference_freq_data_df, reference_distance_eval_params=(True, r))\n",
        "  \n",
        "  dist_min=sc_output_c.data_min_[0] # 80\n",
        "  dist_max=sc_output_c.data_max_[0] # 2000\n",
        "\n",
        "  Y = test_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "  X = test_df.iloc[:, 1:test_df.shape[1]]\n",
        "\n",
        "  Y_test = sc_output_c.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "  X_test = sc_input_c.fit_transform(X) # convert features to values from 0 to 1\n",
        "\n",
        "  # Prediction\n",
        "  Y_test_pred=mm.predict(X_test)\n",
        "  pred=list(Y_test_pred)\n",
        "\n",
        "  \n",
        "  real=list(list(zip(*Y_test))[0])\n",
        "  real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "  real_abs=[int(np.round(i)) for i in real_abs]\n",
        "  pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "  real_split ={}\n",
        "  for d in distances:\n",
        "    matches = [i for i in range(0,len(real_abs)) if real_abs[i]==d]\n",
        "    real_split[d] = matches\n",
        "  \n",
        "  pred_res = []\n",
        "  pred_m = []\n",
        "  for d_real, matches_idx in real_split.items():\n",
        "    pred_m = [pred_abs[i] for i in matches_idx]\n",
        "    Q1 = np.percentile(pred_m, 25,interpolation = 'midpoint')\n",
        "    Q3 = np.percentile(pred_m, 75,interpolation = 'midpoint')\n",
        "\n",
        "    IQR = Q3 - Q1\n",
        "    upper = Q3+1.5*IQR\n",
        "    lower = Q1-1.5*IQR\n",
        "    filtered_pred_m = []\n",
        "    for v in pred_m:\n",
        "      if v <= upper and v >= lower:\n",
        "        filtered_pred_m.append(v)\n",
        "    pred_res.append(round(np.mean(filtered_pred_m), 5))\n",
        "  \n",
        "  res[r] = pred_res\n",
        "print('-----------------------------------------------------------------------')\n",
        "print('Resuslts')\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(distances, distances,  linestyle='--')\n",
        "for re in reference_test:\n",
        "  plt.plot(distances, res[re], label='Reference '+str(re))\n",
        "\n",
        "plt.title('Real vs Predicted Distances')\n",
        "plt.xlabel('Real Distances')\n",
        "plt.ylabel('Predicted Distances')\n",
        "plt.xlim(np.min(distances)-25, np.max(distances)+25)\n",
        "plt.ylim(np.min(distances)-25, np.max(distances)+25)\n",
        "plt.yticks(distances)\n",
        "plt.xticks(distances)\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "\n",
        "for re in reference_test:\n",
        "  print(res[re])\n",
        "\n",
        "print('-----------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "dNaUdEiTbxOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test only pearson"
      ],
      "metadata": {
        "id": "y6uYJIMQznvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load network\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tqdm.keras import TqdmCallback\n",
        "\n",
        "def ann():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(76, input_dim = 3, activation = 'tanh'))\n",
        "  model.add(Dense(38,activation='tanh'))\n",
        "  model.add(Dense(19,activation='tanh'))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "  return model"
      ],
      "metadata": {
        "id": "yXo1Twr-1eoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, Y_train):\n",
        "  print('Training model (freq_'+str(num_of_freq_selected)+'_reference_'+str(reference_amt)+')...')\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense,Conv2D, Flatten\n",
        "  from keras.wrappers.scikit_learn import KerasRegressor\n",
        "  from keras.callbacks import EarlyStopping\n",
        "  from tqdm.keras import TqdmCallback\n",
        "\n",
        "  def ann():\n",
        "      model = Sequential()\n",
        "      model.add(Dense(76, input_dim = 3, activation = 'tanh'))\n",
        "      model.add(Dense(38,activation='tanh'))\n",
        "      model.add(Dense(19,activation='tanh'))\n",
        "      model.add(Dense(1))\n",
        "      model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "      return model\n",
        "\n",
        "  model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "  start_time = time.time()\n",
        "  callback = [EarlyStopping(monitor='loss', patience=500)]#, TqdmCallback(verbose=2)]\n",
        "  model_ann.fit(X_train, Y_train, callbacks=callback)\n",
        "  time_train_ann = time.time() - start_time\n",
        "\n",
        "  # Save model\n",
        "  #pickle.dump(model_ann,open(path+\"/Spectrum/NN/NNs/NN_model_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".pkl\",\"wb\"))\n",
        "\n",
        "  if os.path.isfile(path+\"/Spectrum/NN/NNs/pearson_new_data_NN_model_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".h5\") is False:\n",
        "    model_ann.model.save(path+\"/Spectrum/NN/NNs/pearson_new_data_NN_model_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".h5\")\n",
        "  \n",
        "  return model_ann"
      ],
      "metadata": {
        "id": "r0YBaQtQ1k99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_test(model_ann, Y_test, X_test, sc_output, remove_outliers=0.05):\n",
        "  print('Results (freq_'+str(num_of_freq_selected)+'_reference_'+str(reference_amt)+')')\n",
        "  # Predit\n",
        "  start_time = time.time()\n",
        "  Y_test_pred=model_ann.predict(X_test)\n",
        "  pred=list(Y_test_pred)\n",
        "\n",
        "  time_eval_ann=time.time()-start_time\n",
        "\n",
        "  dist_min=sc_output.data_min_[0] # 80\n",
        "  dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "  real=list(list(zip(*Y_test))[0])\n",
        "  real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "  real_abs=[int(np.round(i)) for i in real_abs]\n",
        "  pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "  real_abs_e1 = real_abs\n",
        "  real_abs_e2 = real_abs\n",
        "\n",
        "  pred_abs_e1 = pred_abs\n",
        "  pred_abs_e2 = pred_abs\n",
        "\n",
        "  error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "  error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "\n",
        "  remove_outliers = math.ceil((len(error_v2)*remove_outliers)/2)\n",
        "\n",
        "  for outlier in range(1, remove_outliers+1):\n",
        "    min_idx_e1 = np.argmin(error)\n",
        "    max_idx_e1 = np.argmax(error)\n",
        "\n",
        "    min_idx_e2 = np.argmin(error_v2)\n",
        "    max_idx_e2 = np.argmax(error_v2)\n",
        "\n",
        "    error = np.delete(error, min_idx_e1)\n",
        "    error = np.delete(error, max_idx_e1)\n",
        "    real_abs_e1 = np.delete(real_abs_e1, min_idx_e1)\n",
        "    real_abs_e1 = np.delete(real_abs_e1, max_idx_e1)\n",
        "    pred_abs_e1 = np.delete(pred_abs_e1, min_idx_e1)\n",
        "    pred_abs_e1 = np.delete(pred_abs_e1, max_idx_e1)\n",
        "\n",
        "    error_v2 = np.delete(error_v2, min_idx_e2)\n",
        "    error_v2 = np.delete(error_v2, max_idx_e2)\n",
        "    real_abs_e2 = np.delete(real_abs_e2, min_idx_e1)\n",
        "    real_abs_e2 = np.delete(real_abs_e2, max_idx_e1)\n",
        "    pred_abs_e2 = np.delete(pred_abs_e2, min_idx_e1)\n",
        "    pred_abs_e2 = np.delete(pred_abs_e2, max_idx_e1)\n",
        "\n",
        "  res=pd.DataFrame({\"dist\":real_abs_e1,\"pred\":pred_abs_e1,\"error\":error})\n",
        "  res.to_csv(path+\"/Spectrum/NN/NN_results_error/new_data_pearson_results_curve_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\", header=True, index=False)\n",
        "\n",
        "  res_v2=pd.DataFrame({\"dist\":real_abs_e2,\"pred\":pred_abs_e2,\"error\":error_v2})\n",
        "  res.to_csv(path+\"/Spectrum/NN/NN_results_error/new_data_pearson_results_deltas_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\", header=True, index=False)\n",
        "  \n",
        "  plt.plot(real_abs_e1,error,'bo')\n",
        "  plt.xlabel('Distances') \n",
        "  plt.ylabel('Error') \n",
        "  plt.title(\"Results Curve (freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\")\")\n",
        "  plt.savefig(path+\"/Spectrum/NN/NN_results_plots/new_data_pearson_results_curve_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".png\")\n",
        "  plt.show()\n",
        "  \n",
        "  plt.plot(real_abs_e2,error_v2,'bo')\n",
        "  plt.xlabel('Distances') \n",
        "  plt.ylabel('Error') \n",
        "  plt.title(\"Results Deltas (freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\")\")\n",
        "  plt.savefig(path+\"/Spectrum/NN/NN_results_plots/new_data_pearson_results_deltas_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".png\")\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "yY8QHJCJAM3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_freq_selected = 10\n",
        "reference_amt = 30\n",
        "training_input_path = path + \"/Spectrum/NN/train_test_splits/new_data_training_data_NN_freq_10_reference_30.csv\"\n",
        "testing_input_path = path + \"/Spectrum/NN/train_test_splits/new_data_testing_data_NN_freq_10_reference_30.csv\"\n",
        "\n",
        "training_df = pd.read_csv(training_input_path)\n",
        "testing_df = pd.read_csv(testing_input_path)\n",
        "\n",
        "Y = training_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "X = training_df.iloc[:, 3:6]\n",
        "\n",
        "sc_input = MinMaxScaler()\n",
        "sc_output = MinMaxScaler()\n",
        "Y_train_p = sc_output.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "X_train_p = sc_input.fit_transform(X) # convert features to values from 0 to 1\n",
        "\n",
        "Y = testing_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "X = testing_df.iloc[:, 3:6]\n",
        "\n",
        "sc_input = MinMaxScaler()\n",
        "sc_output = MinMaxScaler()\n",
        "Y_test_p = sc_output.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "X_test_p = sc_input.fit_transform(X) # convert features to values from 0 to 1"
      ],
      "metadata": {
        "id": "xWm7Dnj1zqiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ann_p = train_model(X_train_p, Y_train_p)"
      ],
      "metadata": {
        "id": "Job0tVyN0gP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_test(model_ann_p, Y_test_p, X_test_p, sc_output)"
      ],
      "metadata": {
        "id": "B493cZEb0ldO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test only stats"
      ],
      "metadata": {
        "id": "-BFRLXf7zqqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load network\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tqdm.keras import TqdmCallback\n",
        "\n",
        "def ann():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(76, input_dim = 3, activation = 'tanh'))\n",
        "  model.add(Dense(38,activation='tanh'))\n",
        "  model.add(Dense(19,activation='tanh'))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "  return model"
      ],
      "metadata": {
        "id": "5WU3J-u56cTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load network\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tqdm.keras import TqdmCallback\n",
        "\n",
        "def ann():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(76, input_dim = 4, activation = 'tanh'))\n",
        "  model.add(Dense(38,activation='tanh'))\n",
        "  model.add(Dense(19,activation='tanh'))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "  return model"
      ],
      "metadata": {
        "id": "7TGNlh7V5zsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, Y_train):\n",
        "  print('Training model (freq_'+str(num_of_freq_selected)+'_reference_'+str(reference_amt)+')...')\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense,Conv2D, Flatten\n",
        "  from keras.wrappers.scikit_learn import KerasRegressor\n",
        "  from keras.callbacks import EarlyStopping\n",
        "  from tqdm.keras import TqdmCallback\n",
        "\n",
        "  def ann():\n",
        "      model = Sequential()\n",
        "      model.add(Dense(76, input_dim = 4, activation = 'tanh'))\n",
        "      model.add(Dense(38,activation='tanh'))\n",
        "      model.add(Dense(19,activation='tanh'))\n",
        "      model.add(Dense(1))\n",
        "      model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "      return model\n",
        "\n",
        "  model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "  start_time = time.time()\n",
        "  callback = [EarlyStopping(monitor='loss', patience=500)]#, TqdmCallback(verbose=2)]\n",
        "  model_ann.fit(X_train, Y_train, callbacks=callback)\n",
        "  time_train_ann = time.time() - start_time\n",
        "\n",
        "  # Save model\n",
        "  #pickle.dump(model_ann,open(path+\"/Spectrum/NN/NNs/NN_model_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".pkl\",\"wb\"))\n",
        "\n",
        "  if os.path.isfile(path+\"/Spectrum/NN/NNs/stats_new_data_NN_model_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".h5\") is False:\n",
        "    model_ann.model.save(path+\"/Spectrum/NN/NNs/stats_new_data_NN_model_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".h5\")\n",
        "  \n",
        "  return model_ann"
      ],
      "metadata": {
        "id": "Lm2iyDF15uvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_test(model_ann, Y_test, X_test, sc_output, remove_outliers=0.05):\n",
        "  print('Results (freq_'+str(num_of_freq_selected)+'_reference_'+str(reference_amt)+')')\n",
        "  # Predit\n",
        "  start_time = time.time()\n",
        "  Y_test_pred=model_ann.predict(X_test)\n",
        "  pred=list(Y_test_pred)\n",
        "\n",
        "  time_eval_ann=time.time()-start_time\n",
        "\n",
        "  dist_min=sc_output.data_min_[0] # 80\n",
        "  dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "  real=list(list(zip(*Y_test))[0])\n",
        "  real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "  real_abs=[int(np.round(i)) for i in real_abs]\n",
        "  pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "  real_abs_e1 = real_abs\n",
        "  real_abs_e2 = real_abs\n",
        "\n",
        "  pred_abs_e1 = pred_abs\n",
        "  pred_abs_e2 = pred_abs\n",
        "\n",
        "  error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "  error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "\n",
        "  remove_outliers = math.ceil((len(error_v2)*remove_outliers)/2)\n",
        "\n",
        "  for outlier in range(1, remove_outliers+1):\n",
        "    min_idx_e1 = np.argmin(error)\n",
        "    max_idx_e1 = np.argmax(error)\n",
        "\n",
        "    min_idx_e2 = np.argmin(error_v2)\n",
        "    max_idx_e2 = np.argmax(error_v2)\n",
        "\n",
        "    error = np.delete(error, min_idx_e1)\n",
        "    error = np.delete(error, max_idx_e1)\n",
        "    real_abs_e1 = np.delete(real_abs_e1, min_idx_e1)\n",
        "    real_abs_e1 = np.delete(real_abs_e1, max_idx_e1)\n",
        "    pred_abs_e1 = np.delete(pred_abs_e1, min_idx_e1)\n",
        "    pred_abs_e1 = np.delete(pred_abs_e1, max_idx_e1)\n",
        "\n",
        "    error_v2 = np.delete(error_v2, min_idx_e2)\n",
        "    error_v2 = np.delete(error_v2, max_idx_e2)\n",
        "    real_abs_e2 = np.delete(real_abs_e2, min_idx_e1)\n",
        "    real_abs_e2 = np.delete(real_abs_e2, max_idx_e1)\n",
        "    pred_abs_e2 = np.delete(pred_abs_e2, min_idx_e1)\n",
        "    pred_abs_e2 = np.delete(pred_abs_e2, max_idx_e1)\n",
        "\n",
        "  res=pd.DataFrame({\"dist\":real_abs_e1,\"pred\":pred_abs_e1,\"error\":error})\n",
        "  res.to_csv(path+\"/Spectrum/NN/NN_results_error/new_data_stats_results_curve_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\", header=True, index=False)\n",
        "\n",
        "  res_v2=pd.DataFrame({\"dist\":real_abs_e2,\"pred\":pred_abs_e2,\"error\":error_v2})\n",
        "  res.to_csv(path+\"/Spectrum/NN/NN_results_error/new_data_stats_results_deltas_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".csv\", header=True, index=False)\n",
        "  \n",
        "  plt.plot(real_abs_e1,error,'bo')\n",
        "  plt.xlabel('Distances') \n",
        "  plt.ylabel('Error') \n",
        "  plt.title(\"Results Curve (freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\")\")\n",
        "  plt.savefig(path+\"/Spectrum/NN/NN_results_plots/new_data_stats_results_curve_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".png\")\n",
        "  plt.show()\n",
        "  \n",
        "  plt.plot(real_abs_e2,error_v2,'bo')\n",
        "  plt.xlabel('Distances') \n",
        "  plt.ylabel('Error') \n",
        "  plt.title(\"Results Deltas (freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\")\")\n",
        "  plt.savefig(path+\"/Spectrum/NN/NN_results_plots/new_data_stats_results_deltas_freq_\"+str(num_of_freq_selected)+\"_reference_\"+str(reference_amt)+\".png\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "zgqD84l5AWXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_freq_selected = 10\n",
        "reference_amt = 30\n",
        "training_input_path = path + \"/Spectrum/NN/train_test_splits/new_data_training_data_NN_freq_10_reference_30.csv\"\n",
        "testing_input_path = path + \"/Spectrum/NN/train_test_splits/new_data_testing_data_NN_freq_10_reference_30.csv\"\n",
        "\n",
        "training_df = pd.read_csv(training_input_path)\n",
        "testing_df = pd.read_csv(testing_input_path)\n",
        "\n",
        "Y = training_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "X = training_df.iloc[:, [1, 2, 5, 6]]\n",
        "\n",
        "sc_input = MinMaxScaler()\n",
        "sc_output = MinMaxScaler()\n",
        "Y_train_s = sc_output.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "X_train_s = sc_input.fit_transform(X) # convert features to values from 0 to 1\n",
        "\n",
        "Y = testing_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "X = testing_df.iloc[:, [1, 2, 5, 6]]\n",
        "\n",
        "\n",
        "sc_input = MinMaxScaler()\n",
        "sc_output = MinMaxScaler()\n",
        "Y_test_s = sc_output.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "X_test_s = sc_input.fit_transform(X) # convert features to values from 0 to 1"
      ],
      "metadata": {
        "id": "w4IOWKxkM0z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ann_s = train_model(X_train_s, Y_train_s)"
      ],
      "metadata": {
        "id": "JvtaIJxm1JiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_test(model_ann_s, Y_test_s, X_test_s, sc_output)"
      ],
      "metadata": {
        "id": "Kf4XiuOw1JsA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}