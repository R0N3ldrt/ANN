{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOn8EMSy8FNJbvIAzAqv+HJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R0N3ldrt/Thesis/blob/main/working_df_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Training NN"
      ],
      "metadata": {
        "id": "4-X0RK1y5lIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing necesary libraries"
      ],
      "metadata": {
        "id": "i75efJ7v52mI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JsQAC6Jj5jZn"
      },
      "outputs": [],
      "source": [
        "# Importing necesary libraries\n",
        "# Libraries for correct code execution \n",
        "\n",
        "import os, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pickle\n",
        "import random\n",
        "import csv\n",
        "import re\n",
        "import array\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "from random import random, gauss\n",
        "from math import modf, pi, cos, sin, sqrt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.stats.stats import pearsonr\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Enviroment"
      ],
      "metadata": {
        "id": "oJ7LiI_k5_Z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE PARA USAR DESDE COLAB\n",
        "\n",
        "# Google drive loading as work station for local-usage of the files.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount= True)\n",
        "\n",
        "#-----------------------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6mTSPkZ541K",
        "outputId": "b20bf5a3-c547-48c8-ca26-dea6268b8fa2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista para cambiar los paths rapido.\n",
        "workers = [\"Ronald\", \"Local\"]\n",
        "\n",
        "# Change the number to change the paths.\n",
        "worker = workers[0]\n",
        "\n",
        "if worker == \"Ronald\":\n",
        "  path = \"/content/gdrive/MyDrive/Thesis_Workstation/ANN_dataset\"\n",
        "else: path = os.getcwd()"
      ],
      "metadata": {
        "id": "MspFTDlu6C-t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Cut tails with un-wanted data"
      ],
      "metadata": {
        "id": "PcC3Atpn6Sqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cutoff utils"
      ],
      "metadata": {
        "id": "gkoAqhH76nf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prime_number_finder(stop_gap, total_num_of_data):\n",
        "  prime_nums = []\n",
        "  i=2\n",
        "  while i <= stop_gap:\n",
        "    if (total_num_of_data % i==0):\n",
        "      prime_nums.append(i)\n",
        "      break\n",
        "    i+=1\n",
        "  return prime_nums"
      ],
      "metadata": {
        "id": "lz8_oqsN6YlY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggreagator_v2(df, stop_gap=50):\n",
        "  total_num_of_data = df.shape[1]\n",
        "  arr_prime = prime_number_finder(stop_gap, total_num_of_data)\n",
        "  #agg_num = np.max(arr_prime)\n",
        "  agg_num = arr_prime[0]\n",
        "\n",
        "  # Creating new df of aggregate values\n",
        "  agg_df = pd.DataFrame()\n",
        "\n",
        "  mid_point = df.shape[1]/2\n",
        "\n",
        "  agg=0\n",
        "  loop_cnt=0\n",
        "  init_column_cnt = 5\n",
        "  while agg<=total_num_of_data:\n",
        "    loop_cnt+=1\n",
        "    # Obtain current last columns stop\n",
        "    agg=agg_num+init_column_cnt\n",
        "    # Select working columns\n",
        "    new_df = df[df.columns[init_column_cnt:agg]]\n",
        "\n",
        "    init_column_cnt += (agg_num)\n",
        "\n",
        "    headers = list(new_df.columns.values) \n",
        "    if loop_cnt <= mid_point:\n",
        "      # Get average of freq values for new header\n",
        "      new_header_name  = headers[-1]\n",
        "    else:\n",
        "      new_header_name = headers[0]\n",
        "    \n",
        "    # Add aggragated values to new df\n",
        "    agg_df[new_header_name] = new_df.mean(axis = 1)\n",
        "  return agg_df"
      ],
      "metadata": {
        "id": "GEgr1I1k6b8i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggreagator_v3(working_df, agg_num):\n",
        "  info_cols = working_df.iloc[:, 0:6]\n",
        "  data_cols = working_df.iloc[:, 6:working_df.shape[1]]\n",
        "\n",
        "  total_num_of_data = working_df.shape[1]\n",
        "\n",
        "  # Creating new df of aggregate values\n",
        "  agg_df = pd.DataFrame()\n",
        "\n",
        "  mid_point = working_df.shape[1]/2\n",
        "\n",
        "  agg=0\n",
        "  loop_cnt=0\n",
        "  init_column_cnt = 5\n",
        "  while agg<=total_num_of_data:\n",
        "    loop_cnt+=1\n",
        "    # Obtain current last columns stop\n",
        "    agg=agg_num+init_column_cnt\n",
        "    # Select working columns\n",
        "    new_df = working_df[working_df.columns[init_column_cnt:agg]]\n",
        "\n",
        "    init_column_cnt += (agg_num)\n",
        "\n",
        "    headers = list(new_df.columns.values) \n",
        "    if loop_cnt <= mid_point:\n",
        "      # Get average of freq values for new header\n",
        "      new_header_name  = headers[-1]\n",
        "    else:\n",
        "      new_header_name = headers[0]\n",
        "    \n",
        "    # Add aggragated values to new df\n",
        "    agg_df[new_header_name] = new_df.mean(axis = 1)\n",
        "\n",
        "    \n",
        "  full_agg_df = pd.merge(info_cols, agg_df, left_index=True, right_index=True)\n",
        "  return full_agg_df"
      ],
      "metadata": {
        "id": "9mlSlbtF6d4V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cualulate cutoff index"
      ],
      "metadata": {
        "id": "w-XUA-F36kmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_tail(df, cut_val, sample_id):\n",
        "  columns_selected = []\n",
        "  old_val = 0\n",
        "  delta = 0\n",
        "  mid_point = int(df.shape[1]/2)\n",
        "  cols_headers  = list(df.columns.values)\n",
        "\n",
        "  for i in range(0, df.shape[1]):\n",
        "    if i == 0:\n",
        "      cell_val = df.iloc[[0], i]\n",
        "      old_val = cell_val[sample_id]\n",
        "    else:\n",
        "      cell_val = df.iloc[[0], i]\n",
        "      val = cell_val[sample_id]\n",
        "      delta = abs(old_val-val)\n",
        "      old_val = val\n",
        "      if delta > cut_val:\n",
        "        if i <= mid_point:\n",
        "          col_name_selected = cols_headers[i+2]\n",
        "        else:\n",
        "          col_name_selected = cols_headers[i-2]\n",
        "          col_name_selected = round(float(col_name_selected), 5)\n",
        "        columns_selected.append(col_name_selected)\n",
        "\n",
        "  return columns_selected"
      ],
      "metadata": {
        "id": "d2-y17vQ6wfp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing tails conducting multiples loops with variable cutoff value until we get only two columns\n",
        "def remove_tail_main(df, cut_val, sample_id):\n",
        "  old_drop_cols = []\n",
        "  drop_cols = remove_tail(df, cut_val, sample_id)\n",
        "  while len(drop_cols) != 2:  \n",
        "    # If len of columns to be dropped are 0 the select the 2 values smaller and bigger of the previous iteration\n",
        "    if len(drop_cols) == 0 or len(drop_cols) == 1:\n",
        "      drop_cols = [np.min(old_drop_cols), np.max(old_drop_cols)]\n",
        "      break\n",
        "    else:\n",
        "      old_drop_cols = drop_cols\n",
        "      drop_cols = remove_tail(df, cut_val, sample_id)\n",
        "      cut_val += 2\n",
        "  return drop_cols"
      ],
      "metadata": {
        "id": "IB6IaXsy6yoJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_working_data_idx_v2(df_mean_sample, sample_id, cut_val = 0):\n",
        "\n",
        "  df = df_mean_sample.iloc[[sample_id - 1]]\n",
        "\n",
        "  agg_df = aggreagator_v2(df)\n",
        "\n",
        "  col_vals = remove_tail_main(agg_df, cut_val=cut_val, sample_id=sample_id)\n",
        "\n",
        "  left_index_no = df.columns.get_loc(col_vals[0])\n",
        "  rigth_index_no = df.columns.get_loc(col_vals[1])\n",
        "\n",
        "  return left_index_no, rigth_index_no"
      ],
      "metadata": {
        "id": "dYRsULST60Xt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cur_off_idx(df): # return tuple with cut-off values (index_left_side, index_rigth_side)\n",
        "  df_mean_sample = df.groupby(['PBRS_id']).mean()\n",
        "  df_mean_sample = df_mean_sample.drop(['row', 'Channels', 'Distance_km', 'power_dBm'], axis = 1)\n",
        "\n",
        "  cut_points = {}\n",
        "  print('Calculating the cutoff values:')\n",
        "  for sample in tqdm(df_mean_sample.index):\n",
        "    left_index_no, rigth_index_no = get_working_data_idx_v2(df_mean_sample, sample_id = sample)\n",
        "    cut_points[sample] = (left_index_no, rigth_index_no)\n",
        "\n",
        "  left_cut_off = np.max([v[0] for k, v in cut_points.items()])\n",
        "  rigth_cut_off = np.min([v[1] for k, v in cut_points.items()])\n",
        "\n",
        "  cut_point = (left_cut_off, rigth_cut_off)\n",
        "  return cut_point"
      ],
      "metadata": {
        "id": "eALdbBmu62mi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select working data"
      ],
      "metadata": {
        "id": "qclU_0Ml7CNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_working_data(df, cut_point): # implement cut_off values and Smoothing original df after cutt-of\n",
        "  extra_info_df = df.iloc[:, 0:6]\n",
        "  data_df = df.iloc[:, cut_point[0]+6:cut_point[1]+6]\n",
        "  smoothed_data_frame = pd.DataFrame(savgol_filter(data_df, window_length = 5, polyorder = 2))\n",
        "  \n",
        "  # Adding back headers to the smoothed data\n",
        "  rename_col = {}\n",
        "  cnt = 0\n",
        "  for col in data_df.columns:\n",
        "    h_col = round(float(col), 5)\n",
        "    rename_col[cnt] = h_col\n",
        "    cnt += 1\n",
        "  smoothed_data_frame.rename(columns=rename_col, inplace=True)\n",
        "\n",
        "  working_df = pd.merge(extra_info_df, smoothed_data_frame, left_index=True, right_index=True)\n",
        "  return working_df"
      ],
      "metadata": {
        "id": "IYnCramn7Gaj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save files with Working df"
      ],
      "metadata": {
        "id": "iqvkw3cB7PIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = path + \"/Spectrum/16QAM/25spans80km_withoutROADMs/dataSet_Spectrum31MHz_Samples_16QAM_75GHz_LongHaul_input_25x80km.xlsx\"\n",
        "\n",
        "output_path = path + \"/Spectrum/CNN/working_df.csv\""
      ],
      "metadata": {
        "id": "hv-lGjaR7IWU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-procesing 1(cleaning original file)\n",
        "df = pd.read_excel(input_path, sheet_name = \"Sheet1\", skiprows=1)\n",
        "\n",
        "#Dropping columns below sample_id 13 since the are empty\n",
        "drop_rows = [drop_row for drop_row in range(325, df.shape[0])]\n",
        "df_13_samples = df.drop(drop_rows)\n",
        "\n",
        "# Adding missing PBRS_id (missing in original file)\n",
        "snippet = []\n",
        "val_cnt = 1\n",
        "idx = 0\n",
        "for sample in range(1, 14):\n",
        "  for sample_id in range(0, 25):\n",
        "    df_13_samples.at[idx, 'PBRS_id'] = val_cnt\n",
        "    idx +=1\n",
        "    snippet.append(val_cnt)\n",
        "  val_cnt += 1\n",
        "df_13_samples['PBRS_id'] = df_13_samples['PBRS_id'].astype(int)\n",
        "\n",
        "# Fill nan values if found\n",
        "df_13_samples.iloc[0].fillna(method='bfill', inplace=True)\n",
        "df_13_samples.iloc[1:df_13_samples.shape[0]].fillna(method='pad', inplace=True)"
      ],
      "metadata": {
        "id": "KuQj6rsu7bsG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-procesing 2 (Removing un wanted data(tails) and implement smoothining)\n",
        "\n",
        "cut_point = cur_off_idx(df_13_samples)\n",
        "working_df = select_working_data(df_13_samples, cut_point)\n",
        "\n",
        "working_df.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC_1D5VZ7gKC",
        "outputId": "359858dd-e816-4709-e5a6-9b4a785ac73b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating the cutoff values:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        }
      ]
    }
  ]
}