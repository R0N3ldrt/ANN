{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOjjSwdS4h/fm1KdUam8ST/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R0N3ldrt/Thesis/blob/main/CNN_boot_v11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traininig NN Spectrum"
      ],
      "metadata": {
        "id": "UT4kgsEJEpyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Necesary Libraries"
      ],
      "metadata": {
        "id": "qsIzw6RGEwWp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0YnA2ltpEoSi"
      },
      "outputs": [],
      "source": [
        "# Importing necesary libraries\n",
        "# Libraries for correct code execution \n",
        "\n",
        "import os, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pickle\n",
        "import random\n",
        "import csv\n",
        "import re\n",
        "import array\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "from functools import reduce\n",
        "from random import random, gauss\n",
        "from math import modf, pi, cos, sin, sqrt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.stats.stats import pearsonr\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "import scipy.stats as st\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import os, time, math, csv, joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "import os, time, math, csv, joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Enviroment"
      ],
      "metadata": {
        "id": "o99IEHDfE2yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE PARA USAR DESDE COLAB\n",
        "\n",
        "# Google drive loading as work station for local-usage of the files.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount= True)\n",
        "\n",
        "#-----------------------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruXEP78XE7E5",
        "outputId": "1b1c8839-0580-4240-f0d9-049a8b545c7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista para cambiar los paths rapido.\n",
        "workers = [\"Ronald\", \"Local\"]\n",
        "\n",
        "# Change the number to change the paths.\n",
        "worker = workers[0]\n",
        "\n",
        "if worker == \"Ronald\":\n",
        "  path = \"/content/gdrive/MyDrive/Thesis_Workstation/ANN_dataset\"\n",
        "else: path = os.getcwd()"
      ],
      "metadata": {
        "id": "1Y2-m9DsE74Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#New Train/Test Split"
      ],
      "metadata": {
        "id": "rIpeu9O5teEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = path + \"/Spectrum/CNN/new_working_df.csv\"\n",
        "\n",
        "working_df = pd.read_csv(input_path)\n",
        "working_df['Distance_km'] = working_df['Distance_km'].astype(int)"
      ],
      "metadata": {
        "id": "KTcWRF8PtiS9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(working_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOUrK-J5ydE2",
        "outputId": "86d1be12-b07c-46cb-db6a-9e5835458e49"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2500, 2073)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "excluding_model_data_df = working_df.loc[working_df['PBRS_id'] != 1]"
      ],
      "metadata": {
        "id": "L-MvMzWr6vno"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Reference data"
      ],
      "metadata": {
        "id": "bgUl5jOx2vOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_data_df = working_df.loc[working_df['PBRS_id'] == 1]\n",
        "model_dict = {}\n",
        "for row in range(model_data_df.shape[0]):\n",
        "  distance = working_df.iloc[row,3]\n",
        "  values = np.array(working_df.iloc[row,6:working_df.shape[1]])\n",
        "  model_dict[distance] = values"
      ],
      "metadata": {
        "id": "jkn74j8u-0Yp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reference_and_sample_data(working_df, num_reference, num_sample):\n",
        "  reference_arr = [x for x in range(1, num_reference+1)]\n",
        "  reference_data_df = working_df[working_df['PBRS_id'].isin(reference_arr)]\n",
        "\n",
        "  sample_arr = [x for x in range(num_reference+1, num_sample+1)]\n",
        "  sample_data_df = working_df[working_df['PBRS_id'].isin(sample_arr)]\n",
        "  \n",
        "  return reference_data_df, sample_data_df"
      ],
      "metadata": {
        "id": "ec3YhJwn9Hxx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_data_df, sample_data_df = get_reference_and_sample_data(working_df, num_reference=50, num_sample=100)"
      ],
      "metadata": {
        "id": "FVf66kD39uvY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get test/train split"
      ],
      "metadata": {
        "id": "gaVlwtnN-Ezo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_idx_train_test_split(working_df, trainingProp = 0.8):\n",
        "  rows_training = np.array([])\n",
        "  rows_testing = np.array([])\n",
        "  distances = [x*80 for x in range(1, 26)]\n",
        "  for d in distances:\n",
        "    distance_working_df = working_df.loc[working_df['Distance_km'] == d]\n",
        "    rows_mixed=np.random.permutation(distance_working_df.shape[0])\n",
        "\n",
        "    training_amt = math.ceil(distance_working_df.shape[0]*trainingProp)\n",
        "    testing_amt = distance_working_df.shape[0] - training_amt\n",
        "\n",
        "    rows_training = np.append(rows_training, rows_mixed[:training_amt])\n",
        "    rows_testing = np.append(rows_testing, rows_mixed[-testing_amt:])\n",
        "  \n",
        "  rows_training = rows_training.astype('int').tolist()\n",
        "  rows_testing = rows_testing.astype('int').tolist()\n",
        "\n",
        "  return rows_training, rows_testing"
      ],
      "metadata": {
        "id": "yKHiNNANyyDB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows_training, rows_testing = get_idx_train_test_split(sample_data_df, trainingProp = 0.8)"
      ],
      "metadata": {
        "id": "YAx8kKMT0rMZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get frequencies filter"
      ],
      "metadata": {
        "id": "WEbwMwMG-9BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frequencies_filter(df, num_selected_freq=20):\n",
        "  info_df = df.iloc[:, 0:6]\n",
        "  data_df = df.iloc[:, 6:df.shape[1]]\n",
        "  span_val = int(data_df.shape[1]/num_selected_freq)\n",
        "  for i in range(num_selected_freq):\n",
        "    df2 = data_df.iloc[:, (i+1)*span_val].to_frame()\n",
        "    if i+1 == 1:\n",
        "      new_df = df2\n",
        "    else:\n",
        "      new_df = pd.merge(new_df, df2, left_index=True, right_index=True)\n",
        "  new_df = info_df.join(new_df)\n",
        "  \n",
        "  return new_df"
      ],
      "metadata": {
        "id": "RqeELlLC_vlH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_freq_data_df = frequencies_filter(reference_data_df, num_selected_freq=20)\n",
        "\n",
        "sample_freq_data_df = frequencies_filter(sample_data_df, num_selected_freq=20)\n",
        "\n",
        "# TEST num_selected_freq values"
      ],
      "metadata": {
        "id": "jxqxR6dwBY6d"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_freq_data_df"
      ],
      "metadata": {
        "id": "8VkCHPRs0lec",
        "outputId": "7b84547f-bba4-433a-ba1b-03b32a1e1f12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       row  PBRS_id  Channels  Distance_km  power_dBm  #span   -28.9375  \\\n",
              "1250  1250       51         1           80          0      1 -11.371108   \n",
              "1251  1251       51         1          160          0      2 -10.211595   \n",
              "1252  1252       51         1          240          0      3 -15.204040   \n",
              "1253  1253       51         1          320          0      4 -13.629725   \n",
              "1254  1254       51         1          400          0      5 -10.817269   \n",
              "...    ...      ...       ...          ...        ...    ...        ...   \n",
              "2495  2495      100         1         1680          0     21 -10.384048   \n",
              "2496  2496      100         1         1760          0     22 -25.989634   \n",
              "2497  2497      100         1         1840          0     23  -6.889217   \n",
              "2498  2498      100         1         1920          0     24 -17.875031   \n",
              "2499  2499      100         1         2000          0     25 -12.975259   \n",
              "\n",
              "      -25.71875      -22.5  -19.28125  ...       3.25    6.46875     9.6875  \\\n",
              "1250  -9.888275 -15.896047 -10.401251  ... -11.985474 -10.041763 -10.323638   \n",
              "1251 -10.649810  -9.031208 -16.839636  ... -12.088256 -11.504632  -7.955859   \n",
              "1252 -19.299046  -9.498365  -4.519786  ... -18.301708 -13.122521  -7.990432   \n",
              "1253 -20.440475 -11.564857 -11.154106  ... -14.078396  -7.761546 -11.642466   \n",
              "1254 -15.656722 -13.972180 -20.629751  ... -18.218828 -15.606157  -8.490232   \n",
              "...         ...        ...        ...  ...        ...        ...        ...   \n",
              "2495 -15.111328 -10.947651 -15.621388  ...  -8.297015 -10.186144  -7.996039   \n",
              "2496  -7.263092 -13.035062 -10.594661  ... -11.335702 -11.592846  -9.906562   \n",
              "2497 -17.560904 -12.411949 -14.249330  ... -11.040326 -13.328168  -8.388157   \n",
              "2498  -8.925251 -11.985465 -14.717908  ... -13.172462 -10.790016 -11.697045   \n",
              "2499 -12.292847 -19.219643 -22.856406  ... -19.238656 -18.467817 -17.271477   \n",
              "\n",
              "       12.90625     16.125   19.34375    22.5625   25.78125       29.0  \\\n",
              "1250 -17.261711 -10.552016 -16.379117 -18.675457 -11.977936 -11.043479   \n",
              "1251 -11.202520  -8.600032 -10.355622  -9.990740  -8.624055 -16.657460   \n",
              "1252  -9.126269 -17.643167 -13.834385 -15.660303  -6.391696  -9.668572   \n",
              "1253 -15.153048 -17.844676 -10.922279  -9.524572 -12.243487 -12.415649   \n",
              "1254 -17.095917  -7.016890 -12.053797 -11.479498  -6.427090 -17.091879   \n",
              "...         ...        ...        ...        ...        ...        ...   \n",
              "2495  -7.510316 -12.258500 -17.332990 -12.781404 -11.458765  -9.875344   \n",
              "2496  -7.139437 -11.066623 -11.801799 -12.502789  -7.710519 -16.097928   \n",
              "2497 -10.133369 -19.487009 -11.853957 -14.058775  -6.946959 -14.632147   \n",
              "2498 -11.293293 -14.034699 -16.059040 -12.485453 -12.692961 -17.194686   \n",
              "2499  -8.429088 -15.266872  -9.899949 -14.370661 -14.245584  -6.555617   \n",
              "\n",
              "       32.21875  \n",
              "1250 -15.544026  \n",
              "1251 -18.454892  \n",
              "1252  -9.910335  \n",
              "1253 -30.553415  \n",
              "1254 -19.309437  \n",
              "...         ...  \n",
              "2495 -22.044932  \n",
              "2496 -19.157402  \n",
              "2497 -21.732752  \n",
              "2498 -22.963780  \n",
              "2499 -22.498299  \n",
              "\n",
              "[1250 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3e0364d-0a7c-4533-b569-4ec8b402b7b6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row</th>\n",
              "      <th>PBRS_id</th>\n",
              "      <th>Channels</th>\n",
              "      <th>Distance_km</th>\n",
              "      <th>power_dBm</th>\n",
              "      <th>#span</th>\n",
              "      <th>-28.9375</th>\n",
              "      <th>-25.71875</th>\n",
              "      <th>-22.5</th>\n",
              "      <th>-19.28125</th>\n",
              "      <th>...</th>\n",
              "      <th>3.25</th>\n",
              "      <th>6.46875</th>\n",
              "      <th>9.6875</th>\n",
              "      <th>12.90625</th>\n",
              "      <th>16.125</th>\n",
              "      <th>19.34375</th>\n",
              "      <th>22.5625</th>\n",
              "      <th>25.78125</th>\n",
              "      <th>29.0</th>\n",
              "      <th>32.21875</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1250</th>\n",
              "      <td>1250</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-11.371108</td>\n",
              "      <td>-9.888275</td>\n",
              "      <td>-15.896047</td>\n",
              "      <td>-10.401251</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.985474</td>\n",
              "      <td>-10.041763</td>\n",
              "      <td>-10.323638</td>\n",
              "      <td>-17.261711</td>\n",
              "      <td>-10.552016</td>\n",
              "      <td>-16.379117</td>\n",
              "      <td>-18.675457</td>\n",
              "      <td>-11.977936</td>\n",
              "      <td>-11.043479</td>\n",
              "      <td>-15.544026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1251</th>\n",
              "      <td>1251</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>160</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-10.211595</td>\n",
              "      <td>-10.649810</td>\n",
              "      <td>-9.031208</td>\n",
              "      <td>-16.839636</td>\n",
              "      <td>...</td>\n",
              "      <td>-12.088256</td>\n",
              "      <td>-11.504632</td>\n",
              "      <td>-7.955859</td>\n",
              "      <td>-11.202520</td>\n",
              "      <td>-8.600032</td>\n",
              "      <td>-10.355622</td>\n",
              "      <td>-9.990740</td>\n",
              "      <td>-8.624055</td>\n",
              "      <td>-16.657460</td>\n",
              "      <td>-18.454892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1252</th>\n",
              "      <td>1252</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>240</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-15.204040</td>\n",
              "      <td>-19.299046</td>\n",
              "      <td>-9.498365</td>\n",
              "      <td>-4.519786</td>\n",
              "      <td>...</td>\n",
              "      <td>-18.301708</td>\n",
              "      <td>-13.122521</td>\n",
              "      <td>-7.990432</td>\n",
              "      <td>-9.126269</td>\n",
              "      <td>-17.643167</td>\n",
              "      <td>-13.834385</td>\n",
              "      <td>-15.660303</td>\n",
              "      <td>-6.391696</td>\n",
              "      <td>-9.668572</td>\n",
              "      <td>-9.910335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1253</th>\n",
              "      <td>1253</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>320</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-13.629725</td>\n",
              "      <td>-20.440475</td>\n",
              "      <td>-11.564857</td>\n",
              "      <td>-11.154106</td>\n",
              "      <td>...</td>\n",
              "      <td>-14.078396</td>\n",
              "      <td>-7.761546</td>\n",
              "      <td>-11.642466</td>\n",
              "      <td>-15.153048</td>\n",
              "      <td>-17.844676</td>\n",
              "      <td>-10.922279</td>\n",
              "      <td>-9.524572</td>\n",
              "      <td>-12.243487</td>\n",
              "      <td>-12.415649</td>\n",
              "      <td>-30.553415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1254</th>\n",
              "      <td>1254</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>-10.817269</td>\n",
              "      <td>-15.656722</td>\n",
              "      <td>-13.972180</td>\n",
              "      <td>-20.629751</td>\n",
              "      <td>...</td>\n",
              "      <td>-18.218828</td>\n",
              "      <td>-15.606157</td>\n",
              "      <td>-8.490232</td>\n",
              "      <td>-17.095917</td>\n",
              "      <td>-7.016890</td>\n",
              "      <td>-12.053797</td>\n",
              "      <td>-11.479498</td>\n",
              "      <td>-6.427090</td>\n",
              "      <td>-17.091879</td>\n",
              "      <td>-19.309437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>2495</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1680</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>-10.384048</td>\n",
              "      <td>-15.111328</td>\n",
              "      <td>-10.947651</td>\n",
              "      <td>-15.621388</td>\n",
              "      <td>...</td>\n",
              "      <td>-8.297015</td>\n",
              "      <td>-10.186144</td>\n",
              "      <td>-7.996039</td>\n",
              "      <td>-7.510316</td>\n",
              "      <td>-12.258500</td>\n",
              "      <td>-17.332990</td>\n",
              "      <td>-12.781404</td>\n",
              "      <td>-11.458765</td>\n",
              "      <td>-9.875344</td>\n",
              "      <td>-22.044932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>2496</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1760</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>-25.989634</td>\n",
              "      <td>-7.263092</td>\n",
              "      <td>-13.035062</td>\n",
              "      <td>-10.594661</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.335702</td>\n",
              "      <td>-11.592846</td>\n",
              "      <td>-9.906562</td>\n",
              "      <td>-7.139437</td>\n",
              "      <td>-11.066623</td>\n",
              "      <td>-11.801799</td>\n",
              "      <td>-12.502789</td>\n",
              "      <td>-7.710519</td>\n",
              "      <td>-16.097928</td>\n",
              "      <td>-19.157402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>2497</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1840</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>-6.889217</td>\n",
              "      <td>-17.560904</td>\n",
              "      <td>-12.411949</td>\n",
              "      <td>-14.249330</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.040326</td>\n",
              "      <td>-13.328168</td>\n",
              "      <td>-8.388157</td>\n",
              "      <td>-10.133369</td>\n",
              "      <td>-19.487009</td>\n",
              "      <td>-11.853957</td>\n",
              "      <td>-14.058775</td>\n",
              "      <td>-6.946959</td>\n",
              "      <td>-14.632147</td>\n",
              "      <td>-21.732752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>2498</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1920</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>-17.875031</td>\n",
              "      <td>-8.925251</td>\n",
              "      <td>-11.985465</td>\n",
              "      <td>-14.717908</td>\n",
              "      <td>...</td>\n",
              "      <td>-13.172462</td>\n",
              "      <td>-10.790016</td>\n",
              "      <td>-11.697045</td>\n",
              "      <td>-11.293293</td>\n",
              "      <td>-14.034699</td>\n",
              "      <td>-16.059040</td>\n",
              "      <td>-12.485453</td>\n",
              "      <td>-12.692961</td>\n",
              "      <td>-17.194686</td>\n",
              "      <td>-22.963780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>2499</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>2000</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>-12.975259</td>\n",
              "      <td>-12.292847</td>\n",
              "      <td>-19.219643</td>\n",
              "      <td>-22.856406</td>\n",
              "      <td>...</td>\n",
              "      <td>-19.238656</td>\n",
              "      <td>-18.467817</td>\n",
              "      <td>-17.271477</td>\n",
              "      <td>-8.429088</td>\n",
              "      <td>-15.266872</td>\n",
              "      <td>-9.899949</td>\n",
              "      <td>-14.370661</td>\n",
              "      <td>-14.245584</td>\n",
              "      <td>-6.555617</td>\n",
              "      <td>-22.498299</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1250 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3e0364d-0a7c-4533-b569-4ec8b402b7b6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a3e0364d-0a7c-4533-b569-4ec8b402b7b6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a3e0364d-0a7c-4533-b569-4ec8b402b7b6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get features"
      ],
      "metadata": {
        "id": "S2Z2xg2yHuoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample_freq_data_df\n",
        "#distance_reference_df\n",
        "#selected_rows = rows_training\n",
        "def get_features(sample_freq_data_df, reference_freq_data_df, selected_rows):\n",
        "  data_distance = []\n",
        "  data_mean = []\n",
        "  data_std = []\n",
        "  data_pearson_min = []\n",
        "  data_pearson_mean = []\n",
        "  data_pearson_max = []\n",
        "  data_CI_lower = []\n",
        "  data_CI_upper = []\n",
        "\n",
        "  for row_idx in tqdm(selected_rows):\n",
        "    data_Y = sample_freq_data_df.iloc[row_idx,3] # distance of row selected\n",
        "    data_X = np.array(sample_freq_data_df.iloc[row_idx,6:sample_freq_data_df.shape[1]]) #array of data in row selected\n",
        "    data_distance.append(data_Y)\n",
        "    data_std.append(np.std(data_X))\n",
        "    data_mean.append(np.mean(data_X))\n",
        "\n",
        "    data_ci = st.t.interval(alpha=0.90, df=len(data_X)-1, loc=np.mean(data_X), scale=st.sem(data_X))\n",
        "    data_confidence_int_lower = data_ci[0]\n",
        "    data_confidence_int_upper = data_ci[1]\n",
        "\n",
        "    # Get reference data\n",
        "    reference_freq_data_df\n",
        "    distance_reference_df = reference_freq_data_df.loc[reference_freq_data_df['Distance_km'] == data_Y]\n",
        "\n",
        "    pearson_vals = []\n",
        "    confidence_vals_upper = []\n",
        "    confidence_vals_lower = []\n",
        "    for reference_row in range(0, distance_reference_df.shape[0]):\n",
        "      reference_X = np.array(distance_reference_df.iloc[reference_row,6:distance_reference_df.shape[1]])\n",
        "      pearson_vals.append(round(pearsonr(data_X, reference_X)[0], 5))\n",
        "\n",
        "      reference_ci = st.t.interval(alpha=0.90, df=len(reference_X)-1, loc=np.mean(reference_X), scale=st.sem(reference_X))\n",
        "      reference_confidence_int_lower = reference_ci[0]\n",
        "      confidence_vals_lower.append(reference_confidence_int_lower)\n",
        "      reference_confidence_int_upper = reference_ci[1]\n",
        "      confidence_vals_upper.append(reference_confidence_int_upper)\n",
        "\n",
        "    data_pearson_min.append(np.min(pearson_vals))\n",
        "    data_pearson_mean.append(np.mean(pearson_vals))    \n",
        "    data_pearson_max.append(np.max(pearson_vals))\n",
        "\n",
        "    data_CI_lower.append(abs(np.min(confidence_vals_lower)-data_confidence_int_lower))\n",
        "    data_CI_upper.append(abs(np.max(confidence_vals_upper)-data_confidence_int_upper))\n",
        "  data = {'distance':data_distance,\n",
        "                  'mean':data_mean,\n",
        "                  'std':data_std,\n",
        "                  'pearson_min':data_pearson_min,\n",
        "                  'pearson_mean':data_pearson_mean,\n",
        "                  'pearson_max':data_pearson_max,\n",
        "                  'delta_CI_min':data_CI_lower,\n",
        "                  'delta_CI_max':data_CI_upper}\n",
        "\n",
        "  data_df = pd.DataFrame(data)\n",
        "\n",
        "  return data_df"
      ],
      "metadata": {
        "id": "8FNsUKTVxZmP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df = get_features(sample_freq_data_df, reference_freq_data_df, rows_training)\n",
        "\n",
        "testing_df = get_features(sample_freq_data_df, reference_freq_data_df, rows_testing)"
      ],
      "metadata": {
        "id": "guPIPhGRAlb3",
        "outputId": "c5cff2c9-a0f1-4d39-bd21-693535c7366f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:56<00:00, 17.79it/s]\n",
            "100%|██████████| 250/250 [00:13<00:00, 18.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def get_features(excluding_model_data_df, selected_rows):\n",
        "\n",
        "  data_distance = []\n",
        "  data_mean = []\n",
        "  data_std = []\n",
        "  data_pearson_min = []\n",
        "  data_pearson_mean = []\n",
        "  data_pearson_max = []\n",
        "\n",
        "  for row_idx in tqdm(selected_rows):\n",
        "    data_Y = excluding_model_data_df.iloc[row_idx,3]\n",
        "    data_X = np.array(excluding_model_data_df.iloc[row_idx,6:excluding_model_data_df.shape[1]])\n",
        "    data_distance.append(data_Y)\n",
        "    data_std.append(np.std(data_X))\n",
        "    data_mean.append(np.mean(data_X))\n",
        "\n",
        "    pearson_vals = []\n",
        "    for k, v in model_dict.items():  \n",
        "      pearson_vals.append(round(pearsonr(v, data_X)[0], 5))\n",
        "      print(v)\n",
        "      print(data_X)\n",
        "      break\n",
        "    data_pearson_min.append(np.min(pearson_vals))\n",
        "    data_pearson_mean.append(np.mean(pearson_vals))    \n",
        "    data_pearson_max.append(np.max(pearson_vals))\n",
        "    break\n",
        "  data = {'distance':data_distance,\n",
        "                  'mean':data_mean,\n",
        "                  'std':data_std,\n",
        "                  'pearson_min':data_pearson_min,\n",
        "                  'pearson_mean':data_pearson_mean,\n",
        "                  'pearson_max':data_pearson_max}\n",
        "\n",
        "  data_df = pd.DataFrame(data)\n",
        "\n",
        "  return data_df\n",
        "training_df = get_features(excluding_model_data_df, rows_training)\n",
        "\n",
        "testing_df = get_features(excluding_model_data_df, rows_testing)\n",
        "'''"
      ],
      "metadata": {
        "id": "FnshPiRq0wxU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bootstrapping"
      ],
      "metadata": {
        "id": "qksszb65u-2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = path + \"/Spectrum/CNN/working_df.csv\"\n",
        "\n",
        "working_df = pd.read_csv(input_path)\n",
        "working_df['Distance_km'] = working_df['Distance_km'].astype(int)"
      ],
      "metadata": {
        "id": "cEYZP7YSvA4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distances = [x*80 for x in range(1, 26)]"
      ],
      "metadata": {
        "id": "vpEDJN-hvodC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get max and min values of each column"
      ],
      "metadata": {
        "id": "qbD-Ts__HVqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_min_val_col(working_df):\n",
        "  data_working_df = working_df.iloc[:, 6:working_df.shape[1]]\n",
        "  h_list = list(data_working_df.columns.values)\n",
        "  max_min_col_dict = {header:[] for header in h_list}\n",
        "\n",
        "  for d in tqdm(distances):\n",
        "    boot_dist_df = working_df.loc[working_df['Distance_km'] == d]\n",
        "    for idx in range(6, boot_dist_df.shape[1]):\n",
        "      max_val_col = float(boot_dist_df.iloc[:, [idx]].max()) # max val in col\n",
        "      min_val_col = float(boot_dist_df.iloc[:, [idx]].min()) # min val in col\n",
        "      h = boot_dist_df.iloc[:, [idx]].columns # header of col\n",
        "      h = h[0]\n",
        "      max_min_col_dict[h].append((d, min_val_col, max_val_col))\n",
        "  return max_min_col_dict"
      ],
      "metadata": {
        "id": "pfmHMEtv-fA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_min_col_dict = get_max_min_val_col(working_df)"
      ],
      "metadata": {
        "id": "vhWI9GaP_k_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_min_col_dict)"
      ],
      "metadata": {
        "id": "Tu_r9lHJCGIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create new samples using bootstrap technique"
      ],
      "metadata": {
        "id": "MUBCgrizHc1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bootstrap_samples(working_df, max_min_col_dict, num_of_new_samples, previos_amt_of_samples=13):\n",
        "  import random\n",
        "\n",
        "  row_data = []\n",
        "  PBRS_id_data = []\n",
        "  Distance_km_data = []\n",
        "  power_dBm_data = []\n",
        "  span_data = []\n",
        "\n",
        "  h_list = list(working_df.columns.values)\n",
        "  new_data = {header:[] for header in h_list}\n",
        "\n",
        "\n",
        "  last_row_in_old_df_val = working_df.shape[0]\n",
        "  for new_sample in tqdm(range(num_of_new_samples)): # generating (num_of_new_samples) new samples for en data\n",
        "    for d in distances:\n",
        "      new_data['row'].append(last_row_in_old_df_val)\n",
        "      last_row_in_old_df_val += 1\n",
        "      new_data['PBRS_id'].append(new_sample+(previos_amt_of_samples+1))\n",
        "      new_data['Channels'].append(1)\n",
        "\n",
        "      new_data['Distance_km'].append(d)\n",
        "      new_data['power_dBm'].append(0)\n",
        "      new_data['#span'].append(int(d/80))\n",
        "\n",
        "      for k, v in max_min_col_dict.items(): # header:(d, min_val_col, max_val_col)\n",
        "        for val in v:\n",
        "          if val[0] == d:\n",
        "            bootstrap_val = round(random.uniform(val[1], val[2]), 14)\n",
        "            new_data[str(k)].append(bootstrap_val)\n",
        "            break\n",
        "  new_data_df = pd.DataFrame.from_dict(new_data)\n",
        "\n",
        "  return new_data_df"
      ],
      "metadata": {
        "id": "96AJljiWv2Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_df = create_bootstrap_samples(working_df, max_min_col_dict, num_of_new_samples=87, previos_amt_of_samples=13)"
      ],
      "metadata": {
        "id": "zN5Uwep1HHvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine previos df with bootstraped df"
      ],
      "metadata": {
        "id": "VtwPz_xDHj1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_working_df = working_df.append(new_data_df)"
      ],
      "metadata": {
        "id": "P6amWSh7HRUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_working_df.to_csv(path+'/Spectrum/CNN/new_working_df.csv', index=False)"
      ],
      "metadata": {
        "id": "nfuCRKffItYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "CGa0J5JcS-BW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing"
      ],
      "metadata": {
        "id": "eydWoJNWTEGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training_df=pd.read_csv(path+'/Spectrum/CNN/training_data_DNN.csv')\n",
        "\n",
        "Y = training_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "X = training_df.iloc[:, 1:training_df.shape[1]]\n",
        "\n",
        "sc_input = MinMaxScaler()\n",
        "sc_output = MinMaxScaler()\n",
        "Y_train = sc_output.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "X_train = sc_input.fit_transform(X) # convert features to values from 0 to 1"
      ],
      "metadata": {
        "id": "ZEGP4I1lTDZy"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing_df=pd.read_csv(path+'/Spectrum/CNN/testing_data_DNN.csv')\n",
        "\n",
        "Y = testing_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "X = testing_df.iloc[:, 1:testing_df.shape[1]]\n",
        "\n",
        "\n",
        "sc_input = MinMaxScaler()\n",
        "sc_output = MinMaxScaler()\n",
        "Y_test = sc_output.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "X_test = sc_input.fit_transform(X) # convert features to values from 0 to 1"
      ],
      "metadata": {
        "id": "zFzruEAeW8gH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "pdv_LIJAV6YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(76, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(38,activation='tanh'))\n",
        "    model.add(Dense(19,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time"
      ],
      "metadata": {
        "id": "E3baz5KzV4mb",
        "outputId": "06f6f385-1b14-4bb0-8b1a-9cf9f9235766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5000\n",
            "32/32 [==============================] - 1s 2ms/step - loss: 0.1169\n",
            "Epoch 2/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0970\n",
            "Epoch 3/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0920\n",
            "Epoch 4/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0917\n",
            "Epoch 5/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0874\n",
            "Epoch 6/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0861\n",
            "Epoch 7/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0836\n",
            "Epoch 8/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0791\n",
            "Epoch 9/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0752\n",
            "Epoch 10/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0704\n",
            "Epoch 11/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0683\n",
            "Epoch 12/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0649\n",
            "Epoch 13/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0620\n",
            "Epoch 14/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0592\n",
            "Epoch 15/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0563\n",
            "Epoch 16/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0549\n",
            "Epoch 17/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0517\n",
            "Epoch 18/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0504\n",
            "Epoch 19/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0480\n",
            "Epoch 20/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0468\n",
            "Epoch 21/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0437\n",
            "Epoch 22/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0412\n",
            "Epoch 23/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0402\n",
            "Epoch 24/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0376\n",
            "Epoch 25/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0349\n",
            "Epoch 26/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0342\n",
            "Epoch 27/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0317\n",
            "Epoch 28/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0296\n",
            "Epoch 29/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0285\n",
            "Epoch 30/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0266\n",
            "Epoch 31/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0247\n",
            "Epoch 32/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0251\n",
            "Epoch 33/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0240\n",
            "Epoch 34/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0230\n",
            "Epoch 35/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0219\n",
            "Epoch 36/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0205\n",
            "Epoch 37/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0192\n",
            "Epoch 38/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0181\n",
            "Epoch 39/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0168\n",
            "Epoch 40/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0154\n",
            "Epoch 41/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0141\n",
            "Epoch 42/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 43/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0123\n",
            "Epoch 44/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0106\n",
            "Epoch 45/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0105\n",
            "Epoch 46/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0093\n",
            "Epoch 47/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0094\n",
            "Epoch 48/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0079\n",
            "Epoch 49/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0075\n",
            "Epoch 50/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0070\n",
            "Epoch 51/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 52/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0060\n",
            "Epoch 53/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0054\n",
            "Epoch 54/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0051\n",
            "Epoch 55/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0046\n",
            "Epoch 56/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0044\n",
            "Epoch 57/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0041\n",
            "Epoch 58/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0040\n",
            "Epoch 59/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0043\n",
            "Epoch 60/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 61/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0035\n",
            "Epoch 62/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 63/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 64/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Epoch 65/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0031\n",
            "Epoch 66/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 67/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Epoch 68/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0028\n",
            "Epoch 69/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 70/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 71/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0028\n",
            "Epoch 72/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 73/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 74/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Epoch 75/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 76/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 77/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 78/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 79/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 80/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 81/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 82/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 83/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 84/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 85/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 86/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 87/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 88/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 89/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 90/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 91/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 92/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 93/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 94/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 95/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 96/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 97/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 98/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 99/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 100/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 101/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 102/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 103/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 104/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 105/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 106/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 107/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 108/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 109/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 110/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 111/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 112/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 113/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 114/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 115/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 116/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 117/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 118/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 119/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 120/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 121/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 122/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 123/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 124/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 125/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 126/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 127/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 128/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 129/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 130/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 131/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 132/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 133/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 134/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 135/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 136/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 137/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 138/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 139/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 140/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 141/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0010\n",
            "Epoch 142/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 143/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 144/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 145/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 146/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 147/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 148/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 149/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.2343e-04\n",
            "Epoch 150/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 151/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 152/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0010\n",
            "Epoch 153/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 9.5089e-04\n",
            "Epoch 154/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 155/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 156/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 9.8710e-04\n",
            "Epoch 157/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.7236e-04\n",
            "Epoch 158/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 159/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 160/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0010\n",
            "Epoch 161/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.8909e-04\n",
            "Epoch 162/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 163/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 164/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 165/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 166/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.6315e-04\n",
            "Epoch 167/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 168/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.8007e-04\n",
            "Epoch 169/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 170/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.3972e-04\n",
            "Epoch 171/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.7718e-04\n",
            "Epoch 172/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.9849e-04\n",
            "Epoch 173/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.4731e-04\n",
            "Epoch 174/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.6866e-04\n",
            "Epoch 175/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.8740e-04\n",
            "Epoch 176/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 177/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.9520e-04\n",
            "Epoch 178/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 179/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.2740e-04\n",
            "Epoch 180/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.7682e-04\n",
            "Epoch 181/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.5017e-04\n",
            "Epoch 182/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.3649e-04\n",
            "Epoch 183/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.2663e-04\n",
            "Epoch 184/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.5430e-04\n",
            "Epoch 185/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 9.4746e-04\n",
            "Epoch 186/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.1657e-04\n",
            "Epoch 187/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 9.7003e-04\n",
            "Epoch 188/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.9212e-04\n",
            "Epoch 189/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.8757e-04\n",
            "Epoch 190/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 9.8475e-04\n",
            "Epoch 191/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.5486e-04\n",
            "Epoch 192/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 8.4898e-04\n",
            "Epoch 193/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 8.3899e-04\n",
            "Epoch 194/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.7531e-04\n",
            "Epoch 195/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.7526e-04\n",
            "Epoch 196/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 9.4722e-04\n",
            "Epoch 197/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 9.7819e-04\n",
            "Epoch 198/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.8566e-04\n",
            "Epoch 199/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.4868e-04\n",
            "Epoch 200/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 9.4977e-04\n",
            "Epoch 201/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 9.7028e-04\n",
            "Epoch 202/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.6244e-04\n",
            "Epoch 203/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.7164e-04\n",
            "Epoch 204/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 9.3426e-04\n",
            "Epoch 205/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.1143e-04\n",
            "Epoch 206/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.4884e-04\n",
            "Epoch 207/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 8.0110e-04\n",
            "Epoch 208/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.4827e-04\n",
            "Epoch 209/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.0935e-04\n",
            "Epoch 210/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 9.5626e-04\n",
            "Epoch 211/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.3278e-04\n",
            "Epoch 212/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.1952e-04\n",
            "Epoch 213/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.8088e-04\n",
            "Epoch 214/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.3292e-04\n",
            "Epoch 215/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.0744e-04\n",
            "Epoch 216/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 8.4094e-04\n",
            "Epoch 217/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.9881e-04\n",
            "Epoch 218/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.3314e-04\n",
            "Epoch 219/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.9509e-04\n",
            "Epoch 220/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.4865e-04\n",
            "Epoch 221/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.7030e-04\n",
            "Epoch 222/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.4912e-04\n",
            "Epoch 223/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 7.0043e-04\n",
            "Epoch 224/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.6347e-04\n",
            "Epoch 225/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 8.3842e-04\n",
            "Epoch 226/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.8650e-04\n",
            "Epoch 227/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.7338e-04\n",
            "Epoch 228/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.6410e-04\n",
            "Epoch 229/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.3479e-04\n",
            "Epoch 230/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.4927e-04\n",
            "Epoch 231/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.1686e-04\n",
            "Epoch 232/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.1904e-04\n",
            "Epoch 233/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.4033e-04\n",
            "Epoch 234/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.2518e-04\n",
            "Epoch 235/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 8.2916e-04\n",
            "Epoch 236/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.6822e-04\n",
            "Epoch 237/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.7888e-04\n",
            "Epoch 238/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.4630e-04\n",
            "Epoch 239/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.3903e-04\n",
            "Epoch 240/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 8.1646e-04\n",
            "Epoch 241/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.4941e-04\n",
            "Epoch 242/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.8767e-04\n",
            "Epoch 243/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.9091e-04\n",
            "Epoch 244/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.8647e-04\n",
            "Epoch 245/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.2267e-04\n",
            "Epoch 246/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 8.2061e-04\n",
            "Epoch 247/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.4844e-04\n",
            "Epoch 248/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.5959e-04\n",
            "Epoch 249/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.3344e-04\n",
            "Epoch 250/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.3913e-04\n",
            "Epoch 251/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.8505e-04\n",
            "Epoch 252/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.7141e-04\n",
            "Epoch 253/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 6.0661e-04\n",
            "Epoch 254/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.5232e-04\n",
            "Epoch 255/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 6.8118e-04\n",
            "Epoch 256/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.2019e-04\n",
            "Epoch 257/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.2719e-04\n",
            "Epoch 258/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.1092e-04\n",
            "Epoch 259/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.8635e-04\n",
            "Epoch 260/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.4313e-04\n",
            "Epoch 261/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.7205e-04\n",
            "Epoch 262/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.8429e-04\n",
            "Epoch 263/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 6.1691e-04\n",
            "Epoch 264/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 6.0759e-04\n",
            "Epoch 265/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.7885e-04\n",
            "Epoch 266/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 6.0310e-04\n",
            "Epoch 267/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 6.0883e-04\n",
            "Epoch 268/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 6.5720e-04\n",
            "Epoch 269/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.2843e-04\n",
            "Epoch 270/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.8350e-04\n",
            "Epoch 271/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9331e-04\n",
            "Epoch 272/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 7.3201e-04\n",
            "Epoch 273/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.3162e-04\n",
            "Epoch 274/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.6364e-04\n",
            "Epoch 275/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.4149e-04\n",
            "Epoch 276/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.6176e-04\n",
            "Epoch 277/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.1654e-04\n",
            "Epoch 278/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.5925e-04\n",
            "Epoch 279/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.4277e-04\n",
            "Epoch 280/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.1421e-04\n",
            "Epoch 281/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.6112e-04\n",
            "Epoch 282/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.9617e-04\n",
            "Epoch 283/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.3207e-04\n",
            "Epoch 284/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.1593e-04\n",
            "Epoch 285/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.1333e-04\n",
            "Epoch 286/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.0832e-04\n",
            "Epoch 287/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.2803e-04\n",
            "Epoch 288/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.2316e-04\n",
            "Epoch 289/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.7090e-04\n",
            "Epoch 290/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9780e-04\n",
            "Epoch 291/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.6170e-04\n",
            "Epoch 292/5000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 6.1067e-04\n",
            "Epoch 293/5000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 5.9355e-04\n",
            "Epoch 294/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.0140e-04\n",
            "Epoch 295/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.5509e-04\n",
            "Epoch 296/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.8855e-04\n",
            "Epoch 297/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.3388e-04\n",
            "Epoch 298/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.1168e-04\n",
            "Epoch 299/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.4011e-04\n",
            "Epoch 300/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.4076e-04\n",
            "Epoch 301/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9464e-04\n",
            "Epoch 302/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.8390e-04\n",
            "Epoch 303/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.2908e-04\n",
            "Epoch 304/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 6.4317e-04\n",
            "Epoch 305/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.3418e-04\n",
            "Epoch 306/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.9539e-04\n",
            "Epoch 307/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.5818e-04\n",
            "Epoch 308/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.2945e-04\n",
            "Epoch 309/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.7597e-04\n",
            "Epoch 310/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.3511e-04\n",
            "Epoch 311/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.1852e-04\n",
            "Epoch 312/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.3278e-04\n",
            "Epoch 313/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.2626e-04\n",
            "Epoch 314/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.6334e-04\n",
            "Epoch 315/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.1070e-04\n",
            "Epoch 316/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.9321e-04\n",
            "Epoch 317/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.5465e-04\n",
            "Epoch 318/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.1300e-04\n",
            "Epoch 319/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.5979e-04\n",
            "Epoch 320/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.2788e-04\n",
            "Epoch 321/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.7442e-04\n",
            "Epoch 322/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.1377e-04\n",
            "Epoch 323/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.7497e-04\n",
            "Epoch 324/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.2388e-04\n",
            "Epoch 325/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.4694e-04\n",
            "Epoch 326/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.0978e-04\n",
            "Epoch 327/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.3305e-04\n",
            "Epoch 328/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.4891e-04\n",
            "Epoch 329/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.1238e-04\n",
            "Epoch 330/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.1077e-04\n",
            "Epoch 331/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.2177e-04\n",
            "Epoch 332/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.4132e-04\n",
            "Epoch 333/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.6802e-04\n",
            "Epoch 334/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.5348e-04\n",
            "Epoch 335/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.6190e-04\n",
            "Epoch 336/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.1818e-04\n",
            "Epoch 337/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.6827e-04\n",
            "Epoch 338/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.9227e-04\n",
            "Epoch 339/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.7290e-04\n",
            "Epoch 340/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.8168e-04\n",
            "Epoch 341/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.0008e-04\n",
            "Epoch 342/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.3009e-04\n",
            "Epoch 343/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.6485e-04\n",
            "Epoch 344/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.9815e-04\n",
            "Epoch 345/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.1506e-04\n",
            "Epoch 346/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.2943e-04\n",
            "Epoch 347/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9459e-04\n",
            "Epoch 348/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.4823e-04\n",
            "Epoch 349/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.8675e-04\n",
            "Epoch 350/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.5140e-04\n",
            "Epoch 351/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.6820e-04\n",
            "Epoch 352/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.9935e-04\n",
            "Epoch 353/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.3297e-04\n",
            "Epoch 354/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.7007e-04\n",
            "Epoch 355/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.7117e-04\n",
            "Epoch 356/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.6617e-04\n",
            "Epoch 357/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.0914e-04\n",
            "Epoch 358/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.3246e-04\n",
            "Epoch 359/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.3261e-04\n",
            "Epoch 360/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.0115e-04\n",
            "Epoch 361/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.0727e-04\n",
            "Epoch 362/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.4730e-04\n",
            "Epoch 363/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.7398e-04\n",
            "Epoch 364/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0411e-04\n",
            "Epoch 365/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.7604e-04\n",
            "Epoch 366/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.1819e-04\n",
            "Epoch 367/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.7890e-04\n",
            "Epoch 368/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.7901e-04\n",
            "Epoch 369/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.6537e-04\n",
            "Epoch 370/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.6943e-04\n",
            "Epoch 371/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.5516e-04\n",
            "Epoch 372/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.2169e-04\n",
            "Epoch 373/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1734e-04\n",
            "Epoch 374/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.3277e-04\n",
            "Epoch 375/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8868e-04\n",
            "Epoch 376/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.5232e-04\n",
            "Epoch 377/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.6465e-04\n",
            "Epoch 378/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.6050e-04\n",
            "Epoch 379/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8545e-04\n",
            "Epoch 380/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9979e-04\n",
            "Epoch 381/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.5981e-04\n",
            "Epoch 382/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0814e-04\n",
            "Epoch 383/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8173e-04\n",
            "Epoch 384/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.3418e-04\n",
            "Epoch 385/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0112e-04\n",
            "Epoch 386/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8199e-04\n",
            "Epoch 387/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.7033e-04\n",
            "Epoch 388/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4592e-04\n",
            "Epoch 389/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.2112e-04\n",
            "Epoch 390/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9692e-04\n",
            "Epoch 391/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4705e-04\n",
            "Epoch 392/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9273e-04\n",
            "Epoch 393/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4127e-04\n",
            "Epoch 394/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8350e-04\n",
            "Epoch 395/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.7375e-04\n",
            "Epoch 396/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4364e-04\n",
            "Epoch 397/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9219e-04\n",
            "Epoch 398/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0779e-04\n",
            "Epoch 399/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.3095e-04\n",
            "Epoch 400/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4255e-04\n",
            "Epoch 401/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9817e-04\n",
            "Epoch 402/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.3819e-04\n",
            "Epoch 403/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0866e-04\n",
            "Epoch 404/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2100e-04\n",
            "Epoch 405/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8993e-04\n",
            "Epoch 406/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4685e-04\n",
            "Epoch 407/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.3111e-04\n",
            "Epoch 408/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.5306e-04\n",
            "Epoch 409/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.3251e-04\n",
            "Epoch 410/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8034e-04\n",
            "Epoch 411/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4693e-04\n",
            "Epoch 412/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.1677e-04\n",
            "Epoch 413/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4627e-04\n",
            "Epoch 414/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9132e-04\n",
            "Epoch 415/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8960e-04\n",
            "Epoch 416/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3859e-04\n",
            "Epoch 417/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3777e-04\n",
            "Epoch 418/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2029e-04\n",
            "Epoch 419/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0666e-04\n",
            "Epoch 420/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0241e-04\n",
            "Epoch 421/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3713e-04\n",
            "Epoch 422/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4876e-04\n",
            "Epoch 423/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.2086e-04\n",
            "Epoch 424/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3158e-04\n",
            "Epoch 425/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.7281e-04\n",
            "Epoch 426/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8026e-04\n",
            "Epoch 427/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.6841e-04\n",
            "Epoch 428/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0956e-04\n",
            "Epoch 429/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.7292e-04\n",
            "Epoch 430/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4873e-04\n",
            "Epoch 431/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1159e-04\n",
            "Epoch 432/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.1606e-04\n",
            "Epoch 433/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3896e-04\n",
            "Epoch 434/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.4295e-04\n",
            "Epoch 435/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.6816e-04\n",
            "Epoch 436/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.0591e-04\n",
            "Epoch 437/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.2493e-04\n",
            "Epoch 438/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.0102e-04\n",
            "Epoch 439/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3104e-04\n",
            "Epoch 440/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4954e-04\n",
            "Epoch 441/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1722e-04\n",
            "Epoch 442/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.8423e-04\n",
            "Epoch 443/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2346e-04\n",
            "Epoch 444/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.5211e-04\n",
            "Epoch 445/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.2144e-04\n",
            "Epoch 446/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9931e-04\n",
            "Epoch 447/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.3769e-04\n",
            "Epoch 448/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2438e-04\n",
            "Epoch 449/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.3903e-04\n",
            "Epoch 450/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7868e-04\n",
            "Epoch 451/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.2120e-04\n",
            "Epoch 452/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.0598e-04\n",
            "Epoch 453/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0324e-04\n",
            "Epoch 454/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4644e-04\n",
            "Epoch 455/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.6828e-04\n",
            "Epoch 456/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.2890e-04\n",
            "Epoch 457/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.1901e-04\n",
            "Epoch 458/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.8911e-04\n",
            "Epoch 459/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8311e-04\n",
            "Epoch 460/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.0045e-04\n",
            "Epoch 461/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.6025e-04\n",
            "Epoch 462/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.3920e-04\n",
            "Epoch 463/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.0659e-04\n",
            "Epoch 464/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.9171e-04\n",
            "Epoch 465/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.1242e-04\n",
            "Epoch 466/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.2142e-04\n",
            "Epoch 467/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.2375e-04\n",
            "Epoch 468/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.2771e-04\n",
            "Epoch 469/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.3575e-04\n",
            "Epoch 470/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7256e-04\n",
            "Epoch 471/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.9084e-04\n",
            "Epoch 472/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2454e-04\n",
            "Epoch 473/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9266e-04\n",
            "Epoch 474/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3875e-04\n",
            "Epoch 475/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3398e-04\n",
            "Epoch 476/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.7060e-04\n",
            "Epoch 477/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8415e-04\n",
            "Epoch 478/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8108e-04\n",
            "Epoch 479/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.3946e-04\n",
            "Epoch 480/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.0102e-04\n",
            "Epoch 481/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.5284e-04\n",
            "Epoch 482/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.6858e-04\n",
            "Epoch 483/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8235e-04\n",
            "Epoch 484/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.0465e-04\n",
            "Epoch 485/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1427e-04\n",
            "Epoch 486/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1205e-04\n",
            "Epoch 487/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2150e-04\n",
            "Epoch 488/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2185e-04\n",
            "Epoch 489/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.2846e-04\n",
            "Epoch 490/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.1129e-04\n",
            "Epoch 491/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.2172e-04\n",
            "Epoch 492/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.4232e-04\n",
            "Epoch 493/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.5218e-04\n",
            "Epoch 494/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.2914e-04\n",
            "Epoch 495/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.5002e-04\n",
            "Epoch 496/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7727e-04\n",
            "Epoch 497/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.5935e-04\n",
            "Epoch 498/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1643e-04\n",
            "Epoch 499/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.1381e-04\n",
            "Epoch 500/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.6680e-04\n",
            "Epoch 501/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4195e-04\n",
            "Epoch 502/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1201e-04\n",
            "Epoch 503/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.9583e-04\n",
            "Epoch 504/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9265e-04\n",
            "Epoch 505/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1000e-04\n",
            "Epoch 506/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5864e-04\n",
            "Epoch 507/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.5558e-04\n",
            "Epoch 508/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7842e-04\n",
            "Epoch 509/5000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2.9873e-04\n",
            "Epoch 510/5000\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.1179e-04\n",
            "Epoch 511/5000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 3.1250e-04\n",
            "Epoch 512/5000\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2.7922e-04\n",
            "Epoch 513/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.2258e-04\n",
            "Epoch 514/5000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2.8224e-04\n",
            "Epoch 515/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.3062e-04\n",
            "Epoch 516/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9362e-04\n",
            "Epoch 517/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9792e-04\n",
            "Epoch 518/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.2572e-04\n",
            "Epoch 519/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8245e-04\n",
            "Epoch 520/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0631e-04\n",
            "Epoch 521/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.0252e-04\n",
            "Epoch 522/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0143e-04\n",
            "Epoch 523/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8367e-04\n",
            "Epoch 524/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8290e-04\n",
            "Epoch 525/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1570e-04\n",
            "Epoch 526/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6904e-04\n",
            "Epoch 527/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8275e-04\n",
            "Epoch 528/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.3067e-04\n",
            "Epoch 529/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7026e-04\n",
            "Epoch 530/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8942e-04\n",
            "Epoch 531/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.1441e-04\n",
            "Epoch 532/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.8427e-04\n",
            "Epoch 533/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.6399e-04\n",
            "Epoch 534/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7261e-04\n",
            "Epoch 535/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.6866e-04\n",
            "Epoch 536/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.1857e-04\n",
            "Epoch 537/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8243e-04\n",
            "Epoch 538/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.1043e-04\n",
            "Epoch 539/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8470e-04\n",
            "Epoch 540/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.6086e-04\n",
            "Epoch 541/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7764e-04\n",
            "Epoch 542/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.5477e-04\n",
            "Epoch 543/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.2326e-04\n",
            "Epoch 544/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0581e-04\n",
            "Epoch 545/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2666e-04\n",
            "Epoch 546/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.2697e-04\n",
            "Epoch 547/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.6336e-04\n",
            "Epoch 548/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.8286e-04\n",
            "Epoch 549/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7149e-04\n",
            "Epoch 550/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.6190e-04\n",
            "Epoch 551/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7661e-04\n",
            "Epoch 552/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8876e-04\n",
            "Epoch 553/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9677e-04\n",
            "Epoch 554/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.5858e-04\n",
            "Epoch 555/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9983e-04\n",
            "Epoch 556/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6727e-04\n",
            "Epoch 557/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9017e-04\n",
            "Epoch 558/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8908e-04\n",
            "Epoch 559/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.1000e-04\n",
            "Epoch 560/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.5453e-04\n",
            "Epoch 561/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0209e-04\n",
            "Epoch 562/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4969e-04\n",
            "Epoch 563/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.6858e-04\n",
            "Epoch 564/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.5066e-04\n",
            "Epoch 565/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7906e-04\n",
            "Epoch 566/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8048e-04\n",
            "Epoch 567/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3122e-04\n",
            "Epoch 568/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9567e-04\n",
            "Epoch 569/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4205e-04\n",
            "Epoch 570/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8866e-04\n",
            "Epoch 571/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5458e-04\n",
            "Epoch 572/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8449e-04\n",
            "Epoch 573/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5934e-04\n",
            "Epoch 574/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2128e-04\n",
            "Epoch 575/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0944e-04\n",
            "Epoch 576/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5675e-04\n",
            "Epoch 577/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9853e-04\n",
            "Epoch 578/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3073e-04\n",
            "Epoch 579/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.4081e-04\n",
            "Epoch 580/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2161e-04\n",
            "Epoch 581/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7842e-04\n",
            "Epoch 582/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8237e-04\n",
            "Epoch 583/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.6363e-04\n",
            "Epoch 584/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4212e-04\n",
            "Epoch 585/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3095e-04\n",
            "Epoch 586/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.0481e-04\n",
            "Epoch 587/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.6137e-04\n",
            "Epoch 588/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.6286e-04\n",
            "Epoch 589/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.9183e-04\n",
            "Epoch 590/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4007e-04\n",
            "Epoch 591/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5266e-04\n",
            "Epoch 592/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.5822e-04\n",
            "Epoch 593/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.1328e-04\n",
            "Epoch 594/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.1615e-04\n",
            "Epoch 595/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8046e-04\n",
            "Epoch 596/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8035e-04\n",
            "Epoch 597/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4937e-04\n",
            "Epoch 598/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.1015e-04\n",
            "Epoch 599/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.1246e-04\n",
            "Epoch 600/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2820e-04\n",
            "Epoch 601/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4958e-04\n",
            "Epoch 602/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.3783e-04\n",
            "Epoch 603/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.5005e-04\n",
            "Epoch 604/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.6886e-04\n",
            "Epoch 605/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.5032e-04\n",
            "Epoch 606/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5733e-04\n",
            "Epoch 607/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7734e-04\n",
            "Epoch 608/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3257e-04\n",
            "Epoch 609/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3872e-04\n",
            "Epoch 610/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4925e-04\n",
            "Epoch 611/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7733e-04\n",
            "Epoch 612/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5393e-04\n",
            "Epoch 613/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3937e-04\n",
            "Epoch 614/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7143e-04\n",
            "Epoch 615/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6083e-04\n",
            "Epoch 616/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.9876e-04\n",
            "Epoch 617/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9588e-04\n",
            "Epoch 618/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3025e-04\n",
            "Epoch 619/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3812e-04\n",
            "Epoch 620/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4665e-04\n",
            "Epoch 621/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8309e-04\n",
            "Epoch 622/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9998e-04\n",
            "Epoch 623/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8050e-04\n",
            "Epoch 624/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.3566e-04\n",
            "Epoch 625/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5429e-04\n",
            "Epoch 626/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3969e-04\n",
            "Epoch 627/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2854e-04\n",
            "Epoch 628/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.5626e-04\n",
            "Epoch 629/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2575e-04\n",
            "Epoch 630/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3762e-04\n",
            "Epoch 631/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4059e-04\n",
            "Epoch 632/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5112e-04\n",
            "Epoch 633/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.9751e-04\n",
            "Epoch 634/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.6042e-04\n",
            "Epoch 635/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2426e-04\n",
            "Epoch 636/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3629e-04\n",
            "Epoch 637/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0862e-04\n",
            "Epoch 638/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8694e-04\n",
            "Epoch 639/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.1258e-04\n",
            "Epoch 640/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1394e-04\n",
            "Epoch 641/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5901e-04\n",
            "Epoch 642/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5251e-04\n",
            "Epoch 643/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2826e-04\n",
            "Epoch 644/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2007e-04\n",
            "Epoch 645/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3208e-04\n",
            "Epoch 646/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3849e-04\n",
            "Epoch 647/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.1172e-04\n",
            "Epoch 648/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.1691e-04\n",
            "Epoch 649/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1302e-04\n",
            "Epoch 650/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3902e-04\n",
            "Epoch 651/5000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.5631e-04\n",
            "Epoch 652/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.7757e-04\n",
            "Epoch 653/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2964e-04\n",
            "Epoch 654/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3309e-04\n",
            "Epoch 655/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.2447e-04\n",
            "Epoch 656/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3341e-04\n",
            "Epoch 657/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.6990e-04\n",
            "Epoch 658/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.5045e-04\n",
            "Epoch 659/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2174e-04\n",
            "Epoch 660/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6224e-04\n",
            "Epoch 661/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.9127e-04\n",
            "Epoch 662/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8443e-04\n",
            "Epoch 663/5000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1.6869e-04\n",
            "Epoch 664/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3305e-04\n",
            "Epoch 665/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2027e-04\n",
            "Epoch 666/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1892e-04\n",
            "Epoch 667/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3081e-04\n",
            "Epoch 668/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2667e-04\n",
            "Epoch 669/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2160e-04\n",
            "Epoch 670/5000\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.9765e-04\n",
            "Epoch 671/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1907e-04\n",
            "Epoch 672/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3541e-04\n",
            "Epoch 673/5000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-d5ea8b6bcf12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel_ann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mtime_train_ann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model"
      ],
      "metadata": {
        "id": "_CRKziCqWmH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kxG4WaGnWkXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing bunch of neural networks"
      ],
      "metadata": {
        "id": "f8UNkpd8eX_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 1 fail"
      ],
      "metadata": {
        "id": "FQxS8WmResIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(50,activation='tanh'))\n",
        "    model.add(Dense(25,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gZ7ZB32heWm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 2 fail"
      ],
      "metadata": {
        "id": "q4R85Ns6ew63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(76, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(38,activation='tanh'))\n",
        "    model.add(Dense(19,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ypS3d69Le5IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 3 better"
      ],
      "metadata": {
        "id": "zo-SqNeoexEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(32,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qptOBLgXe7XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 4"
      ],
      "metadata": {
        "id": "tFut7kzDexPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(256,activation='tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(32,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HFiWHcrL4Oof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 5"
      ],
      "metadata": {
        "id": "kTFOXN7qexeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(256,activation='tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(32,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TCubwZvp5iVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 6 batch size 64"
      ],
      "metadata": {
        "id": "3g9-jT9x6MAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(32,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=64, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xMf1OYfZ6NXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 7 batch size 128"
      ],
      "metadata": {
        "id": "bHIdCv_a6R4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(32,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=128, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GdVLyIjA6UYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 8"
      ],
      "metadata": {
        "id": "6CJIPAOa6WkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(80, input_dim=7, activation='tanh', kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(80, activation='tanh', kernel_regularizer=regularizers.l2(0.001)))\n",
        "    model.add(Dense(40, activation='tanh', kernel_regularizer=regularizers.l2(0.001)))\n",
        "    model.add(Dense(40, activation='tanh', kernel_regularizer=regularizers.l2(0.001)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rEA3aZJS6xxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 9"
      ],
      "metadata": {
        "id": "81JU7C-ngQ9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(256,activation='tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(32,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=128, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1pnQXi75gTje"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}