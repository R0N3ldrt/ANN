{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOjjSwdS4h/fm1KdUam8ST/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R0N3ldrt/Thesis/blob/main/CNN_boot_v11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traininig NN Spectrum"
      ],
      "metadata": {
        "id": "UT4kgsEJEpyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Necesary Libraries"
      ],
      "metadata": {
        "id": "qsIzw6RGEwWp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0YnA2ltpEoSi"
      },
      "outputs": [],
      "source": [
        "# Importing necesary libraries\n",
        "# Libraries for correct code execution \n",
        "\n",
        "import os, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pickle\n",
        "import random\n",
        "import csv\n",
        "import re\n",
        "import array\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "from functools import reduce\n",
        "from random import random, gauss\n",
        "from math import modf, pi, cos, sin, sqrt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.stats.stats import pearsonr\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "import scipy.stats as st\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import os, time, math, csv, joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "import os, time, math, csv, joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Enviroment"
      ],
      "metadata": {
        "id": "o99IEHDfE2yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE PARA USAR DESDE COLAB\n",
        "\n",
        "# Google drive loading as work station for local-usage of the files.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount= True)\n",
        "\n",
        "#-----------------------------------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruXEP78XE7E5",
        "outputId": "1b1c8839-0580-4240-f0d9-049a8b545c7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista para cambiar los paths rapido.\n",
        "workers = [\"Ronald\", \"Local\"]\n",
        "\n",
        "# Change the number to change the paths.\n",
        "worker = workers[0]\n",
        "\n",
        "if worker == \"Ronald\":\n",
        "  path = \"/content/gdrive/MyDrive/Thesis_Workstation/ANN_dataset\"\n",
        "else: path = os.getcwd()"
      ],
      "metadata": {
        "id": "1Y2-m9DsE74Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#New Train/Test Split"
      ],
      "metadata": {
        "id": "rIpeu9O5teEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = path + \"/Spectrum/CNN/new_working_df.csv\"\n",
        "\n",
        "working_df = pd.read_csv(input_path)\n",
        "working_df['Distance_km'] = working_df['Distance_km'].astype(int)"
      ],
      "metadata": {
        "id": "KTcWRF8PtiS9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(working_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOUrK-J5ydE2",
        "outputId": "86d1be12-b07c-46cb-db6a-9e5835458e49"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2500, 2073)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "excluding_model_data_df = working_df.loc[working_df['PBRS_id'] != 1]"
      ],
      "metadata": {
        "id": "L-MvMzWr6vno"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Reference data"
      ],
      "metadata": {
        "id": "bgUl5jOx2vOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_data_df = working_df.loc[working_df['PBRS_id'] == 1]\n",
        "model_dict = {}\n",
        "for row in range(model_data_df.shape[0]):\n",
        "  distance = working_df.iloc[row,3]\n",
        "  values = np.array(working_df.iloc[row,6:working_df.shape[1]])\n",
        "  model_dict[distance] = values"
      ],
      "metadata": {
        "id": "jkn74j8u-0Yp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reference_and_sample_data(working_df, num_reference, num_sample):\n",
        "  reference_arr = [x for x in range(1, num_reference+1)]\n",
        "  reference_data_df = working_df[working_df['PBRS_id'].isin(reference_arr)]\n",
        "\n",
        "  sample_arr = [x for x in range(num_reference+1, num_sample+1)]\n",
        "  sample_data_df = working_df[working_df['PBRS_id'].isin(sample_arr)]\n",
        "  \n",
        "  return reference_data_df, sample_data_df"
      ],
      "metadata": {
        "id": "ec3YhJwn9Hxx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_data_df, sample_data_df = get_reference_and_sample_data(working_df, num_reference=50, num_sample=100)"
      ],
      "metadata": {
        "id": "FVf66kD39uvY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get test/train split"
      ],
      "metadata": {
        "id": "gaVlwtnN-Ezo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_idx_train_test_split(working_df, trainingProp = 0.8):\n",
        "  rows_training = np.array([])\n",
        "  rows_testing = np.array([])\n",
        "  distances = [x*80 for x in range(1, 26)]\n",
        "  for d in distances:\n",
        "    distance_working_df = working_df.loc[working_df['Distance_km'] == d]\n",
        "    rows_mixed=np.random.permutation(distance_working_df.shape[0])\n",
        "\n",
        "    training_amt = math.ceil(distance_working_df.shape[0]*trainingProp)\n",
        "    testing_amt = distance_working_df.shape[0] - training_amt\n",
        "\n",
        "    rows_training = np.append(rows_training, rows_mixed[:training_amt])\n",
        "    rows_testing = np.append(rows_testing, rows_mixed[-testing_amt:])\n",
        "  \n",
        "  rows_training = rows_training.astype('int').tolist()\n",
        "  rows_testing = rows_testing.astype('int').tolist()\n",
        "\n",
        "  return rows_training, rows_testing"
      ],
      "metadata": {
        "id": "yKHiNNANyyDB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows_training, rows_testing = get_idx_train_test_split(sample_data_df, trainingProp = 0.8)"
      ],
      "metadata": {
        "id": "YAx8kKMT0rMZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get frequencies filter"
      ],
      "metadata": {
        "id": "WEbwMwMG-9BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frequencies_filter(df, num_selected_freq=20):\n",
        "  info_df = df.iloc[:, 0:6]\n",
        "  data_df = df.iloc[:, 6:df.shape[1]]\n",
        "  span_val = int(data_df.shape[1]/num_selected_freq)\n",
        "  for i in range(num_selected_freq):\n",
        "    df2 = data_df.iloc[:, (i+1)*span_val].to_frame()\n",
        "    if i+1 == 1:\n",
        "      new_df = df2\n",
        "    else:\n",
        "      new_df = pd.merge(new_df, df2, left_index=True, right_index=True)\n",
        "  new_df = info_df.join(new_df)\n",
        "  \n",
        "  return new_df"
      ],
      "metadata": {
        "id": "RqeELlLC_vlH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_freq_data_df = frequencies_filter(reference_data_df, num_selected_freq=20)\n",
        "\n",
        "sample_freq_data_df = frequencies_filter(sample_data_df, num_selected_freq=20)\n",
        "\n",
        "# TEST num_selected_freq values"
      ],
      "metadata": {
        "id": "jxqxR6dwBY6d"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_freq_data_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "8VkCHPRs0lec",
        "outputId": "7b84547f-bba4-433a-ba1b-03b32a1e1f12"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       row  PBRS_id  Channels  Distance_km  power_dBm  #span   -28.9375  \\\n",
              "1250  1250       51         1           80          0      1 -11.371108   \n",
              "1251  1251       51         1          160          0      2 -10.211595   \n",
              "1252  1252       51         1          240          0      3 -15.204040   \n",
              "1253  1253       51         1          320          0      4 -13.629725   \n",
              "1254  1254       51         1          400          0      5 -10.817269   \n",
              "...    ...      ...       ...          ...        ...    ...        ...   \n",
              "2495  2495      100         1         1680          0     21 -10.384048   \n",
              "2496  2496      100         1         1760          0     22 -25.989634   \n",
              "2497  2497      100         1         1840          0     23  -6.889217   \n",
              "2498  2498      100         1         1920          0     24 -17.875031   \n",
              "2499  2499      100         1         2000          0     25 -12.975259   \n",
              "\n",
              "      -25.71875      -22.5  -19.28125  ...       3.25    6.46875     9.6875  \\\n",
              "1250  -9.888275 -15.896047 -10.401251  ... -11.985474 -10.041763 -10.323638   \n",
              "1251 -10.649810  -9.031208 -16.839636  ... -12.088256 -11.504632  -7.955859   \n",
              "1252 -19.299046  -9.498365  -4.519786  ... -18.301708 -13.122521  -7.990432   \n",
              "1253 -20.440475 -11.564857 -11.154106  ... -14.078396  -7.761546 -11.642466   \n",
              "1254 -15.656722 -13.972180 -20.629751  ... -18.218828 -15.606157  -8.490232   \n",
              "...         ...        ...        ...  ...        ...        ...        ...   \n",
              "2495 -15.111328 -10.947651 -15.621388  ...  -8.297015 -10.186144  -7.996039   \n",
              "2496  -7.263092 -13.035062 -10.594661  ... -11.335702 -11.592846  -9.906562   \n",
              "2497 -17.560904 -12.411949 -14.249330  ... -11.040326 -13.328168  -8.388157   \n",
              "2498  -8.925251 -11.985465 -14.717908  ... -13.172462 -10.790016 -11.697045   \n",
              "2499 -12.292847 -19.219643 -22.856406  ... -19.238656 -18.467817 -17.271477   \n",
              "\n",
              "       12.90625     16.125   19.34375    22.5625   25.78125       29.0  \\\n",
              "1250 -17.261711 -10.552016 -16.379117 -18.675457 -11.977936 -11.043479   \n",
              "1251 -11.202520  -8.600032 -10.355622  -9.990740  -8.624055 -16.657460   \n",
              "1252  -9.126269 -17.643167 -13.834385 -15.660303  -6.391696  -9.668572   \n",
              "1253 -15.153048 -17.844676 -10.922279  -9.524572 -12.243487 -12.415649   \n",
              "1254 -17.095917  -7.016890 -12.053797 -11.479498  -6.427090 -17.091879   \n",
              "...         ...        ...        ...        ...        ...        ...   \n",
              "2495  -7.510316 -12.258500 -17.332990 -12.781404 -11.458765  -9.875344   \n",
              "2496  -7.139437 -11.066623 -11.801799 -12.502789  -7.710519 -16.097928   \n",
              "2497 -10.133369 -19.487009 -11.853957 -14.058775  -6.946959 -14.632147   \n",
              "2498 -11.293293 -14.034699 -16.059040 -12.485453 -12.692961 -17.194686   \n",
              "2499  -8.429088 -15.266872  -9.899949 -14.370661 -14.245584  -6.555617   \n",
              "\n",
              "       32.21875  \n",
              "1250 -15.544026  \n",
              "1251 -18.454892  \n",
              "1252  -9.910335  \n",
              "1253 -30.553415  \n",
              "1254 -19.309437  \n",
              "...         ...  \n",
              "2495 -22.044932  \n",
              "2496 -19.157402  \n",
              "2497 -21.732752  \n",
              "2498 -22.963780  \n",
              "2499 -22.498299  \n",
              "\n",
              "[1250 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3e0364d-0a7c-4533-b569-4ec8b402b7b6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row</th>\n",
              "      <th>PBRS_id</th>\n",
              "      <th>Channels</th>\n",
              "      <th>Distance_km</th>\n",
              "      <th>power_dBm</th>\n",
              "      <th>#span</th>\n",
              "      <th>-28.9375</th>\n",
              "      <th>-25.71875</th>\n",
              "      <th>-22.5</th>\n",
              "      <th>-19.28125</th>\n",
              "      <th>...</th>\n",
              "      <th>3.25</th>\n",
              "      <th>6.46875</th>\n",
              "      <th>9.6875</th>\n",
              "      <th>12.90625</th>\n",
              "      <th>16.125</th>\n",
              "      <th>19.34375</th>\n",
              "      <th>22.5625</th>\n",
              "      <th>25.78125</th>\n",
              "      <th>29.0</th>\n",
              "      <th>32.21875</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1250</th>\n",
              "      <td>1250</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-11.371108</td>\n",
              "      <td>-9.888275</td>\n",
              "      <td>-15.896047</td>\n",
              "      <td>-10.401251</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.985474</td>\n",
              "      <td>-10.041763</td>\n",
              "      <td>-10.323638</td>\n",
              "      <td>-17.261711</td>\n",
              "      <td>-10.552016</td>\n",
              "      <td>-16.379117</td>\n",
              "      <td>-18.675457</td>\n",
              "      <td>-11.977936</td>\n",
              "      <td>-11.043479</td>\n",
              "      <td>-15.544026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1251</th>\n",
              "      <td>1251</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>160</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-10.211595</td>\n",
              "      <td>-10.649810</td>\n",
              "      <td>-9.031208</td>\n",
              "      <td>-16.839636</td>\n",
              "      <td>...</td>\n",
              "      <td>-12.088256</td>\n",
              "      <td>-11.504632</td>\n",
              "      <td>-7.955859</td>\n",
              "      <td>-11.202520</td>\n",
              "      <td>-8.600032</td>\n",
              "      <td>-10.355622</td>\n",
              "      <td>-9.990740</td>\n",
              "      <td>-8.624055</td>\n",
              "      <td>-16.657460</td>\n",
              "      <td>-18.454892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1252</th>\n",
              "      <td>1252</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>240</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-15.204040</td>\n",
              "      <td>-19.299046</td>\n",
              "      <td>-9.498365</td>\n",
              "      <td>-4.519786</td>\n",
              "      <td>...</td>\n",
              "      <td>-18.301708</td>\n",
              "      <td>-13.122521</td>\n",
              "      <td>-7.990432</td>\n",
              "      <td>-9.126269</td>\n",
              "      <td>-17.643167</td>\n",
              "      <td>-13.834385</td>\n",
              "      <td>-15.660303</td>\n",
              "      <td>-6.391696</td>\n",
              "      <td>-9.668572</td>\n",
              "      <td>-9.910335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1253</th>\n",
              "      <td>1253</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>320</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-13.629725</td>\n",
              "      <td>-20.440475</td>\n",
              "      <td>-11.564857</td>\n",
              "      <td>-11.154106</td>\n",
              "      <td>...</td>\n",
              "      <td>-14.078396</td>\n",
              "      <td>-7.761546</td>\n",
              "      <td>-11.642466</td>\n",
              "      <td>-15.153048</td>\n",
              "      <td>-17.844676</td>\n",
              "      <td>-10.922279</td>\n",
              "      <td>-9.524572</td>\n",
              "      <td>-12.243487</td>\n",
              "      <td>-12.415649</td>\n",
              "      <td>-30.553415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1254</th>\n",
              "      <td>1254</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>-10.817269</td>\n",
              "      <td>-15.656722</td>\n",
              "      <td>-13.972180</td>\n",
              "      <td>-20.629751</td>\n",
              "      <td>...</td>\n",
              "      <td>-18.218828</td>\n",
              "      <td>-15.606157</td>\n",
              "      <td>-8.490232</td>\n",
              "      <td>-17.095917</td>\n",
              "      <td>-7.016890</td>\n",
              "      <td>-12.053797</td>\n",
              "      <td>-11.479498</td>\n",
              "      <td>-6.427090</td>\n",
              "      <td>-17.091879</td>\n",
              "      <td>-19.309437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>2495</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1680</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>-10.384048</td>\n",
              "      <td>-15.111328</td>\n",
              "      <td>-10.947651</td>\n",
              "      <td>-15.621388</td>\n",
              "      <td>...</td>\n",
              "      <td>-8.297015</td>\n",
              "      <td>-10.186144</td>\n",
              "      <td>-7.996039</td>\n",
              "      <td>-7.510316</td>\n",
              "      <td>-12.258500</td>\n",
              "      <td>-17.332990</td>\n",
              "      <td>-12.781404</td>\n",
              "      <td>-11.458765</td>\n",
              "      <td>-9.875344</td>\n",
              "      <td>-22.044932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>2496</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1760</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>-25.989634</td>\n",
              "      <td>-7.263092</td>\n",
              "      <td>-13.035062</td>\n",
              "      <td>-10.594661</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.335702</td>\n",
              "      <td>-11.592846</td>\n",
              "      <td>-9.906562</td>\n",
              "      <td>-7.139437</td>\n",
              "      <td>-11.066623</td>\n",
              "      <td>-11.801799</td>\n",
              "      <td>-12.502789</td>\n",
              "      <td>-7.710519</td>\n",
              "      <td>-16.097928</td>\n",
              "      <td>-19.157402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>2497</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1840</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>-6.889217</td>\n",
              "      <td>-17.560904</td>\n",
              "      <td>-12.411949</td>\n",
              "      <td>-14.249330</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.040326</td>\n",
              "      <td>-13.328168</td>\n",
              "      <td>-8.388157</td>\n",
              "      <td>-10.133369</td>\n",
              "      <td>-19.487009</td>\n",
              "      <td>-11.853957</td>\n",
              "      <td>-14.058775</td>\n",
              "      <td>-6.946959</td>\n",
              "      <td>-14.632147</td>\n",
              "      <td>-21.732752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>2498</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1920</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>-17.875031</td>\n",
              "      <td>-8.925251</td>\n",
              "      <td>-11.985465</td>\n",
              "      <td>-14.717908</td>\n",
              "      <td>...</td>\n",
              "      <td>-13.172462</td>\n",
              "      <td>-10.790016</td>\n",
              "      <td>-11.697045</td>\n",
              "      <td>-11.293293</td>\n",
              "      <td>-14.034699</td>\n",
              "      <td>-16.059040</td>\n",
              "      <td>-12.485453</td>\n",
              "      <td>-12.692961</td>\n",
              "      <td>-17.194686</td>\n",
              "      <td>-22.963780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>2499</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>2000</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>-12.975259</td>\n",
              "      <td>-12.292847</td>\n",
              "      <td>-19.219643</td>\n",
              "      <td>-22.856406</td>\n",
              "      <td>...</td>\n",
              "      <td>-19.238656</td>\n",
              "      <td>-18.467817</td>\n",
              "      <td>-17.271477</td>\n",
              "      <td>-8.429088</td>\n",
              "      <td>-15.266872</td>\n",
              "      <td>-9.899949</td>\n",
              "      <td>-14.370661</td>\n",
              "      <td>-14.245584</td>\n",
              "      <td>-6.555617</td>\n",
              "      <td>-22.498299</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1250 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3e0364d-0a7c-4533-b569-4ec8b402b7b6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a3e0364d-0a7c-4533-b569-4ec8b402b7b6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a3e0364d-0a7c-4533-b569-4ec8b402b7b6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get features"
      ],
      "metadata": {
        "id": "S2Z2xg2yHuoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample_freq_data_df\n",
        "#distance_reference_df\n",
        "#selected_rows = rows_training\n",
        "def get_features(sample_freq_data_df, reference_freq_data_df, selected_rows):\n",
        "  data_distance = []\n",
        "  data_mean = []\n",
        "  data_std = []\n",
        "  data_pearson_min = []\n",
        "  data_pearson_mean = []\n",
        "  data_pearson_max = []\n",
        "  data_CI_lower = []\n",
        "  data_CI_upper = []\n",
        "\n",
        "  for row_idx in tqdm(selected_rows):\n",
        "    data_Y = sample_freq_data_df.iloc[row_idx,3] # distance of row selected\n",
        "    data_X = np.array(sample_freq_data_df.iloc[row_idx,6:sample_freq_data_df.shape[1]]) #array of data in row selected\n",
        "    data_distance.append(data_Y)\n",
        "    data_std.append(np.std(data_X))\n",
        "    data_mean.append(np.mean(data_X))\n",
        "\n",
        "    data_ci = st.t.interval(alpha=0.90, df=len(data_X)-1, loc=np.mean(data_X), scale=st.sem(data_X))\n",
        "    data_confidence_int_lower = data_ci[0]\n",
        "    data_confidence_int_upper = data_ci[1]\n",
        "\n",
        "    # Get reference data\n",
        "    reference_freq_data_df\n",
        "    distance_reference_df = reference_freq_data_df.loc[reference_freq_data_df['Distance_km'] == data_Y]\n",
        "\n",
        "    pearson_vals = []\n",
        "    confidence_vals_upper = []\n",
        "    confidence_vals_lower = []\n",
        "    for reference_row in range(0, distance_reference_df.shape[0]):\n",
        "      reference_X = np.array(distance_reference_df.iloc[reference_row,6:distance_reference_df.shape[1]])\n",
        "      pearson_vals.append(round(pearsonr(data_X, reference_X)[0], 5))\n",
        "\n",
        "      reference_ci = st.t.interval(alpha=0.90, df=len(reference_X)-1, loc=np.mean(reference_X), scale=st.sem(reference_X))\n",
        "      reference_confidence_int_lower = reference_ci[0]\n",
        "      confidence_vals_lower.append(reference_confidence_int_lower)\n",
        "      reference_confidence_int_upper = reference_ci[1]\n",
        "      confidence_vals_upper.append(reference_confidence_int_upper)\n",
        "\n",
        "    data_pearson_min.append(np.min(pearson_vals))\n",
        "    data_pearson_mean.append(np.mean(pearson_vals))    \n",
        "    data_pearson_max.append(np.max(pearson_vals))\n",
        "\n",
        "    data_CI_lower.append(abs(np.min(confidence_vals_lower)-data_confidence_int_lower))\n",
        "    data_CI_upper.append(abs(np.max(confidence_vals_upper)-data_confidence_int_upper))\n",
        "  data = {'distance':data_distance,\n",
        "                  'mean':data_mean,\n",
        "                  'std':data_std,\n",
        "                  'pearson_min':data_pearson_min,\n",
        "                  'pearson_mean':data_pearson_mean,\n",
        "                  'pearson_max':data_pearson_max,\n",
        "                  'delta_CI_min':data_CI_lower,\n",
        "                  'delta_CI_max':data_CI_upper}\n",
        "\n",
        "  data_df = pd.DataFrame(data)\n",
        "\n",
        "  return data_df"
      ],
      "metadata": {
        "id": "8FNsUKTVxZmP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df = get_features(sample_freq_data_df, reference_freq_data_df, rows_training)\n",
        "\n",
        "testing_df = get_features(sample_freq_data_df, reference_freq_data_df, rows_testing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guPIPhGRAlb3",
        "outputId": "c5cff2c9-a0f1-4d39-bd21-693535c7366f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:56<00:00, 17.79it/s]\n",
            "100%|██████████| 250/250 [00:13<00:00, 18.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def get_features(excluding_model_data_df, selected_rows):\n",
        "\n",
        "  data_distance = []\n",
        "  data_mean = []\n",
        "  data_std = []\n",
        "  data_pearson_min = []\n",
        "  data_pearson_mean = []\n",
        "  data_pearson_max = []\n",
        "\n",
        "  for row_idx in tqdm(selected_rows):\n",
        "    data_Y = excluding_model_data_df.iloc[row_idx,3]\n",
        "    data_X = np.array(excluding_model_data_df.iloc[row_idx,6:excluding_model_data_df.shape[1]])\n",
        "    data_distance.append(data_Y)\n",
        "    data_std.append(np.std(data_X))\n",
        "    data_mean.append(np.mean(data_X))\n",
        "\n",
        "    pearson_vals = []\n",
        "    for k, v in model_dict.items():  \n",
        "      pearson_vals.append(round(pearsonr(v, data_X)[0], 5))\n",
        "      print(v)\n",
        "      print(data_X)\n",
        "      break\n",
        "    data_pearson_min.append(np.min(pearson_vals))\n",
        "    data_pearson_mean.append(np.mean(pearson_vals))    \n",
        "    data_pearson_max.append(np.max(pearson_vals))\n",
        "    break\n",
        "  data = {'distance':data_distance,\n",
        "                  'mean':data_mean,\n",
        "                  'std':data_std,\n",
        "                  'pearson_min':data_pearson_min,\n",
        "                  'pearson_mean':data_pearson_mean,\n",
        "                  'pearson_max':data_pearson_max}\n",
        "\n",
        "  data_df = pd.DataFrame(data)\n",
        "\n",
        "  return data_df\n",
        "training_df = get_features(excluding_model_data_df, rows_training)\n",
        "\n",
        "testing_df = get_features(excluding_model_data_df, rows_testing)\n",
        "'''"
      ],
      "metadata": {
        "id": "FnshPiRq0wxU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bootstrapping"
      ],
      "metadata": {
        "id": "qksszb65u-2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = path + \"/Spectrum/CNN/working_df.csv\"\n",
        "\n",
        "working_df = pd.read_csv(input_path)\n",
        "working_df['Distance_km'] = working_df['Distance_km'].astype(int)"
      ],
      "metadata": {
        "id": "cEYZP7YSvA4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distances = [x*80 for x in range(1, 26)]"
      ],
      "metadata": {
        "id": "vpEDJN-hvodC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get max and min values of each column"
      ],
      "metadata": {
        "id": "qbD-Ts__HVqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_min_val_col(working_df):\n",
        "  data_working_df = working_df.iloc[:, 6:working_df.shape[1]]\n",
        "  h_list = list(data_working_df.columns.values)\n",
        "  max_min_col_dict = {header:[] for header in h_list}\n",
        "\n",
        "  for d in tqdm(distances):\n",
        "    boot_dist_df = working_df.loc[working_df['Distance_km'] == d]\n",
        "    for idx in range(6, boot_dist_df.shape[1]):\n",
        "      max_val_col = float(boot_dist_df.iloc[:, [idx]].max()) # max val in col\n",
        "      min_val_col = float(boot_dist_df.iloc[:, [idx]].min()) # min val in col\n",
        "      h = boot_dist_df.iloc[:, [idx]].columns # header of col\n",
        "      h = h[0]\n",
        "      max_min_col_dict[h].append((d, min_val_col, max_val_col))\n",
        "  return max_min_col_dict"
      ],
      "metadata": {
        "id": "pfmHMEtv-fA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_min_col_dict = get_max_min_val_col(working_df)"
      ],
      "metadata": {
        "id": "vhWI9GaP_k_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_min_col_dict)"
      ],
      "metadata": {
        "id": "Tu_r9lHJCGIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create new samples using bootstrap technique"
      ],
      "metadata": {
        "id": "MUBCgrizHc1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bootstrap_samples(working_df, max_min_col_dict, num_of_new_samples, previos_amt_of_samples=13):\n",
        "  import random\n",
        "\n",
        "  row_data = []\n",
        "  PBRS_id_data = []\n",
        "  Distance_km_data = []\n",
        "  power_dBm_data = []\n",
        "  span_data = []\n",
        "\n",
        "  h_list = list(working_df.columns.values)\n",
        "  new_data = {header:[] for header in h_list}\n",
        "\n",
        "\n",
        "  last_row_in_old_df_val = working_df.shape[0]\n",
        "  for new_sample in tqdm(range(num_of_new_samples)): # generating (num_of_new_samples) new samples for en data\n",
        "    for d in distances:\n",
        "      new_data['row'].append(last_row_in_old_df_val)\n",
        "      last_row_in_old_df_val += 1\n",
        "      new_data['PBRS_id'].append(new_sample+(previos_amt_of_samples+1))\n",
        "      new_data['Channels'].append(1)\n",
        "\n",
        "      new_data['Distance_km'].append(d)\n",
        "      new_data['power_dBm'].append(0)\n",
        "      new_data['#span'].append(int(d/80))\n",
        "\n",
        "      for k, v in max_min_col_dict.items(): # header:(d, min_val_col, max_val_col)\n",
        "        for val in v:\n",
        "          if val[0] == d:\n",
        "            bootstrap_val = round(random.uniform(val[1], val[2]), 14)\n",
        "            new_data[str(k)].append(bootstrap_val)\n",
        "            break\n",
        "  new_data_df = pd.DataFrame.from_dict(new_data)\n",
        "\n",
        "  return new_data_df"
      ],
      "metadata": {
        "id": "96AJljiWv2Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_df = create_bootstrap_samples(working_df, max_min_col_dict, num_of_new_samples=87, previos_amt_of_samples=13)"
      ],
      "metadata": {
        "id": "zN5Uwep1HHvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine previos df with bootstraped df"
      ],
      "metadata": {
        "id": "VtwPz_xDHj1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_working_df = working_df.append(new_data_df)"
      ],
      "metadata": {
        "id": "P6amWSh7HRUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_working_df.to_csv(path+'/Spectrum/CNN/new_working_df.csv', index=False)"
      ],
      "metadata": {
        "id": "nfuCRKffItYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "CGa0J5JcS-BW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing"
      ],
      "metadata": {
        "id": "eydWoJNWTEGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training_df=pd.read_csv(path+'/Spectrum/CNN/training_data_DNN.csv')\n",
        "\n",
        "Y = training_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "X = training_df.iloc[:, 1:training_df.shape[1]]\n",
        "\n",
        "sc_input = MinMaxScaler()\n",
        "sc_output = MinMaxScaler()\n",
        "Y_train = sc_output.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "X_train = sc_input.fit_transform(X) # convert features to values from 0 to 1"
      ],
      "metadata": {
        "id": "ZEGP4I1lTDZy"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing_df=pd.read_csv(path+'/Spectrum/CNN/testing_data_DNN.csv')\n",
        "\n",
        "Y = testing_df.iloc[:, 0].to_numpy().reshape(-1,1)\n",
        "X = testing_df.iloc[:, 1:testing_df.shape[1]]\n",
        "\n",
        "\n",
        "sc_input = MinMaxScaler()\n",
        "sc_output = MinMaxScaler()\n",
        "Y_test = sc_output.fit_transform(Y) # convert distances to values from 0 to 1\n",
        "X_test = sc_input.fit_transform(X) # convert features to values from 0 to 1"
      ],
      "metadata": {
        "id": "zFzruEAeW8gH"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "pdv_LIJAV6YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(76, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(38,activation='tanh'))\n",
        "    model.add(Dense(19,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time"
      ],
      "metadata": {
        "id": "E3baz5KzV4mb",
        "outputId": "f5ed9f0f-bef5-4626-bc72-000a784b6832",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5000\n",
            "32/32 [==============================] - 1s 2ms/step - loss: 0.1076\n",
            "Epoch 2/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0939\n",
            "Epoch 3/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0888\n",
            "Epoch 4/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0865\n",
            "Epoch 5/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0851\n",
            "Epoch 6/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0798\n",
            "Epoch 7/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0790\n",
            "Epoch 8/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0752\n",
            "Epoch 9/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0720\n",
            "Epoch 10/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0685\n",
            "Epoch 11/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0666\n",
            "Epoch 12/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0635\n",
            "Epoch 13/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0606\n",
            "Epoch 14/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0573\n",
            "Epoch 15/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0564\n",
            "Epoch 16/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0539\n",
            "Epoch 17/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0515\n",
            "Epoch 18/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0498\n",
            "Epoch 19/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0497\n",
            "Epoch 20/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0476\n",
            "Epoch 21/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0469\n",
            "Epoch 22/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0464\n",
            "Epoch 23/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0445\n",
            "Epoch 24/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0439\n",
            "Epoch 25/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0440\n",
            "Epoch 26/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0436\n",
            "Epoch 27/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0428\n",
            "Epoch 28/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0427\n",
            "Epoch 29/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0426\n",
            "Epoch 30/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0414\n",
            "Epoch 31/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0404\n",
            "Epoch 32/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0406\n",
            "Epoch 33/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0386\n",
            "Epoch 34/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0386\n",
            "Epoch 35/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0394\n",
            "Epoch 36/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0382\n",
            "Epoch 37/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0375\n",
            "Epoch 38/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0374\n",
            "Epoch 39/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0377\n",
            "Epoch 40/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0355\n",
            "Epoch 41/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0355\n",
            "Epoch 42/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0349\n",
            "Epoch 43/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0348\n",
            "Epoch 44/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0332\n",
            "Epoch 45/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0317\n",
            "Epoch 46/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0319\n",
            "Epoch 47/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0306\n",
            "Epoch 48/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0308\n",
            "Epoch 49/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0297\n",
            "Epoch 50/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0290\n",
            "Epoch 51/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0278\n",
            "Epoch 52/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0272\n",
            "Epoch 53/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0259\n",
            "Epoch 54/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0250\n",
            "Epoch 55/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0247\n",
            "Epoch 56/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0238\n",
            "Epoch 57/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0217\n",
            "Epoch 58/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0226\n",
            "Epoch 59/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0217\n",
            "Epoch 60/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0205\n",
            "Epoch 61/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0193\n",
            "Epoch 62/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0189\n",
            "Epoch 63/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0182\n",
            "Epoch 64/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 65/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 66/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 67/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0167\n",
            "Epoch 68/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0156\n",
            "Epoch 69/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0151\n",
            "Epoch 70/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0137\n",
            "Epoch 71/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0146\n",
            "Epoch 72/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0144\n",
            "Epoch 73/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0137\n",
            "Epoch 74/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 75/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 76/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0122\n",
            "Epoch 77/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0125\n",
            "Epoch 78/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0116\n",
            "Epoch 79/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0121\n",
            "Epoch 80/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0111\n",
            "Epoch 81/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0114\n",
            "Epoch 82/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0100\n",
            "Epoch 83/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0103\n",
            "Epoch 84/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0100\n",
            "Epoch 85/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0093\n",
            "Epoch 86/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0087\n",
            "Epoch 87/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0088\n",
            "Epoch 88/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0081\n",
            "Epoch 89/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0073\n",
            "Epoch 90/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0076\n",
            "Epoch 91/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0074\n",
            "Epoch 92/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0074\n",
            "Epoch 93/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0074\n",
            "Epoch 94/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0068\n",
            "Epoch 95/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0057\n",
            "Epoch 96/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0066\n",
            "Epoch 97/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0051\n",
            "Epoch 98/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0061\n",
            "Epoch 99/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0048\n",
            "Epoch 100/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0050\n",
            "Epoch 101/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0047\n",
            "Epoch 102/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0047\n",
            "Epoch 103/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0044\n",
            "Epoch 104/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0041\n",
            "Epoch 105/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0041\n",
            "Epoch 106/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0044\n",
            "Epoch 107/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 108/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0042\n",
            "Epoch 109/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0037\n",
            "Epoch 110/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0039\n",
            "Epoch 111/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 112/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 113/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0035\n",
            "Epoch 114/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 115/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 116/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 117/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 118/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 119/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 120/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 121/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 122/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 123/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Epoch 124/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 125/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 126/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0031\n",
            "Epoch 127/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 128/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 129/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 130/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0028\n",
            "Epoch 131/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 132/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0028\n",
            "Epoch 133/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 134/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 135/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 136/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 137/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 138/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 139/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 140/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 141/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 142/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 143/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 144/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 145/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 146/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 147/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 148/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 149/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 150/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 151/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 152/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 153/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 154/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 155/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 156/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 157/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 158/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 159/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 160/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 161/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 162/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 163/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 164/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 165/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 166/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 167/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 168/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 169/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 170/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 171/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 172/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 173/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 174/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 175/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 176/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 177/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 178/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 179/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 180/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 181/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 182/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 183/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 184/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 185/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 186/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 187/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 188/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 189/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 190/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 191/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 192/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 193/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 194/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 195/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 196/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 197/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 198/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 199/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 200/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 201/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 202/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 203/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 204/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 205/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 206/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 207/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 208/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 209/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 210/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 211/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 212/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 213/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 214/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 215/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 216/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 217/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 218/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 219/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 220/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.7789e-04\n",
            "Epoch 221/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 222/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.8592e-04\n",
            "Epoch 223/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 224/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 225/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 226/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.9974e-04\n",
            "Epoch 227/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 228/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.1623e-04\n",
            "Epoch 229/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 230/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 231/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 232/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 233/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 234/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 235/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.8361e-04\n",
            "Epoch 236/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 237/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 238/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 239/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 240/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 241/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.7895e-04\n",
            "Epoch 242/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 243/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 244/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.0032e-04\n",
            "Epoch 245/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 246/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.3963e-04\n",
            "Epoch 247/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.3351e-04\n",
            "Epoch 248/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 249/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 250/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 251/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 252/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 253/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.5380e-04\n",
            "Epoch 254/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.0366e-04\n",
            "Epoch 255/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 256/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.7538e-04\n",
            "Epoch 257/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.9194e-04\n",
            "Epoch 258/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 259/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.5644e-04\n",
            "Epoch 260/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 261/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.6619e-04\n",
            "Epoch 262/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.1756e-04\n",
            "Epoch 263/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.7708e-04\n",
            "Epoch 264/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 265/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.5088e-04\n",
            "Epoch 266/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 267/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 268/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.4332e-04\n",
            "Epoch 269/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 270/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.0253e-04\n",
            "Epoch 271/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 272/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 273/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.0482e-04\n",
            "Epoch 274/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.8345e-04\n",
            "Epoch 275/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.3050e-04\n",
            "Epoch 276/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 277/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.4425e-04\n",
            "Epoch 278/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.9355e-04\n",
            "Epoch 279/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 280/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.6511e-04\n",
            "Epoch 281/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.8373e-04\n",
            "Epoch 282/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.1079e-04\n",
            "Epoch 283/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.2879e-04\n",
            "Epoch 284/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.7983e-04\n",
            "Epoch 285/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.3542e-04\n",
            "Epoch 286/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.8270e-04\n",
            "Epoch 287/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.6942e-04\n",
            "Epoch 288/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.2970e-04\n",
            "Epoch 289/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.1718e-04\n",
            "Epoch 290/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.3837e-04\n",
            "Epoch 291/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.7035e-04\n",
            "Epoch 292/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 293/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.0456e-04\n",
            "Epoch 294/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.9514e-04\n",
            "Epoch 295/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.4873e-04\n",
            "Epoch 296/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.5812e-04\n",
            "Epoch 297/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.6482e-04\n",
            "Epoch 298/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.4066e-04\n",
            "Epoch 299/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.4302e-04\n",
            "Epoch 300/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.4650e-04\n",
            "Epoch 301/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.3417e-04\n",
            "Epoch 302/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.5219e-04\n",
            "Epoch 303/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.5547e-04\n",
            "Epoch 304/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.6480e-04\n",
            "Epoch 305/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.6854e-04\n",
            "Epoch 306/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.7706e-04\n",
            "Epoch 307/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.9887e-04\n",
            "Epoch 308/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.4977e-04\n",
            "Epoch 309/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.6317e-04\n",
            "Epoch 310/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.2324e-04\n",
            "Epoch 311/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.4063e-04\n",
            "Epoch 312/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.7043e-04\n",
            "Epoch 313/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.3744e-04\n",
            "Epoch 314/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.3899e-04\n",
            "Epoch 315/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.6421e-04\n",
            "Epoch 316/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.5686e-04\n",
            "Epoch 317/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.0948e-04\n",
            "Epoch 318/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.4892e-04\n",
            "Epoch 319/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.5459e-04\n",
            "Epoch 320/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.3259e-04\n",
            "Epoch 321/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.8562e-04\n",
            "Epoch 322/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.5294e-04\n",
            "Epoch 323/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.4700e-04\n",
            "Epoch 324/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.9312e-04\n",
            "Epoch 325/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.8908e-04\n",
            "Epoch 326/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.2541e-04\n",
            "Epoch 327/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.2677e-04\n",
            "Epoch 328/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.5998e-04\n",
            "Epoch 329/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.1580e-04\n",
            "Epoch 330/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.1796e-04\n",
            "Epoch 331/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.0729e-04\n",
            "Epoch 332/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.4744e-04\n",
            "Epoch 333/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.5241e-04\n",
            "Epoch 334/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.2103e-04\n",
            "Epoch 335/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.0304e-04\n",
            "Epoch 336/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.1182e-04\n",
            "Epoch 337/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.9746e-04\n",
            "Epoch 338/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.2802e-04\n",
            "Epoch 339/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.7410e-04\n",
            "Epoch 340/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.8510e-04\n",
            "Epoch 341/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.9377e-04\n",
            "Epoch 342/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.6429e-04\n",
            "Epoch 343/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.3051e-04\n",
            "Epoch 344/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.8087e-04\n",
            "Epoch 345/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.2227e-04\n",
            "Epoch 346/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.7396e-04\n",
            "Epoch 347/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.6742e-04\n",
            "Epoch 348/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.6642e-04\n",
            "Epoch 349/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.9800e-04\n",
            "Epoch 350/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.8927e-04\n",
            "Epoch 351/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.7712e-04\n",
            "Epoch 352/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.3415e-04\n",
            "Epoch 353/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.4579e-04\n",
            "Epoch 354/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.8615e-04\n",
            "Epoch 355/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.0870e-04\n",
            "Epoch 356/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.1183e-04\n",
            "Epoch 357/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.4091e-04\n",
            "Epoch 358/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.9487e-04\n",
            "Epoch 359/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 9.6051e-04\n",
            "Epoch 360/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.2707e-04\n",
            "Epoch 361/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.2728e-04\n",
            "Epoch 362/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.7942e-04\n",
            "Epoch 363/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.1270e-04\n",
            "Epoch 364/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.2672e-04\n",
            "Epoch 365/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.2433e-04\n",
            "Epoch 366/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.0642e-04\n",
            "Epoch 367/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.6111e-04\n",
            "Epoch 368/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.5454e-04\n",
            "Epoch 369/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.2509e-04\n",
            "Epoch 370/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.1020e-04\n",
            "Epoch 371/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.0523e-04\n",
            "Epoch 372/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.2896e-04\n",
            "Epoch 373/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.0728e-04\n",
            "Epoch 374/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.6995e-04\n",
            "Epoch 375/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.9387e-04\n",
            "Epoch 376/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.9704e-04\n",
            "Epoch 377/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.0640e-04\n",
            "Epoch 378/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.6417e-04\n",
            "Epoch 379/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.3875e-04\n",
            "Epoch 380/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.7344e-04\n",
            "Epoch 381/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.1102e-04\n",
            "Epoch 382/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.7270e-04\n",
            "Epoch 383/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.3120e-04\n",
            "Epoch 384/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.7170e-04\n",
            "Epoch 385/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.8396e-04\n",
            "Epoch 386/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.2808e-04\n",
            "Epoch 387/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.4928e-04\n",
            "Epoch 388/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.4071e-04\n",
            "Epoch 389/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.8906e-04\n",
            "Epoch 390/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 7.1775e-04\n",
            "Epoch 391/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4892e-04\n",
            "Epoch 392/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.8772e-04\n",
            "Epoch 393/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.0342e-04\n",
            "Epoch 394/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.7134e-04\n",
            "Epoch 395/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.9628e-04\n",
            "Epoch 396/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.9832e-04\n",
            "Epoch 397/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.6537e-04\n",
            "Epoch 398/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.1923e-04\n",
            "Epoch 399/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.5964e-04\n",
            "Epoch 400/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.7213e-04\n",
            "Epoch 401/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.3068e-04\n",
            "Epoch 402/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.3809e-04\n",
            "Epoch 403/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.3693e-04\n",
            "Epoch 404/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.2173e-04\n",
            "Epoch 405/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.1414e-04\n",
            "Epoch 406/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.8733e-04\n",
            "Epoch 407/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.9897e-04\n",
            "Epoch 408/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.7751e-04\n",
            "Epoch 409/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.1932e-04\n",
            "Epoch 410/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.6414e-04\n",
            "Epoch 411/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.6518e-04\n",
            "Epoch 412/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.4037e-04\n",
            "Epoch 413/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4058e-04\n",
            "Epoch 414/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.4836e-04\n",
            "Epoch 415/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.0486e-04\n",
            "Epoch 416/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.7547e-04\n",
            "Epoch 417/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.5439e-04\n",
            "Epoch 418/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.8058e-04\n",
            "Epoch 419/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.9685e-04\n",
            "Epoch 420/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9507e-04\n",
            "Epoch 421/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.6910e-04\n",
            "Epoch 422/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4492e-04\n",
            "Epoch 423/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.9109e-04\n",
            "Epoch 424/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.6154e-04\n",
            "Epoch 425/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.7103e-04\n",
            "Epoch 426/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.5953e-04\n",
            "Epoch 427/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.1974e-04\n",
            "Epoch 428/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.7236e-04\n",
            "Epoch 429/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.7684e-04\n",
            "Epoch 430/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.7476e-04\n",
            "Epoch 431/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8691e-04\n",
            "Epoch 432/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.8977e-04\n",
            "Epoch 433/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.9510e-04\n",
            "Epoch 434/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.6649e-04\n",
            "Epoch 435/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.8419e-04\n",
            "Epoch 436/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.5702e-04\n",
            "Epoch 437/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.6237e-04\n",
            "Epoch 438/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.2211e-04\n",
            "Epoch 439/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.6146e-04\n",
            "Epoch 440/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4247e-04\n",
            "Epoch 441/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.4384e-04\n",
            "Epoch 442/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.6029e-04\n",
            "Epoch 443/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.9231e-04\n",
            "Epoch 444/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.6135e-04\n",
            "Epoch 445/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.1016e-04\n",
            "Epoch 446/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.5195e-04\n",
            "Epoch 447/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4381e-04\n",
            "Epoch 448/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0561e-04\n",
            "Epoch 449/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.6756e-04\n",
            "Epoch 450/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.0369e-04\n",
            "Epoch 451/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.5628e-04\n",
            "Epoch 452/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0672e-04\n",
            "Epoch 453/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.2621e-04\n",
            "Epoch 454/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.7917e-04\n",
            "Epoch 455/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.4253e-04\n",
            "Epoch 456/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.7207e-04\n",
            "Epoch 457/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 6.1545e-04\n",
            "Epoch 458/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3525e-04\n",
            "Epoch 459/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.7709e-04\n",
            "Epoch 460/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4496e-04\n",
            "Epoch 461/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.0590e-04\n",
            "Epoch 462/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.2901e-04\n",
            "Epoch 463/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.3259e-04\n",
            "Epoch 464/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.0669e-04\n",
            "Epoch 465/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.5024e-04\n",
            "Epoch 466/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.3636e-04\n",
            "Epoch 467/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4962e-04\n",
            "Epoch 468/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.9065e-04\n",
            "Epoch 469/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9801e-04\n",
            "Epoch 470/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.3179e-04\n",
            "Epoch 471/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4322e-04\n",
            "Epoch 472/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.5651e-04\n",
            "Epoch 473/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4278e-04\n",
            "Epoch 474/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.8624e-04\n",
            "Epoch 475/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.0199e-04\n",
            "Epoch 476/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.1964e-04\n",
            "Epoch 477/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.0730e-04\n",
            "Epoch 478/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.5122e-04\n",
            "Epoch 479/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8056e-04\n",
            "Epoch 480/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4493e-04\n",
            "Epoch 481/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.9029e-04\n",
            "Epoch 482/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.7439e-04\n",
            "Epoch 483/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3387e-04\n",
            "Epoch 484/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.8642e-04\n",
            "Epoch 485/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.6931e-04\n",
            "Epoch 486/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.8702e-04\n",
            "Epoch 487/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3497e-04\n",
            "Epoch 488/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9717e-04\n",
            "Epoch 489/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.6065e-04\n",
            "Epoch 490/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9822e-04\n",
            "Epoch 491/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.6114e-04\n",
            "Epoch 492/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.9890e-04\n",
            "Epoch 493/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.9178e-04\n",
            "Epoch 494/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.8579e-04\n",
            "Epoch 495/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.5785e-04\n",
            "Epoch 496/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.7351e-04\n",
            "Epoch 497/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.2610e-04\n",
            "Epoch 498/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9012e-04\n",
            "Epoch 499/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.2115e-04\n",
            "Epoch 500/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.7277e-04\n",
            "Epoch 501/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0899e-04\n",
            "Epoch 502/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.2042e-04\n",
            "Epoch 503/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0218e-04\n",
            "Epoch 504/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.6095e-04\n",
            "Epoch 505/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.1677e-04\n",
            "Epoch 506/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.5667e-04\n",
            "Epoch 507/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4728e-04\n",
            "Epoch 508/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2601e-04\n",
            "Epoch 509/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0697e-04\n",
            "Epoch 510/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4211e-04\n",
            "Epoch 511/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4641e-04\n",
            "Epoch 512/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.9197e-04\n",
            "Epoch 513/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.1976e-04\n",
            "Epoch 514/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9586e-04\n",
            "Epoch 515/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.7604e-04\n",
            "Epoch 516/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8581e-04\n",
            "Epoch 517/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.6921e-04\n",
            "Epoch 518/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2785e-04\n",
            "Epoch 519/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.7086e-04\n",
            "Epoch 520/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4091e-04\n",
            "Epoch 521/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1376e-04\n",
            "Epoch 522/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 5.0114e-04\n",
            "Epoch 523/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8740e-04\n",
            "Epoch 524/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.9633e-04\n",
            "Epoch 525/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.7385e-04\n",
            "Epoch 526/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.0885e-04\n",
            "Epoch 527/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9647e-04\n",
            "Epoch 528/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.6273e-04\n",
            "Epoch 529/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.6990e-04\n",
            "Epoch 530/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2360e-04\n",
            "Epoch 531/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.0289e-04\n",
            "Epoch 532/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8696e-04\n",
            "Epoch 533/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1622e-04\n",
            "Epoch 534/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.5142e-04\n",
            "Epoch 535/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9714e-04\n",
            "Epoch 536/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8076e-04\n",
            "Epoch 537/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0506e-04\n",
            "Epoch 538/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.0928e-04\n",
            "Epoch 539/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.7681e-04\n",
            "Epoch 540/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3471e-04\n",
            "Epoch 541/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.6440e-04\n",
            "Epoch 542/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2788e-04\n",
            "Epoch 543/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2862e-04\n",
            "Epoch 544/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4589e-04\n",
            "Epoch 545/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.4853e-04\n",
            "Epoch 546/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.0916e-04\n",
            "Epoch 547/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.6228e-04\n",
            "Epoch 548/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8253e-04\n",
            "Epoch 549/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8173e-04\n",
            "Epoch 550/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4888e-04\n",
            "Epoch 551/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1645e-04\n",
            "Epoch 552/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2399e-04\n",
            "Epoch 553/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2903e-04\n",
            "Epoch 554/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.8014e-04\n",
            "Epoch 555/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.6031e-04\n",
            "Epoch 556/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2910e-04\n",
            "Epoch 557/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1736e-04\n",
            "Epoch 558/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4796e-04\n",
            "Epoch 559/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5215e-04\n",
            "Epoch 560/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.6144e-04\n",
            "Epoch 561/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3279e-04\n",
            "Epoch 562/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4769e-04\n",
            "Epoch 563/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1853e-04\n",
            "Epoch 564/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8329e-04\n",
            "Epoch 565/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.0726e-04\n",
            "Epoch 566/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2909e-04\n",
            "Epoch 567/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2263e-04\n",
            "Epoch 568/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4298e-04\n",
            "Epoch 569/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1131e-04\n",
            "Epoch 570/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.7019e-04\n",
            "Epoch 571/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.5928e-04\n",
            "Epoch 572/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8835e-04\n",
            "Epoch 573/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4333e-04\n",
            "Epoch 574/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1107e-04\n",
            "Epoch 575/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.9886e-04\n",
            "Epoch 576/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8158e-04\n",
            "Epoch 577/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4924e-04\n",
            "Epoch 578/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.3447e-04\n",
            "Epoch 579/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7480e-04\n",
            "Epoch 580/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1912e-04\n",
            "Epoch 581/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5217e-04\n",
            "Epoch 582/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4459e-04\n",
            "Epoch 583/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6901e-04\n",
            "Epoch 584/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1745e-04\n",
            "Epoch 585/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1602e-04\n",
            "Epoch 586/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1562e-04\n",
            "Epoch 587/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2307e-04\n",
            "Epoch 588/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8374e-04\n",
            "Epoch 589/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4712e-04\n",
            "Epoch 590/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.7501e-04\n",
            "Epoch 591/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.7858e-04\n",
            "Epoch 592/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9091e-04\n",
            "Epoch 593/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.7504e-04\n",
            "Epoch 594/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0326e-04\n",
            "Epoch 595/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.0325e-04\n",
            "Epoch 596/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8381e-04\n",
            "Epoch 597/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.9266e-04\n",
            "Epoch 598/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8114e-04\n",
            "Epoch 599/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.0975e-04\n",
            "Epoch 600/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8007e-04\n",
            "Epoch 601/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2786e-04\n",
            "Epoch 602/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.7715e-04\n",
            "Epoch 603/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8552e-04\n",
            "Epoch 604/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.0674e-04\n",
            "Epoch 605/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8914e-04\n",
            "Epoch 606/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8024e-04\n",
            "Epoch 607/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8943e-04\n",
            "Epoch 608/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1155e-04\n",
            "Epoch 609/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.0059e-04\n",
            "Epoch 610/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6595e-04\n",
            "Epoch 611/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6306e-04\n",
            "Epoch 612/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8219e-04\n",
            "Epoch 613/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6355e-04\n",
            "Epoch 614/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8452e-04\n",
            "Epoch 615/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4093e-04\n",
            "Epoch 616/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7531e-04\n",
            "Epoch 617/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.7297e-04\n",
            "Epoch 618/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.1259e-04\n",
            "Epoch 619/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6302e-04\n",
            "Epoch 620/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8174e-04\n",
            "Epoch 621/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5990e-04\n",
            "Epoch 622/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.0074e-04\n",
            "Epoch 623/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3233e-04\n",
            "Epoch 624/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6679e-04\n",
            "Epoch 625/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.7899e-04\n",
            "Epoch 626/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6788e-04\n",
            "Epoch 627/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.7742e-04\n",
            "Epoch 628/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.7913e-04\n",
            "Epoch 629/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3904e-04\n",
            "Epoch 630/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6315e-04\n",
            "Epoch 631/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8357e-04\n",
            "Epoch 632/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9770e-04\n",
            "Epoch 633/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6948e-04\n",
            "Epoch 634/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4840e-04\n",
            "Epoch 635/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.9712e-04\n",
            "Epoch 636/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6970e-04\n",
            "Epoch 637/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5971e-04\n",
            "Epoch 638/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4585e-04\n",
            "Epoch 639/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5840e-04\n",
            "Epoch 640/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3123e-04\n",
            "Epoch 641/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.9488e-04\n",
            "Epoch 642/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5549e-04\n",
            "Epoch 643/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2989e-04\n",
            "Epoch 644/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6398e-04\n",
            "Epoch 645/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4972e-04\n",
            "Epoch 646/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9850e-04\n",
            "Epoch 647/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8399e-04\n",
            "Epoch 648/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4063e-04\n",
            "Epoch 649/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6720e-04\n",
            "Epoch 650/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2851e-04\n",
            "Epoch 651/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5148e-04\n",
            "Epoch 652/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6554e-04\n",
            "Epoch 653/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3443e-04\n",
            "Epoch 654/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8286e-04\n",
            "Epoch 655/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3157e-04\n",
            "Epoch 656/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5298e-04\n",
            "Epoch 657/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5230e-04\n",
            "Epoch 658/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1024e-04\n",
            "Epoch 659/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.0956e-04\n",
            "Epoch 660/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5564e-04\n",
            "Epoch 661/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1117e-04\n",
            "Epoch 662/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5416e-04\n",
            "Epoch 663/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4419e-04\n",
            "Epoch 664/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3484e-04\n",
            "Epoch 665/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1810e-04\n",
            "Epoch 666/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.7242e-04\n",
            "Epoch 667/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4891e-04\n",
            "Epoch 668/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2805e-04\n",
            "Epoch 669/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5662e-04\n",
            "Epoch 670/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4573e-04\n",
            "Epoch 671/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3286e-04\n",
            "Epoch 672/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4747e-04\n",
            "Epoch 673/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4381e-04\n",
            "Epoch 674/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0095e-04\n",
            "Epoch 675/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.9287e-04\n",
            "Epoch 676/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.8131e-04\n",
            "Epoch 677/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3268e-04\n",
            "Epoch 678/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2149e-04\n",
            "Epoch 679/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1203e-04\n",
            "Epoch 680/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5752e-04\n",
            "Epoch 681/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4198e-04\n",
            "Epoch 682/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3355e-04\n",
            "Epoch 683/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8285e-04\n",
            "Epoch 684/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2073e-04\n",
            "Epoch 685/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5646e-04\n",
            "Epoch 686/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7564e-04\n",
            "Epoch 687/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1920e-04\n",
            "Epoch 688/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4282e-04\n",
            "Epoch 689/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5285e-04\n",
            "Epoch 690/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8113e-04\n",
            "Epoch 691/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.7065e-04\n",
            "Epoch 692/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.8289e-04\n",
            "Epoch 693/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6808e-04\n",
            "Epoch 694/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1044e-04\n",
            "Epoch 695/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2464e-04\n",
            "Epoch 696/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4334e-04\n",
            "Epoch 697/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0705e-04\n",
            "Epoch 698/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5031e-04\n",
            "Epoch 699/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8211e-04\n",
            "Epoch 700/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0214e-04\n",
            "Epoch 701/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2752e-04\n",
            "Epoch 702/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9564e-04\n",
            "Epoch 703/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.7256e-04\n",
            "Epoch 704/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7034e-04\n",
            "Epoch 705/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1285e-04\n",
            "Epoch 706/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.9707e-04\n",
            "Epoch 707/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.2963e-04\n",
            "Epoch 708/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.3698e-04\n",
            "Epoch 709/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.2335e-04\n",
            "Epoch 710/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2371e-04\n",
            "Epoch 711/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6462e-04\n",
            "Epoch 712/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1110e-04\n",
            "Epoch 713/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0062e-04\n",
            "Epoch 714/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4407e-04\n",
            "Epoch 715/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2343e-04\n",
            "Epoch 716/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7425e-04\n",
            "Epoch 717/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5127e-04\n",
            "Epoch 718/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0499e-04\n",
            "Epoch 719/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7614e-04\n",
            "Epoch 720/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.5862e-04\n",
            "Epoch 721/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6517e-04\n",
            "Epoch 722/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1427e-04\n",
            "Epoch 723/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1868e-04\n",
            "Epoch 724/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1731e-04\n",
            "Epoch 725/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0034e-04\n",
            "Epoch 726/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4231e-04\n",
            "Epoch 727/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5904e-04\n",
            "Epoch 728/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1838e-04\n",
            "Epoch 729/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2169e-04\n",
            "Epoch 730/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9388e-04\n",
            "Epoch 731/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8721e-04\n",
            "Epoch 732/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9316e-04\n",
            "Epoch 733/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.6031e-04\n",
            "Epoch 734/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.7749e-04\n",
            "Epoch 735/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6555e-04\n",
            "Epoch 736/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.8266e-04\n",
            "Epoch 737/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.5753e-04\n",
            "Epoch 738/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4417e-04\n",
            "Epoch 739/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9495e-04\n",
            "Epoch 740/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9903e-04\n",
            "Epoch 741/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0649e-04\n",
            "Epoch 742/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1704e-04\n",
            "Epoch 743/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1377e-04\n",
            "Epoch 744/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6811e-04\n",
            "Epoch 745/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.9308e-04\n",
            "Epoch 746/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5923e-04\n",
            "Epoch 747/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1020e-04\n",
            "Epoch 748/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1047e-04\n",
            "Epoch 749/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9125e-04\n",
            "Epoch 750/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1112e-04\n",
            "Epoch 751/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0126e-04\n",
            "Epoch 752/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7754e-04\n",
            "Epoch 753/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8334e-04\n",
            "Epoch 754/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1695e-04\n",
            "Epoch 755/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2411e-04\n",
            "Epoch 756/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7448e-04\n",
            "Epoch 757/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8770e-04\n",
            "Epoch 758/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9519e-04\n",
            "Epoch 759/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2669e-04\n",
            "Epoch 760/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9439e-04\n",
            "Epoch 761/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.7286e-04\n",
            "Epoch 762/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1822e-04\n",
            "Epoch 763/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0731e-04\n",
            "Epoch 764/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7270e-04\n",
            "Epoch 765/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0324e-04\n",
            "Epoch 766/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1871e-04\n",
            "Epoch 767/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6603e-04\n",
            "Epoch 768/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.1448e-04\n",
            "Epoch 769/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6631e-04\n",
            "Epoch 770/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.4432e-04\n",
            "Epoch 771/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6172e-04\n",
            "Epoch 772/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9545e-04\n",
            "Epoch 773/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.8327e-04\n",
            "Epoch 774/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5617e-04\n",
            "Epoch 775/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0342e-04\n",
            "Epoch 776/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9327e-04\n",
            "Epoch 777/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6780e-04\n",
            "Epoch 778/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0889e-04\n",
            "Epoch 779/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0474e-04\n",
            "Epoch 780/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.1496e-04\n",
            "Epoch 781/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.5944e-04\n",
            "Epoch 782/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9813e-04\n",
            "Epoch 783/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7211e-04\n",
            "Epoch 784/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6578e-04\n",
            "Epoch 785/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2838e-04\n",
            "Epoch 786/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.5652e-04\n",
            "Epoch 787/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9108e-04\n",
            "Epoch 788/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9975e-04\n",
            "Epoch 789/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9983e-04\n",
            "Epoch 790/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.4592e-04\n",
            "Epoch 791/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9076e-04\n",
            "Epoch 792/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9514e-04\n",
            "Epoch 793/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3860e-04\n",
            "Epoch 794/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8740e-04\n",
            "Epoch 795/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.8098e-04\n",
            "Epoch 796/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.1930e-04\n",
            "Epoch 797/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6315e-04\n",
            "Epoch 798/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9345e-04\n",
            "Epoch 799/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6034e-04\n",
            "Epoch 800/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7979e-04\n",
            "Epoch 801/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0285e-04\n",
            "Epoch 802/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7249e-04\n",
            "Epoch 803/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7908e-04\n",
            "Epoch 804/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5611e-04\n",
            "Epoch 805/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.9667e-04\n",
            "Epoch 806/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6289e-04\n",
            "Epoch 807/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.7000e-04\n",
            "Epoch 808/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8008e-04\n",
            "Epoch 809/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6035e-04\n",
            "Epoch 810/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3877e-04\n",
            "Epoch 811/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6023e-04\n",
            "Epoch 812/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0173e-04\n",
            "Epoch 813/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7984e-04\n",
            "Epoch 814/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7268e-04\n",
            "Epoch 815/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.0303e-04\n",
            "Epoch 816/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4914e-04\n",
            "Epoch 817/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6383e-04\n",
            "Epoch 818/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7601e-04\n",
            "Epoch 819/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5087e-04\n",
            "Epoch 820/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7319e-04\n",
            "Epoch 821/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0088e-04\n",
            "Epoch 822/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.5381e-04\n",
            "Epoch 823/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8077e-04\n",
            "Epoch 824/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9394e-04\n",
            "Epoch 825/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7345e-04\n",
            "Epoch 826/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6360e-04\n",
            "Epoch 827/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7957e-04\n",
            "Epoch 828/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6334e-04\n",
            "Epoch 829/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7712e-04\n",
            "Epoch 830/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5998e-04\n",
            "Epoch 831/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8635e-04\n",
            "Epoch 832/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8899e-04\n",
            "Epoch 833/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3885e-04\n",
            "Epoch 834/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0163e-04\n",
            "Epoch 835/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.3127e-04\n",
            "Epoch 836/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.0418e-04\n",
            "Epoch 837/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4761e-04\n",
            "Epoch 838/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6878e-04\n",
            "Epoch 839/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8300e-04\n",
            "Epoch 840/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2161e-04\n",
            "Epoch 841/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 7.7130e-05\n",
            "Epoch 842/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8368e-04\n",
            "Epoch 843/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7973e-04\n",
            "Epoch 844/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4471e-04\n",
            "Epoch 845/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.7262e-04\n",
            "Epoch 846/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.8671e-04\n",
            "Epoch 847/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.4260e-04\n",
            "Epoch 848/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3360e-04\n",
            "Epoch 849/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7789e-04\n",
            "Epoch 850/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7615e-04\n",
            "Epoch 851/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4619e-04\n",
            "Epoch 852/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6924e-04\n",
            "Epoch 853/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3348e-04\n",
            "Epoch 854/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4254e-04\n",
            "Epoch 855/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 8.8302e-05\n",
            "Epoch 856/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.8647e-04\n",
            "Epoch 857/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6330e-04\n",
            "Epoch 858/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4806e-04\n",
            "Epoch 859/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2306e-04\n",
            "Epoch 860/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.1438e-04\n",
            "Epoch 861/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7719e-04\n",
            "Epoch 862/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7748e-04\n",
            "Epoch 863/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3291e-04\n",
            "Epoch 864/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.5689e-04\n",
            "Epoch 865/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6364e-04\n",
            "Epoch 866/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6631e-04\n",
            "Epoch 867/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4834e-04\n",
            "Epoch 868/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7283e-04\n",
            "Epoch 869/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6770e-04\n",
            "Epoch 870/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.1011e-04\n",
            "Epoch 871/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7271e-04\n",
            "Epoch 872/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.4241e-04\n",
            "Epoch 873/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7463e-04\n",
            "Epoch 874/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4052e-04\n",
            "Epoch 875/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5001e-04\n",
            "Epoch 876/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.7689e-04\n",
            "Epoch 877/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.2484e-04\n",
            "Epoch 878/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3625e-04\n",
            "Epoch 879/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9773e-04\n",
            "Epoch 880/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6681e-04\n",
            "Epoch 881/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.2526e-04\n",
            "Epoch 882/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7079e-04\n",
            "Epoch 883/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3735e-04\n",
            "Epoch 884/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.5064e-04\n",
            "Epoch 885/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7364e-04\n",
            "Epoch 886/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.2684e-04\n",
            "Epoch 887/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.6915e-04\n",
            "Epoch 888/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3247e-04\n",
            "Epoch 889/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3989e-04\n",
            "Epoch 890/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6458e-04\n",
            "Epoch 891/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5166e-04\n",
            "Epoch 892/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.7369e-04\n",
            "Epoch 893/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4253e-04\n",
            "Epoch 894/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4327e-04\n",
            "Epoch 895/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4550e-04\n",
            "Epoch 896/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.4462e-04\n",
            "Epoch 897/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5326e-04\n",
            "Epoch 898/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.3884e-04\n",
            "Epoch 899/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5941e-04\n",
            "Epoch 900/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5605e-04\n",
            "Epoch 901/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4468e-04\n",
            "Epoch 902/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5459e-04\n",
            "Epoch 903/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.6413e-04\n",
            "Epoch 904/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4198e-04\n",
            "Epoch 905/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.3545e-04\n",
            "Epoch 906/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4396e-04\n",
            "Epoch 907/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3123e-04\n",
            "Epoch 908/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.4274e-04\n",
            "Epoch 909/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6847e-04\n",
            "Epoch 910/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.2775e-04\n",
            "Epoch 911/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.2696e-04\n",
            "Epoch 912/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5407e-04\n",
            "Epoch 913/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4296e-04\n",
            "Epoch 914/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4579e-04\n",
            "Epoch 915/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3858e-04\n",
            "Epoch 916/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4825e-04\n",
            "Epoch 917/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4700e-04\n",
            "Epoch 918/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.3213e-04\n",
            "Epoch 919/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6052e-04\n",
            "Epoch 920/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.2604e-04\n",
            "Epoch 921/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.6824e-04\n",
            "Epoch 922/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.2618e-04\n",
            "Epoch 923/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4427e-04\n",
            "Epoch 924/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3640e-04\n",
            "Epoch 925/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5544e-04\n",
            "Epoch 926/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4260e-04\n",
            "Epoch 927/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.3227e-04\n",
            "Epoch 928/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4169e-04\n",
            "Epoch 929/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.3494e-04\n",
            "Epoch 930/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3421e-04\n",
            "Epoch 931/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3324e-04\n",
            "Epoch 932/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.5827e-04\n",
            "Epoch 933/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.3963e-04\n",
            "Epoch 934/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.2430e-04\n",
            "Epoch 935/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5998e-04\n",
            "Epoch 936/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.2678e-04\n",
            "Epoch 937/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.2582e-04\n",
            "Epoch 938/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6708e-04\n",
            "Epoch 939/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.2935e-04\n",
            "Epoch 940/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4053e-04\n",
            "Epoch 941/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.4135e-04\n",
            "Epoch 942/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4065e-04\n",
            "Epoch 943/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3330e-04\n",
            "Epoch 944/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.1995e-04\n",
            "Epoch 945/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.4931e-04\n",
            "Epoch 946/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.6672e-04\n",
            "Epoch 947/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.0821e-04\n",
            "Epoch 948/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.5533e-04\n",
            "Epoch 949/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.6461e-04\n",
            "Epoch 950/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 9.7875e-05\n",
            "Epoch 951/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4356e-04\n",
            "Epoch 952/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.3626e-04\n",
            "Epoch 953/5000\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.1142e-04\n",
            "Epoch 954/5000\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4254e-04\n",
            "Epoch 955/5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model"
      ],
      "metadata": {
        "id": "_CRKziCqWmH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kxG4WaGnWkXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing bunch of neural networks"
      ],
      "metadata": {
        "id": "f8UNkpd8eX_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 1 fail"
      ],
      "metadata": {
        "id": "FQxS8WmResIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(50,activation='tanh'))\n",
        "    model.add(Dense(25,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gZ7ZB32heWm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 2 fail"
      ],
      "metadata": {
        "id": "q4R85Ns6ew63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(76, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(38,activation='tanh'))\n",
        "    model.add(Dense(19,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ypS3d69Le5IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 3 better"
      ],
      "metadata": {
        "id": "zo-SqNeoexEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(32,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qptOBLgXe7XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 4"
      ],
      "metadata": {
        "id": "tFut7kzDexPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(256,activation='tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(32,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HFiWHcrL4Oof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 5"
      ],
      "metadata": {
        "id": "kTFOXN7qexeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(256,activation='tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(32,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TCubwZvp5iVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 6 batch size 64"
      ],
      "metadata": {
        "id": "3g9-jT9x6MAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(32,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=64, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xMf1OYfZ6NXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 7 batch size 128"
      ],
      "metadata": {
        "id": "bHIdCv_a6R4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(32,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=128, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GdVLyIjA6UYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 8"
      ],
      "metadata": {
        "id": "6CJIPAOa6WkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(80, input_dim=7, activation='tanh', kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(80, activation='tanh', kernel_regularizer=regularizers.l2(0.001)))\n",
        "    model.add(Dense(40, activation='tanh', kernel_regularizer=regularizers.l2(0.001)))\n",
        "    model.add(Dense(40, activation='tanh', kernel_regularizer=regularizers.l2(0.001)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=32, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rEA3aZJS6xxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 9"
      ],
      "metadata": {
        "id": "81JU7C-ngQ9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D, Flatten\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def ann():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim = 7, activation = 'tanh'))\n",
        "    model.add(Dense(256,activation='tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(128,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(32,activation='tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss ='mean_squared_error',optimizer = 'RMSprop')\n",
        "    return model\n",
        "\n",
        "\n",
        "model_ann = KerasRegressor(build_fn=ann,epochs=5000,batch_size=128, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "callback = EarlyStopping(monitor='loss', patience=500)\n",
        "model_ann.fit(X_train, Y_train, callbacks=[callback])\n",
        "time_train_ann = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "Y_test_pred=model_ann.predict(X_test)\n",
        "pred=list(Y_test_pred)\n",
        "\n",
        "time_eval_ann=time.time()-start_time\n",
        "\n",
        "dist_min=sc_output.data_min_[0] # 80\n",
        "dist_max=sc_output.data_max_[0] # 2000\n",
        "\n",
        "real=list(list(zip(*Y_test))[0])\n",
        "real_abs=np.add(dist_min,np.multiply((dist_max-dist_min),real)) # convert back to distances values from scalar\n",
        "real_abs=[int(np.round(i)) for i in real_abs]\n",
        "pred_abs=np.add(dist_min,np.multiply((dist_max-dist_min),pred)) # convert back to distances values from scalar\n",
        "\n",
        "error=np.divide(np.abs(np.subtract(np.array(real_abs),np.array(pred_abs))),np.array(real_abs))\n",
        "error_v2 = np.abs(np.subtract(np.array(real_abs),np.array(pred_abs)))\n",
        "res=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error})\n",
        "display(res)\n",
        "res_v2=pd.DataFrame({\"dist\":real_abs,\"pred\":pred_abs,\"error\":error_v2})\n",
        "display(res_v2)\n",
        "#res.to_csv(path+\"/Spectrum/CNN/results_pme_supervisedFeatures.csv\", header=True, index=False)\n",
        "plt.plot(real_abs,error,'bo')\n",
        "plt.show()\n",
        "plt.plot(real_abs,error_v2,'bo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1pnQXi75gTje"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}